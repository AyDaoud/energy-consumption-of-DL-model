{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38d22ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /home/daouda/.cache/torch/hub/v0.10.0.zip\n",
      "/home/daouda/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/daouda/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/daouda/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:12<00:00, 19.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132e234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd7f88b",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca9f8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "2\n",
      "gotstarted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/daouda/workspace_test/Experiments_Final_11.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final_11.ipynb#W4sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m x \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final_11.ipynb#W4sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m x \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfc3(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final_11.ipynb#W4sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m p\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final_11.ipynb#W4sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m q\u001b[39m.\u001b[39mput(\u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final_11.ipynb#W4sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m# Signal the end of the power measurement\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(os\u001b[39m.\u001b[39;49mWNOHANG \u001b[39mif\u001b[39;49;00m timeout \u001b[39m==\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[39m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[39m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.adaptive_avg_pooling = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(self.maxpool(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Start the power measurement\n",
    "        q = multiprocessing.Queue()\n",
    "        q.put('start')\n",
    "        p = multiprocessing.Process(target=measure, args=(q,))\n",
    "        p.start()\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(self.maxpool(x)))\n",
    "        x = self.adaptive_avg_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # Signal the end of the measurement\n",
    "        q.put('stop')\n",
    "        # Wait for the measurement to finish\n",
    "        p.join()\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not q.empty():\n",
    "            power_output = q.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name(1))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = AlexNet().to(device)\n",
    "batch_size =128\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    \n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'                \n",
    "    x = X.clone()\n",
    "    x = model.conv1(x)\n",
    "    x = model.relu(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = model.maxpool(x)\n",
    "    x = model.conv2(x)\n",
    "    x = model.relu(x) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #x = model.maxpool(x)\n",
    "    x = model.conv3(x)\n",
    "    x = model.relu(x)\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "\n",
    "    x = model.conv4(x)\n",
    "    x = model.relu(x) \n",
    "    \n",
    "    \n",
    "    x = model.conv5(x)\n",
    "    x = model.relu(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = model.adaptive_avg_pooling(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = model.dropout(x)\n",
    "    \n",
    "    #x = model.maxpool(x)\n",
    "    x = model.fc1(x)\n",
    "    x = model.dropout(model.relu(x)) \n",
    "\n",
    "    x = model.fc2(x)\n",
    "    x = model.relu(x)\n",
    "\n",
    "    x = model.fc3(x)\n",
    "    q.put('stop')\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    output = x\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "    p.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power)\n",
    "percentile_90 = np.percentile(power_measurements, 95)\n",
    "average_above_90th = np.mean(np.array(power_measurements)[np.array(power_measurements) >= percentile_90])\n",
    "print('Average of values >= __th percentile:', average_above_90th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2044fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72.94, 94.21, 86.97, 93.0, 91.56, 89.91, 90.69, 90.3, 90.61, 90.46, 90.28, 87.76, 89.15, 91.11, 90.49, 90.13, 91.36, 88.44, 89.13, 90.02, 87.99, 89.47, 90.77, 81.0, 88.26, 90.78, 89.65, 87.6, 88.26, 86.41] xxxxx 93.8591\n"
     ]
    }
   ],
   "source": [
    "print(power_measurements,'xxxxx',np.percentile(power_measurements, 99)) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28f9e23a",
   "metadata": {},
   "source": [
    "Let's try it with 1 process for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46781b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "2\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.02 W\n",
      "49.37 W\n",
      "70.87 W\n",
      "76.47 W\n",
      "59.15 W\n",
      "39.76 W\n",
      "40.44 W\n",
      "39.47 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.83 W\n",
      "48.59 W\n",
      "67.96 W\n",
      "74.65 W\n",
      "62.85 W\n",
      "39.48 W\n",
      "39.77 W\n",
      "39.28 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.12 W\n",
      "48.89 W\n",
      "65.35 W\n",
      "76.00 W\n",
      "62.85 W\n",
      "39.38 W\n",
      "40.16 W\n",
      "39.67 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.01 W\n",
      "48.79 W\n",
      "66.80 W\n",
      "76.12 W\n",
      "62.94 W\n",
      "39.28 W\n",
      "40.36 W\n",
      "39.49 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.51 W\n",
      "48.50 W\n",
      "66.91 W\n",
      "75.02 W\n",
      "63.03 W\n",
      "39.09 W\n",
      "39.96 W\n",
      "39.47 W\n",
      "39.10 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.22 W\n",
      "48.89 W\n",
      "68.55 W\n",
      "72.13 W\n",
      "62.95 W\n",
      "39.68 W\n",
      "40.06 W\n",
      "39.38 W\n",
      "39.00 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.21 W\n",
      "49.47 W\n",
      "68.75 W\n",
      "76.47 W\n",
      "63.44 W\n",
      "39.57 W\n",
      "39.86 W\n",
      "39.18 W\n",
      "39.00 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.43 W\n",
      "48.59 W\n",
      "67.87 W\n",
      "76.08 W\n",
      "63.44 W\n",
      "39.48 W\n",
      "40.44 W\n",
      "39.49 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.52 W\n",
      "48.40 W\n",
      "67.78 W\n",
      "43.06 W\n",
      "61.89 W\n",
      "39.38 W\n",
      "39.96 W\n",
      "39.29 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.22 W\n",
      "48.88 W\n",
      "68.46 W\n",
      "76.38 W\n",
      "64.31 W\n",
      "39.68 W\n",
      "39.95 W\n",
      "39.18 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.22 W\n",
      "49.18 W\n",
      "68.36 W\n",
      "75.12 W\n",
      "62.66 W\n",
      "39.58 W\n",
      "40.26 W\n",
      "39.38 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.94 W\n",
      "48.69 W\n",
      "67.58 W\n",
      "75.61 W\n",
      "56.16 W\n",
      "39.48 W\n",
      "40.06 W\n",
      "39.29 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.93 W\n",
      "48.88 W\n",
      "68.65 W\n",
      "76.27 W\n",
      "63.34 W\n",
      "39.68 W\n",
      "40.55 W\n",
      "39.29 W\n",
      "39.09 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.23 W\n",
      "49.37 W\n",
      "67.88 W\n",
      "75.61 W\n",
      "63.34 W\n",
      "39.67 W\n",
      "39.77 W\n",
      "39.48 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.92 W\n",
      "48.79 W\n",
      "68.37 W\n",
      "76.99 W\n",
      "63.53 W\n",
      "39.27 W\n",
      "40.06 W\n",
      "39.57 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.22 W\n",
      "48.89 W\n",
      "69.52 W\n",
      "76.02 W\n",
      "63.12 W\n",
      "39.67 W\n",
      "40.44 W\n",
      "39.39 W\n",
      "39.09 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.41 W\n",
      "48.89 W\n",
      "69.72 W\n",
      "76.56 W\n",
      "63.61 W\n",
      "39.39 W\n",
      "39.88 W\n",
      "39.58 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.52 W\n",
      "48.79 W\n",
      "64.78 W\n",
      "76.87 W\n",
      "63.61 W\n",
      "39.39 W\n",
      "40.16 W\n",
      "39.58 W\n",
      "39.00 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.03 W\n",
      "48.98 W\n",
      "68.26 W\n",
      "75.22 W\n",
      "58.09 W\n",
      "39.68 W\n",
      "39.67 W\n",
      "39.29 W\n",
      "39.09 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.22 W\n",
      "48.31 W\n",
      "68.07 W\n",
      "76.56 W\n",
      "63.61 W\n",
      "39.67 W\n",
      "40.26 W\n",
      "39.28 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.42 W\n",
      "46.85 W\n",
      "68.44 W\n",
      "76.00 W\n",
      "63.53 W\n",
      "39.76 W\n",
      "40.44 W\n",
      "39.47 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.42 W\n",
      "49.47 W\n",
      "62.15 W\n",
      "76.08 W\n",
      "62.46 W\n",
      "39.28 W\n",
      "40.06 W\n",
      "39.37 W\n",
      "39.10 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.51 W\n",
      "48.80 W\n",
      "68.16 W\n",
      "76.37 W\n",
      "63.13 W\n",
      "39.29 W\n",
      "39.67 W\n",
      "39.28 W\n",
      "39.09 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.93 W\n",
      "48.89 W\n",
      "68.35 W\n",
      "76.56 W\n",
      "58.98 W\n",
      "39.67 W\n",
      "40.16 W\n",
      "39.18 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.62 W\n",
      "48.50 W\n",
      "68.54 W\n",
      "74.83 W\n",
      "63.12 W\n",
      "39.49 W\n",
      "40.26 W\n",
      "39.38 W\n",
      "39.00 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.72 W\n",
      "47.92 W\n",
      "67.78 W\n",
      "75.02 W\n",
      "63.12 W\n",
      "39.37 W\n",
      "40.15 W\n",
      "39.19 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.83 W\n",
      "48.40 W\n",
      "67.97 W\n",
      "75.80 W\n",
      "62.85 W\n",
      "39.28 W\n",
      "39.76 W\n",
      "39.27 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.90 W\n",
      "48.79 W\n",
      "68.74 W\n",
      "75.69 W\n",
      "64.28 W\n",
      "39.57 W\n",
      "40.54 W\n",
      "39.58 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.71 W\n",
      "49.18 W\n",
      "68.07 W\n",
      "74.07 W\n",
      "63.33 W\n",
      "39.37 W\n",
      "40.45 W\n",
      "39.57 W\n",
      "39.08 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.71 W\n",
      "48.98 W\n",
      "69.33 W\n",
      "74.74 W\n",
      "60.62 W\n",
      "39.67 W\n",
      "40.35 W\n",
      "39.37 W\n",
      "39.19 W\n",
      "torch.Size([32, 1000])\n",
      "Mean power:  50.33814814814814\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.adaptive_avg_pooling = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(self.maxpool(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Start the power measurement\n",
    "\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(self.maxpool(x)))\n",
    "        x = self.adaptive_avg_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "def inference():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"use_cuda : \", use_cuda)\n",
    "    print(torch.cuda.device_count())\n",
    "    #print(torch.cuda.get_device_name(1))\n",
    "    FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    model = AlexNet().to(device)\n",
    "    batch_size =32\n",
    "    X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device)\n",
    "    model.train()\n",
    "    power_measurements = []\n",
    "    for i in range(30):\n",
    "        # Start the power measurement\n",
    "        q = multiprocessing.Queue()\n",
    "        rq = multiprocessing.Queue()\n",
    "        q1 = multiprocessing.Queue()\n",
    "        rq1 = multiprocessing.Queue()\n",
    "        q2 = multiprocessing.Queue()\n",
    "        rq2 = multiprocessing.Queue()\n",
    "        q3 = multiprocessing.Queue()\n",
    "        rq3 = multiprocessing.Queue()\n",
    "        q4 = multiprocessing.Queue()\n",
    "        rq4 = multiprocessing.Queue() \n",
    "        rq5 = multiprocessing.Queue()\n",
    "        q5 = multiprocessing.Queue()\n",
    "        rq6 = multiprocessing.Queue()\n",
    "        q6 = multiprocessing.Queue()\n",
    "        rq6 = multiprocessing.Queue()\n",
    "        q7 = multiprocessing.Queue()\n",
    "        rq7 = multiprocessing.Queue() \n",
    "        q8 = multiprocessing.Queue()\n",
    "        rq8 = multiprocessing.Queue()\n",
    "\n",
    "        # Signal the start of the measurement\n",
    "\n",
    "        # Run the inference\n",
    "        x = X.clone()\n",
    "        p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "        p.start()\n",
    "        q.put('start')\n",
    "        m = rq.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'                  \n",
    "        x = model.conv1(x)\n",
    "        x = model.relu(x)\n",
    "        q.put('stop')\n",
    "\n",
    "        \n",
    "        p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "        p1.start()\n",
    "        q1.put('start')\n",
    "        m = rq1.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'               \n",
    "        #x = model.maxpool(x)\n",
    "        x = model.conv2(x)\n",
    "        x = model.relu(x) \n",
    "        q1.put('stop')\n",
    "        \n",
    "        \n",
    "        p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "        p2.start()\n",
    "        q2.put('start')\n",
    "        m = rq2.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'              \n",
    "        #x = model.maxpool(x)\n",
    "        x = model.conv3(x)\n",
    "        x = model.relu(x)\n",
    "        q2.put('stop')\n",
    "       \n",
    "        \n",
    "                \n",
    "          \n",
    "        p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "        p3.start()\n",
    "        q3.put('start')\n",
    "        m = rq3.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'          \n",
    "        x = model.conv4(x)\n",
    "        x = model.relu(x) \n",
    "        q3.put('stop')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "        p4.start()\n",
    "        q4.put('start')\n",
    "        m = rq4.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'          \n",
    "        x = model.conv5(x)\n",
    "        x = model.relu(x)\n",
    "        q4.put('stop')\n",
    "        \n",
    "        \n",
    "      \n",
    "        p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "        p5.start()\n",
    "        q5.put('start')\n",
    "        m = rq5.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.adaptive_avg_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = model.dropout(x)\n",
    "        q5.put('stop')\n",
    "        \n",
    "        #x = model.maxpool(x)\n",
    "        p6 = multiprocessing.Process(target=measure, args=(q6,rq6,1))\n",
    "        p6.start()\n",
    "        q6.put('start')\n",
    "        m = rq6.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.fc1(x)\n",
    "        x = model.dropout(model.relu(x)) \n",
    "        q6.put('stop')\n",
    "\n",
    "\n",
    "        p7 = multiprocessing.Process(target=measure, args=(q7,rq7,1))\n",
    "        p7.start()\n",
    "        q7.put('start')\n",
    "        m = rq7.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.fc2(x)\n",
    "        x = model.relu(x)\n",
    "        q7.put('stop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        p8 = multiprocessing.Process(target=measure, args=(q8,rq8,1))\n",
    "        p8.start()\n",
    "        q8.put('start')\n",
    "        m = rq8.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.fc3(x)\n",
    "        q8.put('stop')\n",
    "\n",
    "        # Signal the end of the power measurement\n",
    "        output = x\n",
    "\n",
    "        # Wait for the measurement to finish\n",
    "        p.join() \n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "        p5.join()\n",
    "        p6.join()\n",
    "        p7.join()\n",
    "        p8.join()\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq.empty():\n",
    "            power_output = rq.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq1.empty():\n",
    "            power_output = rq1.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq2.empty():\n",
    "            power_output = rq2.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq3.empty():\n",
    "            power_output = rq3.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq4.empty():\n",
    "            power_output = rq4.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq5.empty():\n",
    "            power_output = rq5.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov] \n",
    "    \n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq6.empty():\n",
    "            power_output = rq6.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq7.empty():\n",
    "            power_output = rq7.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq8.empty():\n",
    "            power_output = rq8.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov]\n",
    "            \n",
    "    print(output.shape) \n",
    "    mean_power = np.mean(power_measurements)\n",
    "    print(\"Mean power: \", mean_power)\n",
    "inference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f3db98",
   "metadata": {},
   "source": [
    " Now let's repeat each layer several times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f2aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "2\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.37 W\n",
      "42.88 W\n",
      "49.66 W\n",
      "50.93 W\n",
      "47.14 W\n",
      "39.18 W\n",
      "40.35 W\n",
      "41.61 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.49 W\n",
      "42.77 W\n",
      "48.50 W\n",
      "48.88 W\n",
      "46.37 W\n",
      "39.09 W\n",
      "39.86 W\n",
      "41.81 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.68 W\n",
      "48.69 W\n",
      "50.91 W\n",
      "46.56 W\n",
      "39.08 W\n",
      "39.67 W\n",
      "41.81 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.86 W\n",
      "48.69 W\n",
      "50.83 W\n",
      "45.20 W\n",
      "38.99 W\n",
      "39.87 W\n",
      "41.82 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.49 W\n",
      "42.78 W\n",
      "48.49 W\n",
      "49.86 W\n",
      "46.66 W\n",
      "38.99 W\n",
      "39.86 W\n",
      "41.32 W\n",
      "39.57 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.68 W\n",
      "42.96 W\n",
      "48.41 W\n",
      "51.22 W\n",
      "46.75 W\n",
      "38.99 W\n",
      "39.67 W\n",
      "41.62 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.39 W\n",
      "42.76 W\n",
      "48.51 W\n",
      "51.03 W\n",
      "46.66 W\n",
      "39.00 W\n",
      "40.36 W\n",
      "41.72 W\n",
      "39.39 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.49 W\n",
      "42.49 W\n",
      "48.01 W\n",
      "50.34 W\n",
      "45.31 W\n",
      "39.00 W\n",
      "40.26 W\n",
      "41.22 W\n",
      "39.38 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.88 W\n",
      "48.41 W\n",
      "52.00 W\n",
      "47.13 W\n",
      "39.09 W\n",
      "40.26 W\n",
      "41.61 W\n",
      "39.68 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.38 W\n",
      "42.97 W\n",
      "48.69 W\n",
      "50.83 W\n",
      "46.74 W\n",
      "38.99 W\n",
      "39.87 W\n",
      "41.72 W\n",
      "39.77 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.38 W\n",
      "42.98 W\n",
      "48.41 W\n",
      "50.34 W\n",
      "47.14 W\n",
      "38.99 W\n",
      "39.67 W\n",
      "41.32 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.37 W\n",
      "42.77 W\n",
      "48.59 W\n",
      "51.01 W\n",
      "47.04 W\n",
      "38.99 W\n",
      "39.96 W\n",
      "41.81 W\n",
      "39.57 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.58 W\n",
      "42.88 W\n",
      "48.29 W\n",
      "51.61 W\n",
      "46.36 W\n",
      "38.99 W\n",
      "40.26 W\n",
      "41.51 W\n",
      "39.57 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.47 W\n",
      "42.77 W\n",
      "47.43 W\n",
      "51.78 W\n",
      "46.56 W\n",
      "38.99 W\n",
      "39.87 W\n",
      "41.82 W\n",
      "39.38 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.58 W\n",
      "42.88 W\n",
      "48.69 W\n",
      "50.92 W\n",
      "47.04 W\n",
      "39.08 W\n",
      "39.75 W\n",
      "41.61 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.77 W\n",
      "42.98 W\n",
      "48.70 W\n",
      "51.01 W\n",
      "46.17 W\n",
      "39.00 W\n",
      "40.16 W\n",
      "41.72 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.38 W\n",
      "42.78 W\n",
      "49.18 W\n",
      "50.72 W\n",
      "46.65 W\n",
      "38.99 W\n",
      "39.66 W\n",
      "41.81 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.47 W\n",
      "42.77 W\n",
      "48.11 W\n",
      "51.31 W\n",
      "46.56 W\n",
      "39.19 W\n",
      "40.36 W\n",
      "41.71 W\n",
      "39.28 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.39 W\n",
      "42.68 W\n",
      "48.40 W\n",
      "51.41 W\n",
      "46.56 W\n",
      "38.90 W\n",
      "39.66 W\n",
      "41.42 W\n",
      "39.29 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.96 W\n",
      "48.69 W\n",
      "51.22 W\n",
      "47.23 W\n",
      "38.98 W\n",
      "39.86 W\n",
      "41.60 W\n",
      "39.67 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.58 W\n",
      "42.57 W\n",
      "48.50 W\n",
      "50.73 W\n",
      "46.65 W\n",
      "39.09 W\n",
      "39.76 W\n",
      "41.61 W\n",
      "39.86 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.97 W\n",
      "48.50 W\n",
      "51.51 W\n",
      "47.33 W\n",
      "39.09 W\n",
      "40.45 W\n",
      "43.85 W\n",
      "39.87 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.47 W\n",
      "42.86 W\n",
      "48.70 W\n",
      "51.69 W\n",
      "46.75 W\n",
      "39.09 W\n",
      "40.15 W\n",
      "41.61 W\n",
      "39.76 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.67 W\n",
      "43.06 W\n",
      "48.59 W\n",
      "50.81 W\n",
      "46.75 W\n",
      "39.18 W\n",
      "40.15 W\n",
      "41.99 W\n",
      "39.58 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.48 W\n",
      "42.96 W\n",
      "48.79 W\n",
      "51.20 W\n",
      "46.95 W\n",
      "38.99 W\n",
      "40.06 W\n",
      "41.60 W\n",
      "39.67 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.37 W\n",
      "42.68 W\n",
      "48.40 W\n",
      "51.69 W\n",
      "46.46 W\n",
      "38.89 W\n",
      "40.15 W\n",
      "41.61 W\n",
      "39.76 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.47 W\n",
      "42.77 W\n",
      "48.20 W\n",
      "50.81 W\n",
      "46.27 W\n",
      "39.09 W\n",
      "40.04 W\n",
      "41.71 W\n",
      "39.67 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.76 W\n",
      "42.77 W\n",
      "48.01 W\n",
      "50.25 W\n",
      "46.66 W\n",
      "38.99 W\n",
      "40.06 W\n",
      "41.42 W\n",
      "39.77 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.58 W\n",
      "42.39 W\n",
      "48.50 W\n",
      "49.47 W\n",
      "46.18 W\n",
      "39.00 W\n",
      "39.88 W\n",
      "41.32 W\n",
      "39.68 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.28 W\n",
      "42.39 W\n",
      "48.79 W\n",
      "51.80 W\n",
      "47.24 W\n",
      "39.19 W\n",
      "40.36 W\n",
      "41.81 W\n",
      "39.87 W\n",
      "torch.Size([8, 1000])\n",
      "conv1 power:  39.49133333333332\n",
      "conv2 power:  42.796666666666674\n",
      "conv3 power:  48.51766666666666\n",
      "conv4 power:  50.937333333333335\n",
      "conv5 power:  46.63566666666667\n",
      "avg drop power:  39.03666666666667\n",
      "fc1 power:  40.01\n",
      "fc2 power:  41.70399999999999\n",
      "fc3 power:  39.608666666666664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.adaptive_avg_pooling = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(self.maxpool(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        # Start the power measurement\n",
    "\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(self.maxpool(x)))\n",
    "        x = self.adaptive_avg_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(self.relu(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "#def inference():\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name(1))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = AlexNet().to(device)\n",
    "batch_size =8\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "power_measurements4 = []\n",
    "power_measurements5 = []\n",
    "power_measurements6 = []\n",
    "power_measurements7 = []\n",
    "power_measurements8 = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue()\n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue()\n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue() \n",
    "    rq5 = multiprocessing.Queue()\n",
    "    q5 = multiprocessing.Queue()\n",
    "    rq6 = multiprocessing.Queue()\n",
    "    q6 = multiprocessing.Queue()\n",
    "    rq6 = multiprocessing.Queue()\n",
    "    q7 = multiprocessing.Queue()\n",
    "    rq7 = multiprocessing.Queue() \n",
    "    q8 = multiprocessing.Queue()\n",
    "    rq8 = multiprocessing.Queue()\n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "    x = X.clone()\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(1):                \n",
    "        x1 = model.conv1(x)\n",
    "        x1 = model.relu(x1)\n",
    "    q.put('stop')\n",
    "    p.join() \n",
    "    \n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'               \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(1):                \n",
    "        x2 = model.conv2(x1)\n",
    "        x2 = model.relu(x2) \n",
    "    q1.put('stop')\n",
    "    p1.join()\n",
    "    \n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'              \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(1):                \n",
    "        x3 = model.conv3(x2)\n",
    "        x3 = model.relu(x3)\n",
    "    q2.put('stop')\n",
    "    p2.join()       \n",
    "    \n",
    "            \n",
    "        \n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(1):                         \n",
    "        x4 = model.conv4(x3)\n",
    "        x4 = model.relu(x4) \n",
    "    q3.put('stop')\n",
    "    p3.join()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'\n",
    "    for i in range(1):                          \n",
    "        x5 = model.conv5(x4)\n",
    "        x5 = model.relu(x5)\n",
    "    q4.put('stop')\n",
    "    p4.join()\n",
    "\n",
    "    \n",
    "    \n",
    "    p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "    p5.start()\n",
    "    q5.put('start')\n",
    "    m = rq5.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'\n",
    "    for i in range(1):                   \n",
    "        x6 = model.adaptive_avg_pooling(x5)\n",
    "        x6 = x6.view(x.size(0), -1)\n",
    "        x6 = model.dropout(x6)\n",
    "    q5.put('stop')\n",
    "    p5.join()\n",
    "\n",
    "\n",
    "\n",
    "    #x = model.maxpool(x)\n",
    "    p6 = multiprocessing.Process(target=measure, args=(q6,rq6,1))\n",
    "    p6.start()\n",
    "    q6.put('start')\n",
    "    m = rq6.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(1):                \n",
    "        x7 = model.fc1(x6)\n",
    "        x7 = model.dropout(model.relu(x7)) \n",
    "    q6.put('stop')\n",
    "    p6.join()\n",
    "\n",
    "\n",
    "    p7 = multiprocessing.Process(target=measure, args=(q7,rq7,1))\n",
    "    p7.start()\n",
    "    q7.put('start')\n",
    "    m = rq7.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'  \n",
    "    for i in range(5):                 \n",
    "        x8 = model.fc2(x7)\n",
    "        x8 = model.relu(x8)\n",
    "    q7.put('stop')\n",
    "    p7.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p8 = multiprocessing.Process(target=measure, args=(q8,rq8,1))\n",
    "    p8.start()\n",
    "    q8.put('start')\n",
    "    m = rq8.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'  \n",
    "    for i in range(5):                 \n",
    "        x9 = model.fc3(x8)\n",
    "    q8.put('stop')\n",
    "    p8.join()\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    output = x9\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq5.empty():\n",
    "        power_output = rq5.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements5.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq6.empty():\n",
    "        power_output = rq6.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements6.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq7.empty():\n",
    "        power_output = rq7.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements7.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq8.empty():\n",
    "        power_output = rq8.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements8.append(float(power_output.split()[0]))  # Remov]\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"conv1 power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"conv2 power: \", mean_power1) \n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"conv3 power: \", mean_power2)\n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"conv4 power: \", mean_power3) \n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"conv5 power: \", mean_power4) \n",
    "mean_power5 = np.mean(power_measurements5)\n",
    "print(\"avg drop power: \", mean_power5) \n",
    "mean_power6 = np.mean(power_measurements6)\n",
    "print(\"fc1 power: \", mean_power6)\n",
    "mean_power7 = np.mean(power_measurements7)\n",
    "print(\"fc2 power: \", mean_power7) \n",
    "mean_power8 = np.mean(power_measurements8)\n",
    "print(\"fc3 power: \", mean_power8)\n",
    "#inference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3fa217b",
   "metadata": {},
   "source": [
    "## Profiling Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "778c4d16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                  conv1         0.59%      89.000us         1.82%     276.000us     276.000us       6.000us         0.07%     309.000us     309.000us             1  \n",
      "                                           aten::conv2d         0.33%      50.000us         4.70%     714.000us     142.800us      19.000us         0.21%       7.750ms       1.550ms             5  \n",
      "                                      aten::convolution         0.51%      77.000us         4.37%     664.000us     132.800us      21.000us         0.23%       7.731ms       1.546ms             5  \n",
      "                                     aten::_convolution         1.05%     159.000us         3.86%     587.000us     117.400us      46.000us         0.50%       7.710ms       1.542ms             5  \n",
      "                                aten::cudnn_convolution         1.53%     232.000us         1.98%     301.000us      60.200us       7.033ms        76.82%       7.033ms       1.407ms             5  \n",
      "                                  cudaStreamIsCapturing         0.01%       1.000us         0.01%       1.000us       0.143us       0.000us         0.00%       0.000us       0.000us             7  \n",
      "                                  cudaStreamGetPriority         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             5  \n",
      "                                       cudaLaunchKernel         0.94%     143.000us         0.94%     143.000us       3.865us       0.000us         0.00%       0.000us       0.000us            37  \n",
      "                                        cudaMemsetAsync         0.18%      27.000us         0.18%      27.000us       9.000us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                          aten::reshape         0.39%      60.000us         0.43%      65.000us      13.000us      22.000us         0.24%      28.000us       5.600us             5  \n",
      "                                   aten::_reshape_alias         0.03%       5.000us         0.03%       5.000us       1.000us       6.000us         0.07%       6.000us       1.200us             5  \n",
      "                                             aten::add_         0.30%      45.000us         0.41%      62.000us      12.400us     603.000us         6.59%     603.000us     120.600us             5  \n",
      "                                            aten::relu_         0.80%     121.000us         1.26%     191.000us      27.286us      28.000us         0.31%     622.000us      88.857us             7  \n",
      "                                       aten::clamp_min_         0.31%      47.000us         0.46%      70.000us      10.000us     594.000us         6.49%     594.000us      84.857us             7  \n",
      "                                                 Conv_2         0.36%      55.000us         1.53%     232.000us     232.000us       6.000us         0.07%       1.233ms       1.233ms             1  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.05%       7.000us         0.05%       7.000us       0.250us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                                  conv3         0.36%      54.000us         1.43%     218.000us     218.000us       7.000us         0.08%       2.248ms       2.248ms             1  \n",
      "                                                  conv4         0.43%      66.000us         1.47%     224.000us     224.000us       6.000us         0.07%       2.662ms       2.662ms             1  \n",
      "                                                  conv5         0.35%      53.000us         1.38%     210.000us     210.000us       6.000us         0.07%       1.935ms       1.935ms             1  \n",
      "                                               avg_drop         0.59%      90.000us         1.53%     233.000us     233.000us       8.000us         0.09%     168.000us     168.000us             1  \n",
      "                              aten::adaptive_avg_pool2d         0.06%       9.000us         0.38%      57.000us      57.000us       4.000us         0.04%     131.000us     131.000us             1  \n",
      "                             aten::_adaptive_avg_pool2d         0.20%      31.000us         0.32%      48.000us      48.000us     125.000us         1.37%     127.000us     127.000us             1  \n",
      "                                            aten::empty         0.11%      16.000us         0.11%      16.000us       4.000us       4.000us         0.04%       4.000us       1.000us             4  \n",
      "                                          aten::resize_         0.04%       6.000us         0.04%       6.000us       6.000us       1.000us         0.01%       1.000us       1.000us             1  \n",
      "                                             aten::view         0.04%       6.000us         0.04%       6.000us       6.000us       2.000us         0.02%       2.000us       2.000us             1  \n",
      "                                          aten::dropout         0.15%      23.000us         1.01%     153.000us      76.500us       8.000us         0.09%      52.000us      26.000us             2  \n",
      "                                   aten::native_dropout         0.44%      67.000us         0.86%     130.000us      65.000us      24.000us         0.26%      44.000us      22.000us             2  \n",
      "                                       aten::empty_like         0.24%      37.000us         0.35%      53.000us      13.250us      16.000us         0.17%      20.000us       5.000us             4  \n",
      "                                    aten::empty_strided         0.11%      16.000us         0.11%      16.000us       4.000us       4.000us         0.04%       4.000us       1.000us             4  \n",
      "                                                    fc1         0.50%      76.000us         1.97%     299.000us     299.000us       8.000us         0.09%     353.000us     353.000us             1  \n",
      "                                           aten::linear         0.38%      58.000us         2.28%     347.000us     115.667us      20.000us         0.22%     539.000us     179.667us             3  \n",
      "                                                aten::t         0.26%      40.000us         0.51%      77.000us      25.667us      12.000us         0.13%      27.000us       9.000us             3  \n",
      "                                        aten::transpose         0.22%      34.000us         0.24%      37.000us      12.333us      12.000us         0.13%      15.000us       5.000us             3  \n",
      "                                       aten::as_strided         0.02%       3.000us         0.02%       3.000us       1.000us       3.000us         0.03%       3.000us       1.000us             3  \n",
      "                                            aten::addmm         0.97%     148.000us         1.40%     212.000us      70.667us     489.000us         5.34%     492.000us     164.000us             3  \n",
      "                                                    fc2         0.45%      69.000us         1.44%     219.000us     219.000us       7.000us         0.08%     174.000us     174.000us             1  \n",
      "                                                    fc3         0.23%      35.000us         0.95%     144.000us     144.000us       5.000us         0.05%      73.000us      73.000us             1  \n",
      "                                  cudaDeviceSynchronize        86.48%      13.142ms        86.48%      13.142ms      13.142ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.197ms\n",
      "Self CUDA time total: 9.155ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-07 06:53:28 48906:48906 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time as timer_l\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from numba import cuda\n",
    "import re\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.autograd.profiler.record_function(\"conv1\"):\n",
    "            x = self.relu(self.conv1(x))\n",
    "        with torch.autograd.profiler.record_function(\"Conv_2\"):\n",
    "            x = self.relu(self.conv2(x))\n",
    "        with torch.autograd.profiler.record_function(\"conv3\"):\n",
    "            x = self.relu(self.conv3(x))\n",
    "        with torch.autograd.profiler.record_function(\"conv4\"):\n",
    "            x = self.relu(self.conv4(x))\n",
    "        with torch.autograd.profiler.record_function(\"conv5\"):\n",
    "            x = self.relu(self.conv5(x))\n",
    "        with torch.autograd.profiler.record_function(\"avg_drop\"):\n",
    "            x = self.adaptive_avg_polling(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "        with torch.autograd.profiler.record_function(\"fc1\"):\n",
    "            x = self.fc1(x)\n",
    "            x = self.dropout(self.relu(x))\n",
    "        with torch.autograd.profiler.record_function(\"fc2\"):\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu(x)\n",
    "        with torch.autograd.profiler.record_function(\"fc3\"):\n",
    "            x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = AlexNet().to(device)    \n",
    "batch_size = 16\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device) \n",
    "\n",
    "# Run the model and record the inference time of each layer\n",
    "for i in range(10):\n",
    "\n",
    "    model.forward(X)\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True) \n",
    "    with profiler:\n",
    "        #cuda.synchronize()\n",
    "        output = model.forward(X)\n",
    "        #cuda.synchronize()# Get the table of profiling results\n",
    "profiling_results0 = profiler.key_averages().table()\n",
    "print(profiling_results0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b68d8c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.045 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Find the index of \"Self CUDA time total\"\n",
    "start_index = profiling_results0.find(\"Self CUDA time total: \")\n",
    "if start_index != -1:\n",
    "    # Extract the substring starting from the index of the value\n",
    "    start_index += len(\"Self CUDA time total: \")\n",
    "    end_index = profiling_results0.find(\"ms\", start_index)\n",
    "    if end_index != -1:\n",
    "        # Extract the value as a float\n",
    "        self_cuda_time_total = float(profiling_results0[start_index:end_index].strip())\n",
    "        print( self_cuda_time_total, \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1f479a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA time avg for conv1: 0.589 ms\n",
      "CUDA time avg for Conv_2: 2.292 ms\n",
      "CUDA time avg for conv3: 4.658 ms\n",
      "CUDA time avg for conv4: 5.362 ms\n",
      "CUDA time avg for conv5: 3.939 ms\n",
      "CUDA time avg for avg_drop: 0.273 ms\n",
      "CUDA time avg for fc1: 0.361 ms\n",
      "CUDA time avg for fc2: 0.17 ms\n",
      "CUDA time avg for fc3: 0.113 ms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results0.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['conv1', 'Conv_2', 'conv3', 'conv4', 'conv5', 'avg_drop', 'fc1', 'fc2', 'fc3']\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers and convert to ms\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Print the 'CUDA time avg' values in ms\n",
    "for layer_name, cuda_time_avg in cuda_time_avgs.items():\n",
    "    print(f\"CUDA time avg for {layer_name}: {cuda_time_avg} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "415ea0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_alexnet.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>CUDA Time Avg (ms)</th>\n",
       "      <th>Power Measurements(W)</th>\n",
       "      <th>Power Kernel (W)</th>\n",
       "      <th>Energy(mJ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conv1</td>\n",
       "      <td>2.089</td>\n",
       "      <td>126.460667</td>\n",
       "      <td>88.460667</td>\n",
       "      <td>184.794333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conv_2</td>\n",
       "      <td>8.848</td>\n",
       "      <td>143.769000</td>\n",
       "      <td>105.769000</td>\n",
       "      <td>935.844112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conv3</td>\n",
       "      <td>18.964</td>\n",
       "      <td>191.467667</td>\n",
       "      <td>153.467667</td>\n",
       "      <td>2910.360831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conv4</td>\n",
       "      <td>21.870</td>\n",
       "      <td>187.165333</td>\n",
       "      <td>149.165333</td>\n",
       "      <td>3262.245840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conv5</td>\n",
       "      <td>15.881</td>\n",
       "      <td>184.538667</td>\n",
       "      <td>146.538667</td>\n",
       "      <td>2327.180565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_drop</td>\n",
       "      <td>0.928</td>\n",
       "      <td>65.527667</td>\n",
       "      <td>27.527667</td>\n",
       "      <td>25.545675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fc1</td>\n",
       "      <td>0.871</td>\n",
       "      <td>76.480333</td>\n",
       "      <td>38.480333</td>\n",
       "      <td>33.516370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fc2</td>\n",
       "      <td>0.423</td>\n",
       "      <td>58.013667</td>\n",
       "      <td>20.013667</td>\n",
       "      <td>8.465781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fc3</td>\n",
       "      <td>0.171</td>\n",
       "      <td>50.399000</td>\n",
       "      <td>12.399000</td>\n",
       "      <td>2.120229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Layer Name  CUDA Time Avg (ms)  Power Measurements(W)  Power Kernel (W)  \\\n",
       "0      conv1               2.089             126.460667         88.460667   \n",
       "1     Conv_2               8.848             143.769000        105.769000   \n",
       "2      conv3              18.964             191.467667        153.467667   \n",
       "3      conv4              21.870             187.165333        149.165333   \n",
       "4      conv5              15.881             184.538667        146.538667   \n",
       "5   avg_drop               0.928              65.527667         27.527667   \n",
       "6        fc1               0.871              76.480333         38.480333   \n",
       "7        fc2               0.423              58.013667         20.013667   \n",
       "8        fc3               0.171              50.399000         12.399000   \n",
       "\n",
       "    Energy(mJ)  \n",
       "0   184.794333  \n",
       "1   935.844112  \n",
       "2  2910.360831  \n",
       "3  3262.245840  \n",
       "4  2327.180565  \n",
       "5    25.545675  \n",
       "6    33.516370  \n",
       "7     8.465781  \n",
       "8     2.120229  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results0.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['conv1', 'Conv_2', 'conv3', 'conv4', 'conv5', 'avg_drop', 'fc1', 'fc2', 'fc3']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4, mean_power5, mean_power6, mean_power7, mean_power8]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "power_idle = 38 #depends on the gpu\n",
    "power_measurements_kernel = [power_measurement - power_idle for power_measurement in power_measurements]\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements_kernel[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file = 'profiler_results_alexnet.csv'\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)','Power Kernel (W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i],power_measurements_kernel[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the table\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6539ca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CUDA Time Avg (ms)'].sum() == self_cuda_time_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e2014e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum of Energy</th>\n",
       "      <th>Self CUDA Time * X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9690.073736</td>\n",
       "      <td>7919.637925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sum of Energy  Self CUDA Time * X\n",
       "0    9690.073736         7919.637925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =average_above_90th - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32c18d07",
   "metadata": {},
   "source": [
    "## Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52e902c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df['Layer Name'], df['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f95fc5",
   "metadata": {},
   "source": [
    "# Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf54df1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nuse_cuda = torch.cuda.is_available()\\nprint(\"use_cuda : \", use_cuda)\\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\\ndevice= torch.device(\"cuda:1\" if use_cuda else \"cpu\")\\nmodel = ResNet50(img_channels=3, num_classes=1000).to(device)    \\nbatch_size = 1\\nX = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device) \\noutput = model.forward(X)\\n# Print the output tensor shape\\nprint(output.shape)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], \"should be a a valid architecture\"\n",
    "        super(Block, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        if self.num_layers > 34:\n",
    "            self.expansion = 4\n",
    "        else:\n",
    "            self.expansion = 1\n",
    "        # ResNet50, 101, and 152 include additional layer of 1x1 kernels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        if self.num_layers > 34:\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        else:\n",
    "            # for ResNet18 and 34, connect input directly to (3x3) kernel (skip first (1x1))\n",
    "            self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.num_layers > 34:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
    "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers < 50:\n",
    "            self.expansion = 1\n",
    "        else:\n",
    "            self.expansion = 4\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "        elif num_layers == 101:\n",
    "            layers = [3, 4, 23, 3]\n",
    "        else:\n",
    "            layers = [3, 8, 36, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def ResNet18(img_channels=3, num_classes=1000):\n",
    "    return ResNet(18, Block, img_channels, num_classes)\n",
    "\n",
    "\n",
    "def ResNet34(img_channels=3, num_classes=1000):\n",
    "    return ResNet(34, Block, img_channels, num_classes)\n",
    "\n",
    "\n",
    "def ResNet50(img_channels=3, num_classes=1000):\n",
    "    return ResNet(50, Block, img_channels, num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(img_channels=3, num_classes=1000):\n",
    "    return ResNet(101, Block, img_channels, num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(img_channels=3, num_classes=1000):\n",
    "    return ResNet(152, Block, img_channels, num_classes)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18(img_channels=3, num_classes=1000)\n",
    "    y = net(torch.randn(4, 3, 224, 224)).to(\"cuda\")\n",
    "    print(y.size())\n",
    "\n",
    "'''\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device= torch.device(\"cuda:1\" if use_cuda else \"cpu\")\n",
    "model = ResNet50(img_channels=3, num_classes=1000).to(device)    \n",
    "batch_size = 1\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor).to(device) \n",
    "output = model.forward(X)\n",
    "# Print the output tensor shape\n",
    "print(output.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f666e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "45.39 W\n",
      "gotstarted\n",
      "129.06 W\n",
      "gotstarted\n",
      "123.41 W\n",
      "gotstarted\n",
      "132.33 W\n",
      "gotstarted\n",
      "128.19 W\n",
      "gotstarted\n",
      "128.05 W\n",
      "gotstarted\n",
      "123.70 W\n",
      "gotstarted\n",
      "130.81 W\n",
      "gotstarted\n",
      "129.60 W\n",
      "gotstarted\n",
      "124.14 W\n",
      "gotstarted\n",
      "123.93 W\n",
      "gotstarted\n",
      "124.67 W\n",
      "gotstarted\n",
      "126.46 W\n",
      "gotstarted\n",
      "136.15 W\n",
      "gotstarted\n",
      "123.62 W\n",
      "gotstarted\n",
      "134.33 W\n",
      "gotstarted\n",
      "127.88 W\n",
      "gotstarted\n",
      "127.24 W\n",
      "gotstarted\n",
      "124.86 W\n",
      "gotstarted\n",
      "127.66 W\n",
      "gotstarted\n",
      "123.42 W\n",
      "gotstarted\n",
      "127.80 W\n",
      "gotstarted\n",
      "128.58 W\n",
      "gotstarted\n",
      "129.16 W\n",
      "gotstarted\n",
      "144.92 W\n",
      "gotstarted\n",
      "128.65 W\n",
      "gotstarted\n",
      "124.71 W\n",
      "gotstarted\n",
      "125.29 W\n",
      "gotstarted\n",
      "137.40 W\n",
      "gotstarted\n",
      "128.81 W\n",
      "torch.Size([64, 1000])\n",
      "Mean power:  125.67399999999999\n",
      "Average of values >= __th percentile: 141.16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
    "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers < 50:\n",
    "            self.expansion = 1\n",
    "        else:\n",
    "            self.expansion = 4\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "        elif num_layers == 101:\n",
    "            layers = [3, 4, 23, 3]\n",
    "        else:\n",
    "            layers = [3, 8, 36, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def ResNet50(img_channels=3, num_classes=1000):\n",
    "        return ResNet(50, Block, img_channels, num_classes)\n",
    "    \n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = ResNet50(img_channels=3, num_classes=1000).to(device)\n",
    "batch_size = 64\n",
    "X= torch.randn(size=(batch_size, 3, 224, 224)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    \n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference########################\n",
    "    #\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'         \n",
    "    #output = model.forward(X)   \n",
    "\n",
    "    x = X.clone()\n",
    "        \n",
    "    x = model.conv1(x) \n",
    "\n",
    "    x = model.bn1(x)\n",
    "\n",
    "    x = model.relu(x)\n",
    "\n",
    "    x = model.maxpool(x)\n",
    "\n",
    "    x = model.layer1(x)\n",
    "\n",
    "    x = model.layer2(x)\n",
    "\n",
    "    x = model.layer3(x)\n",
    "    \n",
    "    x = model.layer4(x)\n",
    "\n",
    "    x = model.avgpool(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "    x = model.fc(x)\n",
    "    \n",
    "    output = x\n",
    "    q.put('stop')\n",
    "\n",
    "\n",
    "    #################\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "    p.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power)\n",
    "percentile_90 = np.percentile(power_measurements, 95)\n",
    "average_above_90th = np.mean(np.array(power_measurements)[np.array(power_measurements) >= percentile_90])\n",
    "print('Average of values >= __th percentile:', average_above_90th) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfef156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f8252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "41.51 W\n",
      "39.17 W\n",
      "39.28 W\n",
      "39.19 W\n",
      "52.96 W\n",
      "50.91 W\n",
      "57.84 W\n",
      "48.38 W\n",
      "38.89 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.05 W\n",
      "39.09 W\n",
      "39.28 W\n",
      "39.19 W\n",
      "51.21 W\n",
      "55.79 W\n",
      "58.68 W\n",
      "46.27 W\n",
      "38.99 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.27 W\n",
      "39.19 W\n",
      "39.57 W\n",
      "49.37 W\n",
      "53.65 W\n",
      "57.52 W\n",
      "47.72 W\n",
      "38.99 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.44 W\n",
      "39.28 W\n",
      "39.09 W\n",
      "39.37 W\n",
      "52.00 W\n",
      "55.68 W\n",
      "54.89 W\n",
      "50.14 W\n",
      "39.00 W\n",
      "38.98 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.37 W\n",
      "39.28 W\n",
      "39.37 W\n",
      "53.35 W\n",
      "54.03 W\n",
      "58.89 W\n",
      "48.30 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.65 W\n",
      "39.37 W\n",
      "39.28 W\n",
      "39.28 W\n",
      "50.45 W\n",
      "54.42 W\n",
      "58.41 W\n",
      "46.26 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.97 W\n",
      "39.18 W\n",
      "39.28 W\n",
      "39.19 W\n",
      "50.52 W\n",
      "53.89 W\n",
      "56.74 W\n",
      "48.40 W\n",
      "38.89 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.16 W\n",
      "39.18 W\n",
      "39.19 W\n",
      "39.28 W\n",
      "51.32 W\n",
      "53.45 W\n",
      "58.99 W\n",
      "48.51 W\n",
      "39.00 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.26 W\n",
      "39.18 W\n",
      "39.19 W\n",
      "39.38 W\n",
      "51.41 W\n",
      "55.38 W\n",
      "57.11 W\n",
      "49.56 W\n",
      "38.99 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.15 W\n",
      "39.28 W\n",
      "39.16 W\n",
      "39.18 W\n",
      "52.17 W\n",
      "55.58 W\n",
      "56.62 W\n",
      "48.86 W\n",
      "38.90 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.44 W\n",
      "39.29 W\n",
      "39.19 W\n",
      "39.18 W\n",
      "51.98 W\n",
      "56.18 W\n",
      "57.60 W\n",
      "48.31 W\n",
      "38.89 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.25 W\n",
      "39.28 W\n",
      "39.28 W\n",
      "39.37 W\n",
      "53.45 W\n",
      "53.62 W\n",
      "58.70 W\n",
      "48.30 W\n",
      "38.99 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.54 W\n",
      "39.37 W\n",
      "39.09 W\n",
      "39.25 W\n",
      "50.99 W\n",
      "53.74 W\n",
      "55.46 W\n",
      "48.10 W\n",
      "38.99 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.85 W\n",
      "39.28 W\n",
      "39.18 W\n",
      "39.37 W\n",
      "49.76 W\n",
      "53.43 W\n",
      "56.17 W\n",
      "47.82 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.06 W\n",
      "39.47 W\n",
      "39.18 W\n",
      "39.28 W\n",
      "51.89 W\n",
      "50.52 W\n",
      "57.49 W\n",
      "48.60 W\n",
      "38.98 W\n",
      "38.88 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.83 W\n",
      "39.18 W\n",
      "39.08 W\n",
      "39.37 W\n",
      "50.84 W\n",
      "52.28 W\n",
      "54.49 W\n",
      "46.93 W\n",
      "38.78 W\n",
      "38.88 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.37 W\n",
      "39.08 W\n",
      "39.38 W\n",
      "53.45 W\n",
      "53.92 W\n",
      "57.32 W\n",
      "48.79 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.35 W\n",
      "39.08 W\n",
      "39.08 W\n",
      "39.19 W\n",
      "50.43 W\n",
      "54.39 W\n",
      "58.21 W\n",
      "48.79 W\n",
      "38.98 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.33 W\n",
      "39.28 W\n",
      "39.28 W\n",
      "39.08 W\n",
      "53.16 W\n",
      "53.43 W\n",
      "57.70 W\n",
      "48.29 W\n",
      "38.99 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.64 W\n",
      "39.18 W\n",
      "39.08 W\n",
      "39.18 W\n",
      "50.71 W\n",
      "55.58 W\n",
      "55.28 W\n",
      "48.09 W\n",
      "38.79 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.38 W\n",
      "39.19 W\n",
      "39.38 W\n",
      "51.30 W\n",
      "54.14 W\n",
      "57.70 W\n",
      "46.95 W\n",
      "38.79 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.54 W\n",
      "39.19 W\n",
      "39.18 W\n",
      "39.28 W\n",
      "53.83 W\n",
      "51.40 W\n",
      "55.11 W\n",
      "47.23 W\n",
      "38.89 W\n",
      "38.99 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.15 W\n",
      "39.28 W\n",
      "39.28 W\n",
      "39.28 W\n",
      "52.85 W\n",
      "52.55 W\n",
      "59.93 W\n",
      "48.11 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.25 W\n",
      "39.38 W\n",
      "39.19 W\n",
      "39.27 W\n",
      "52.48 W\n",
      "53.25 W\n",
      "56.09 W\n",
      "48.50 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.28 W\n",
      "39.19 W\n",
      "39.28 W\n",
      "51.70 W\n",
      "54.81 W\n",
      "58.93 W\n",
      "48.41 W\n",
      "38.89 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.95 W\n",
      "39.29 W\n",
      "39.18 W\n",
      "39.18 W\n",
      "49.76 W\n",
      "55.26 W\n",
      "56.39 W\n",
      "48.30 W\n",
      "38.89 W\n",
      "38.90 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.54 W\n",
      "39.18 W\n",
      "39.28 W\n",
      "39.37 W\n",
      "51.51 W\n",
      "54.02 W\n",
      "58.62 W\n",
      "48.79 W\n",
      "38.79 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.96 W\n",
      "39.08 W\n",
      "39.09 W\n",
      "39.18 W\n",
      "51.69 W\n",
      "51.69 W\n",
      "57.32 W\n",
      "47.81 W\n",
      "38.99 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "40.16 W\n",
      "39.18 W\n",
      "39.18 W\n",
      "39.28 W\n",
      "52.37 W\n",
      "54.13 W\n",
      "58.20 W\n",
      "47.63 W\n",
      "38.90 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.87 W\n",
      "39.37 W\n",
      "39.19 W\n",
      "39.19 W\n",
      "48.51 W\n",
      "52.96 W\n",
      "58.01 W\n",
      "48.59 W\n",
      "38.90 W\n",
      "38.90 W\n",
      "torch.Size([8, 1000])\n",
      "Mean power:  44.66866666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
    "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers < 50:\n",
    "            self.expansion = 1\n",
    "        else:\n",
    "            self.expansion = 4\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "        elif num_layers == 101:\n",
    "            layers = [3, 4, 23, 3]\n",
    "        else:\n",
    "            layers = [3, 8, 36, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def ResNet50(img_channels=3, num_classes=1000):\n",
    "        return ResNet(50, Block, img_channels, num_classes)\n",
    "    \n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "def inference():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"use_cuda : \", use_cuda)\n",
    "    FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    model = ResNet50(img_channels=3, num_classes=1000).to(device)\n",
    "    batch_size = 64\n",
    "    X= torch.randn(size=(batch_size, 3, 224, 224)).type(FloatTensor).to(device)\n",
    "    model.train()\n",
    "    power_measurements = [] \n",
    "    for i in range(30):\n",
    "        # Start the power measurement\n",
    "        q = multiprocessing.Queue()\n",
    "        rq = multiprocessing.Queue()\n",
    "        q1 = multiprocessing.Queue()\n",
    "        rq1 = multiprocessing.Queue()\n",
    "        q2 = multiprocessing.Queue()\n",
    "        rq2 = multiprocessing.Queue()\n",
    "        q3 = multiprocessing.Queue()\n",
    "        rq3 = multiprocessing.Queue()\n",
    "        q4 = multiprocessing.Queue()\n",
    "        rq4 = multiprocessing.Queue() \n",
    "        rq5 = multiprocessing.Queue()\n",
    "        q5 = multiprocessing.Queue()\n",
    "        rq6 = multiprocessing.Queue()\n",
    "        q6 = multiprocessing.Queue()\n",
    "        rq6 = multiprocessing.Queue()\n",
    "        q7 = multiprocessing.Queue()\n",
    "        rq7 = multiprocessing.Queue() \n",
    "        q8 = multiprocessing.Queue()\n",
    "        rq8 = multiprocessing.Queue() \n",
    "        q9 = multiprocessing.Queue()\n",
    "        rq9 = multiprocessing.Queue()\n",
    "\n",
    "        # Signal the start of the measurement\n",
    "\n",
    "        # Run the inference\n",
    "        p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "        p.start()\n",
    "        q.put('start')\n",
    "        m = rq.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'         \n",
    "        x = X.clone()\n",
    "        x = model.conv1(x) \n",
    "        q.put('stop')\n",
    "\n",
    "        \n",
    "        p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "        p1.start()\n",
    "        q1.put('start')\n",
    "        m = rq1.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'               \n",
    "        #x = model.maxpool(x)\n",
    "        x = model.bn1(x)\n",
    "        q1.put('stop')\n",
    "        \n",
    "        \n",
    "        p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "        p2.start()\n",
    "        q2.put('start')\n",
    "        m = rq2.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'              \n",
    "        #x = model.maxpool(x)\n",
    "        x = model.relu(x)\n",
    "        q2.put('stop')\n",
    "       \n",
    "        \n",
    "                \n",
    "          \n",
    "        p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "        p3.start()\n",
    "        q3.put('start')\n",
    "        m = rq3.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'          \n",
    "        x = model.maxpool(x)\n",
    "        q3.put('stop')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "        p4.start()\n",
    "        q4.put('start')\n",
    "        m = rq4.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'          \n",
    "        x = model.layer1(x)\n",
    "        q4.put('stop')\n",
    "        \n",
    "        \n",
    "      \n",
    "        p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "        p5.start()\n",
    "        q5.put('start')\n",
    "        m = rq5.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.layer2(x)\n",
    "        q5.put('stop')\n",
    "        \n",
    "        #x = model.maxpool(x)\n",
    "        p6 = multiprocessing.Process(target=measure, args=(q6,rq6,1))\n",
    "        p6.start()\n",
    "        q6.put('start')\n",
    "        m = rq6.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.layer3(x)\n",
    "        q6.put('stop')\n",
    "\n",
    "\n",
    "        p7 = multiprocessing.Process(target=measure, args=(q7,rq7,1))\n",
    "        p7.start()\n",
    "        q7.put('start')\n",
    "        m = rq7.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.layer4(x)\n",
    "        q7.put('stop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        p8 = multiprocessing.Process(target=measure, args=(q8,rq8,1))\n",
    "        p8.start()\n",
    "        q8.put('start')\n",
    "        m = rq8.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        q8.put('stop')\n",
    "\n",
    "        # Signal the end of the power measurement\n",
    "        #output = model.forward(X)   \n",
    "\n",
    "\n",
    "        p9 = multiprocessing.Process(target=measure, args=(q9,rq9,1))\n",
    "        p9.start()\n",
    "        q9.put('start')\n",
    "        m = rq9.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started'   \n",
    "        x = model.fc(x)\n",
    "        q9.put('stop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        output = x\n",
    "\n",
    "       \n",
    "        #################\n",
    "\n",
    "        # Wait for the measurement to finish\n",
    "        p.join() \n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "        p5.join()\n",
    "        p6.join()\n",
    "        p7.join()\n",
    "        p8.join() \n",
    "        p9.join()\n",
    "\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq.empty():\n",
    "            power_output = rq.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq1.empty():\n",
    "            power_output = rq1.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq2.empty():\n",
    "            power_output = rq2.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq3.empty():\n",
    "            power_output = rq3.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq4.empty():\n",
    "            power_output = rq4.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq5.empty():\n",
    "            power_output = rq5.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov] \n",
    "    \n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq6.empty():\n",
    "            power_output = rq6.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq7.empty():\n",
    "            power_output = rq7.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq8.empty():\n",
    "            power_output = rq8.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "        while not rq9.empty():\n",
    "            power_output = rq9.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remov]\n",
    "\n",
    "\n",
    "    print(output.shape) \n",
    "    mean_power = np.mean(power_measurements)\n",
    "    print(\"Mean power: \", mean_power)\n",
    "inference()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f78fe854",
   "metadata": {},
   "source": [
    "## process for Each layer _ Power_ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52781e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 31.74 GiB total capacity; 8.80 GiB already allocated; 137.12 MiB free; 10.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/daouda/workspace_test/Experiments_F.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39massert\u001b[39;00m m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstarted\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m     x5 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlayer1(x4)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m q4\u001b[39m.\u001b[39mput(\u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m p4\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/daouda/workspace_test/Experiments_F.ipynb Cell 25\u001b[0m in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentity_downsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49midentity_downsample(identity)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_F.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 31.74 GiB total capacity; 8.80 GiB already allocated; 137.12 MiB free; 10.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
    "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers < 50:\n",
    "            self.expansion = 1\n",
    "        else:\n",
    "            self.expansion = 4\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "        elif num_layers == 101:\n",
    "            layers = [3, 4, 23, 3]\n",
    "        else:\n",
    "            layers = [3, 8, 36, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def ResNet50(img_channels=3, num_classes=1000):\n",
    "        return ResNet(50, Block, img_channels, num_classes)\n",
    "    \n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = ResNet50(img_channels=3, num_classes=1000).to(device)\n",
    "batch_size = 64\n",
    "X= torch.randn(size=(batch_size, 3, 224, 224)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "power_measurements4 = []\n",
    "power_measurements5 = []\n",
    "power_measurements6 = []\n",
    "power_measurements7 = []\n",
    "power_measurements8 = [] \n",
    "power_measurements9 = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue()\n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue()\n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue() \n",
    "    rq5 = multiprocessing.Queue()\n",
    "    q5 = multiprocessing.Queue()\n",
    "    rq6 = multiprocessing.Queue()\n",
    "    q6 = multiprocessing.Queue()\n",
    "    rq6 = multiprocessing.Queue()\n",
    "    q7 = multiprocessing.Queue()\n",
    "    rq7 = multiprocessing.Queue() \n",
    "    q8 = multiprocessing.Queue()\n",
    "    rq8 = multiprocessing.Queue() \n",
    "    q9 = multiprocessing.Queue()\n",
    "    rq9 = multiprocessing.Queue()\n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'         \n",
    "    x = X.clone()\n",
    "    for i in range(10):\n",
    "        x1 = model.conv1(x) \n",
    "    q.put('stop')\n",
    "    p.join() \n",
    "\n",
    "    \n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'               \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(10):\n",
    "        x2 = model.bn1(x1)\n",
    "    q1.put('stop')\n",
    "    p1.join()\n",
    "    \n",
    "    \n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'              \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(10):\n",
    "        x3 = model.relu(x2)\n",
    "    q2.put('stop')\n",
    "    p2.join()\n",
    "\n",
    "    \n",
    "            \n",
    "        \n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        x4 = model.maxpool(x3)\n",
    "    q3.put('stop')\n",
    "    p3.join()\n",
    "\n",
    "\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        x5 = model.layer1(x4)\n",
    "    q4.put('stop')\n",
    "    p4.join()\n",
    "\n",
    "    \n",
    "    \n",
    "    p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "    p5.start()\n",
    "    q5.put('start')\n",
    "    m = rq5.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'  \n",
    "    for i in range(10):\n",
    "        x6 = model.layer2(x5)\n",
    "    q5.put('stop')\n",
    "    p5.join()\n",
    "\n",
    "\n",
    "    #x = model.maxpool(x)\n",
    "    p6 = multiprocessing.Process(target=measure, args=(q6,rq6,1))\n",
    "    p6.start()\n",
    "    q6.put('start')\n",
    "    m = rq6.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x7 = model.layer3(x6)\n",
    "    q6.put('stop')\n",
    "    p6.join()\n",
    "\n",
    "\n",
    "    p7 = multiprocessing.Process(target=measure, args=(q7,rq7,1))\n",
    "    p7.start()\n",
    "    q7.put('start')\n",
    "    m = rq7.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x8 = model.layer4(x7)\n",
    "    q7.put('stop')\n",
    "    p7.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p8 = multiprocessing.Process(target=measure, args=(q8,rq8,1))\n",
    "    p8.start()\n",
    "    q8.put('start')\n",
    "    m = rq8.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        x9 = model.avgpool(x8)\n",
    "        x9 = x9.reshape(x9.shape[0], -1)\n",
    "    q8.put('stop')\n",
    "    p8.join() \n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    #output = model.forward(X)   \n",
    "\n",
    "\n",
    "    p9 = multiprocessing.Process(target=measure, args=(q9,rq9,1))\n",
    "    p9.start()\n",
    "    q9.put('start')\n",
    "    m = rq9.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'\n",
    "    for i in range(10):\n",
    "        x10 = model.fc(x9)\n",
    "    q9.put('stop')\n",
    "    p9.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output = x10\n",
    "\n",
    "    \n",
    "    #################\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq5.empty():\n",
    "        power_output = rq5.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements5.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq6.empty():\n",
    "        power_output = rq6.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements6.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq7.empty():\n",
    "        power_output = rq7.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements7.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq8.empty():\n",
    "        power_output = rq8.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements8.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    while not rq9.empty():\n",
    "        power_output = rq9.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements9.append(float(power_output.split()[0]))  # Remov]\n",
    "\n",
    "\n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"conv1 power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"BatchNorm2d power: \", mean_power1) \n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"relu power: \", mean_power2)\n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"maxpool power: \", mean_power3) \n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"ResNetBlock1 power: \", mean_power4) \n",
    "mean_power5 = np.mean(power_measurements5)\n",
    "print(\"ResNetBlock2 power: \", mean_power5) \n",
    "mean_power6 = np.mean(power_measurements6)\n",
    "print(\"ResNetBlock3 power: \", mean_power6)\n",
    "mean_power7 = np.mean(power_measurements7)\n",
    "print(\"ResNetBlock4 power: \", mean_power7) \n",
    "mean_power8 = np.mean(power_measurements8)\n",
    "print(\"avgpool power: \", mean_power8) \n",
    "mean_power9 = np.mean(power_measurements9)\n",
    "print(\"fc power: \", mean_power9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59410277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-21 10:08:46 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:47 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:48 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:49 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:49 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 10:08:49 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 10:08:49 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 10:08:49 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                  conv1         0.07%      93.000us         0.19%     244.000us     244.000us       4.000us         0.01%       1.807ms       1.807ms             1  \n",
      "                                           aten::conv2d         1.81%       2.338ms        21.88%      28.310ms     534.151us     212.000us         0.29%      45.060ms     850.189us            53  \n",
      "                                      aten::convolution         3.51%       4.547ms        20.07%      25.972ms     490.038us     216.000us         0.30%      44.848ms     846.189us            53  \n",
      "                                     aten::_convolution         9.54%      12.352ms        16.56%      21.425ms     404.245us     489.000us         0.67%      44.632ms     842.113us            53  \n",
      "                                aten::cudnn_convolution         1.79%       2.310ms         4.07%       5.261ms      99.264us      36.428ms        49.83%      36.428ms     687.321us            53  \n",
      "                                  cudaStreamIsCapturing         0.01%       9.000us         0.01%       9.000us       0.085us       0.000us         0.00%       0.000us       0.000us           106  \n",
      "                                  cudaStreamGetPriority         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           106  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           106  \n",
      "                                       cudaLaunchKernel         7.62%       9.857ms         7.62%       9.857ms      28.244us       0.000us         0.00%       0.000us       0.000us           349  \n",
      "                                        cudaMemsetAsync         0.40%     517.000us         0.40%     517.000us      16.677us       0.000us         0.00%       0.000us       0.000us            31  \n",
      "                                          aten::reshape         1.74%       2.252ms         1.79%       2.314ms      42.852us     226.000us         0.31%     292.000us       5.407us            54  \n",
      "                                   aten::_reshape_alias         0.05%      62.000us         0.05%      62.000us       1.148us      66.000us         0.09%      66.000us       1.222us            54  \n",
      "                                             aten::add_         0.87%       1.128ms         2.72%       3.523ms      28.877us      12.911ms        17.66%      12.911ms     105.828us           122  \n",
      "                                            BatchNorm2d         0.06%      73.000us         0.18%     231.000us     231.000us       7.000us         0.01%       1.053ms       1.053ms             1  \n",
      "                                       aten::batch_norm         2.71%       3.501ms        38.27%      49.529ms     934.509us     216.000us         0.30%      14.840ms     280.000us            53  \n",
      "                           aten::_batch_norm_impl_index         4.08%       5.279ms        35.57%      46.028ms     868.453us     215.000us         0.29%      14.624ms     275.925us            53  \n",
      "                                 aten::cudnn_batch_norm        24.02%      31.083ms        31.49%      40.749ms     768.849us      13.857ms        18.96%      14.409ms     271.868us            53  \n",
      "                                       aten::empty_like         3.44%       4.446ms         3.67%       4.751ms      89.642us     215.000us         0.29%     270.000us       5.094us            53  \n",
      "                                            aten::empty         0.79%       1.020ms         0.79%       1.020ms       3.835us     282.000us         0.39%     282.000us       1.060us           266  \n",
      "                                             aten::view         0.04%      56.000us         0.04%      56.000us       1.057us      56.000us         0.08%      56.000us       1.057us            53  \n",
      "                                                   relu         0.03%      34.000us         0.05%      68.000us      68.000us       4.000us         0.01%     520.000us     520.000us             1  \n",
      "                                             aten::relu         3.46%       4.477ms         4.59%       5.936ms     121.143us     198.000us         0.27%       6.472ms     132.082us            49  \n",
      "                                        aten::clamp_min         0.42%     544.000us         1.13%       1.459ms      29.776us       6.274ms         8.58%       6.274ms     128.041us            49  \n",
      "                                                maxpool         0.03%      38.000us         0.06%      78.000us      78.000us       4.000us         0.01%     526.000us     526.000us             1  \n",
      "                                       aten::max_pool2d         0.01%      12.000us         0.03%      40.000us      40.000us       4.000us         0.01%     522.000us     522.000us             1  \n",
      "                          aten::max_pool2d_with_indices         0.02%      21.000us         0.02%      28.000us      28.000us     518.000us         0.71%     518.000us     518.000us             1  \n",
      "                                           ResNetBlock1         0.76%     986.000us         3.08%       3.991ms       3.991ms     108.000us         0.15%      19.142ms      19.142ms             1  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.03%      42.000us         0.03%      42.000us       0.385us       0.000us         0.00%       0.000us       0.000us           109  \n",
      "                                           ResNetBlock2        10.06%      13.016ms        44.12%      57.096ms      57.096ms     137.000us         0.19%      19.034ms      19.034ms             1  \n",
      "                                    cudaStreamWaitEvent         0.01%      13.000us         0.01%      13.000us       0.255us       0.000us         0.00%       0.000us       0.000us            51  \n",
      "                                           ResNetBlock3         5.30%       6.857ms        26.54%      34.348ms      34.348ms     204.000us         0.28%      20.570ms      20.570ms             1  \n",
      "                                           ResNetBlock4         1.37%       1.771ms         9.76%      12.633ms      12.633ms     102.000us         0.14%      10.295ms      10.295ms             1  \n",
      "                                                avgpool         0.05%      67.000us         0.09%     116.000us     116.000us       3.000us         0.00%      77.000us      77.000us             1  \n",
      "                              aten::adaptive_avg_pool2d         0.01%      12.000us         0.04%      49.000us      49.000us       4.000us         0.01%      74.000us      74.000us             1  \n",
      "                                             aten::mean         0.02%      29.000us         0.03%      37.000us      37.000us      70.000us         0.10%      70.000us      70.000us             1  \n",
      "                                                     Fc         0.05%      62.000us         0.18%     234.000us     234.000us       7.000us         0.01%      80.000us      80.000us             1  \n",
      "                                           aten::linear         0.02%      22.000us         0.12%     153.000us     153.000us       6.000us         0.01%      68.000us      68.000us             1  \n",
      "                                                aten::t         0.01%      13.000us         0.02%      28.000us      28.000us       4.000us         0.01%       9.000us       9.000us             1  \n",
      "                                        aten::transpose         0.01%      12.000us         0.01%      15.000us      15.000us       4.000us         0.01%       5.000us       5.000us             1  \n",
      "                                       aten::as_strided         0.00%       3.000us         0.00%       3.000us       3.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                            aten::addmm         0.07%      85.000us         0.08%     103.000us     103.000us      52.000us         0.07%      53.000us      53.000us             1  \n",
      "                                  cudaDeviceSynchronize        15.74%      20.372ms        15.74%      20.372ms      20.372ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 129.411ms\n",
      "Self CUDA time total: 73.104ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes):\n",
    "        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n",
    "                                                     f'to be 18, 34, 50, 101, or 152 '\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers < 50:\n",
    "            self.expansion = 1\n",
    "        else:\n",
    "            self.expansion = 4\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "        elif num_layers == 101:\n",
    "            layers = [3, 4, 23, 3]\n",
    "        else:\n",
    "            layers = [3, 8, 36, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.autograd.profiler.record_function(\"conv1\"):\n",
    "            x = self.conv1(x)\n",
    "        with torch.autograd.profiler.record_function(\"BatchNorm2d\"):\n",
    "            x = self.bn1(x)\n",
    "        with torch.autograd.profiler.record_function(\"relu\"):\n",
    "            x = self.relu(x)\n",
    "        with torch.autograd.profiler.record_function(\"maxpool\"):\n",
    "            x = self.maxpool(x)\n",
    "        with torch.autograd.profiler.record_function(\"ResNetBlock1\"):\n",
    "            x = self.layer1(x)\n",
    "        with torch.autograd.profiler.record_function(\"ResNetBlock2\"):\n",
    "            x = self.layer2(x)\n",
    "        with torch.autograd.profiler.record_function(\"ResNetBlock3\"):\n",
    "            x = self.layer3(x)\n",
    "        with torch.autograd.profiler.record_function(\"ResNetBlock4\"):\n",
    "            x = self.layer4(x)\n",
    "        with torch.autograd.profiler.record_function(\"avgpool\"):\n",
    "            x = self.avgpool(x)\n",
    "        with torch.autograd.profiler.record_function(\"Fc\"):\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def ResNet50(img_channels=3, num_classes=1000):\n",
    "        return ResNet(50, Block, img_channels, num_classes)\n",
    "    \n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = ResNet50(img_channels=3, num_classes=1000).to(device)\n",
    "batch_size = 64\n",
    "\n",
    "X= torch.randn(size=(batch_size, 3, 224, 224)).to(device)\n",
    "model.train()\n",
    "\n",
    "# Run the model and record the inference time of each layer\n",
    "for i in range(10):\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True)\n",
    "    model.forward(X)\n",
    "    with profiler:\n",
    "        output = model.forward(X)\n",
    "\n",
    "# Get the table of profiling results\n",
    "profiling_results1 = profiler.key_averages().table()\n",
    "print(profiling_results1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of \"Self CUDA time total\"\n",
    "start_index = profiling_results1.find(\"Self CUDA time total: \")\n",
    "if start_index != -1:\n",
    "    # Extract the substring starting from the index of the value\n",
    "    start_index += len(\"Self CUDA time total: \")\n",
    "    end_index = profiling_results1.find(\"ms\", start_index)\n",
    "    if end_index != -1:\n",
    "        # Extract the value as a float\n",
    "        self_cuda_time_total = float(profiling_results1[start_index:end_index].strip())\n",
    "        print( self_cuda_time_total, \"ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9786f22b",
   "metadata": {},
   "source": [
    "## Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d569ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_ResNet50.csv'.\n",
      "     Layer Name  CUDA Time Avg (ms)  Power Measurements(W)   Energy(mJ)\n",
      "0         conv1               1.807             156.122000   282.112454\n",
      "1   BatchNorm2d               1.053              89.536000    94.281408\n",
      "2          relu               0.520              63.031667    32.776467\n",
      "3       maxpool               0.526              78.758333    41.426883\n",
      "4  ResNetBlock1              19.142             128.988333  2469.094677\n",
      "5  ResNetBlock2              19.034             137.888333  2624.566537\n",
      "6  ResNetBlock3              20.570             145.053000  2983.740210\n",
      "7  ResNetBlock4              10.295             157.442000  1620.865390\n",
      "8       avgpool               0.077              50.635000     3.898895\n",
      "9            Fc               0.080              49.321667     3.945733\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results1.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['conv1', 'BatchNorm2d', 'relu', 'maxpool', 'ResNetBlock1', 'ResNetBlock2', 'ResNetBlock3', 'ResNetBlock4', 'avgpool', 'Fc']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4, mean_power5, mean_power6, mean_power7, mean_power8,mean_power9]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file = 'profiler_results_ResNet50.csv'\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df1 = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the table\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051bba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bf9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df1['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =average_above_90th - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "425d21f4",
   "metadata": {},
   "source": [
    "## Bar Plot_ ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382afd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4ea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAFRCAYAAACFRYHCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqyUlEQVR4nO3deZgdVZnH8e8vYd+DicgewAATZDUgigvKFlAMoiI4QkAEdUBERsfgBqPAADOIiIJGCARlF5CgCMYIKiBLAiEQAhIDkcQAQfZVEt7545ybVG66u7qbVN1efp/nuU9Xnaq677m3u+9765xTpxQRmJmZdWRAqytgZmY9n5OFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWSknC7NeQtJPJH271fWw/snJop+T9KikVyS9WHj8qNX1KqPkGEn3S3pJ0hxJV0rautV1WxYkHSrplmJZRHwhIr7Xqjp1l6QLJZ3U6nrYm7NcqytgPcK+EfH7KgNIWi4iFizDpzwL+DBwBHArMBD4WC67bxnGsS6o4Pf8pvXEOvVGPrOwdjW+3Ur6P0nPSHpE0t6F7WtKOl/SPElzJZ0kaWDh2FslnSnpn8CJkt4i6TpJz0u6K+9/S97/x5LOaIo/QdJX2qjXMOAo4KCI+ENEvBYRL0fExRFxaqFuF0maL2m2pG9JGtDJ13WopFmSXsjb/j2XnyjpF4X9hkoKScvl9Zvza7otn6Fdl1/zxYXXPLRwfOSzo1mSnpL0v5IGSPo34CfAu/PzPJv3X+IbuqQjJM2U9HR+r9Zreu4vSHpY0rP5/VU7v+cTJf1S0uX5Nd8tadvC9vUkXZXfy0ckHdPGsb+Q9DxwaFsx2iPpLEmP5fdniqT35fK3SXpZ0lsK++6Q67B8Xv+spBn5d3ijpI2bXv9Rkh4GHu5KnaxtThZW5l3AQ8Bg4HTg/MKHzoXAAuDtwPbAnsDnmo6dBawDnAz8GHgJeBswOj8axgMHFT7QBwO7A5e0UafdgDkRcWcH9T4bWBPYFPgAcAhwWNnrkrQq8ENg74hYHXgPMLWDOM0OBA4G1gc2A/4CXACsDcwATmja/2PACGAHYBTw2YiYAXwB+EtErBYRazUHkfQh4H+AA4B1gdnAZU27fQTYEdgm77dXB/UeBVyZ63kJ8CtJy+ffx3XAvfk17QYcK2mvpmN/CawFXNxBjLbcBWxXiHulpJUi4nHg5lzvhoOByyLidUmjgG8A+wNDgD8DlzY9936k3/PwLtbJ2hIRfvTjB/Ao8CLwbOFxRN52KDCzsO8qQJA+7NcBXgNWLmw/CLipcOzfC9sGAq8DWxTKTgJuKazPAPbIy0cD17dT528Ct3fwmgYC/wKGF8o+D9zcide1an4PPl58bXm/E4FfFNaH5uOWy+s3A98sbD8D+G1hfV9gamE9gJGF9f8AJhXqeEtT/AuBk/Ly+cDphW2r5fd3aOG531vYfgUwpp3368Ti+0n6EjkPeB/pw/bvTfsfD1xQOPZPJX9ji+rdib/HZ4Bt8/KngFsLv9PHgZ3y+m+Bw5vq/DKwceH1f6jV/1996eEzCwPYLyLWKjx+Vtj2eGMhIl7Oi6sBGwPLA/NyM8ezwE+BtxaOfaywPITUR/ZYO9shnV18Ji9/Bvh5O/X9J+nbdHsG57rNLpTNJn0zbmjzdUXES6QPqS+QXttvJG3ZQaxmTxSWX2ljfbWm/YvvwWxgPTpnPQqvLyJeJL0vbb5G0gdpc+w26xERbwBzcoyNgfUav+P8e/4G6ctCW6+hSyR9NTclPZefe03S7w/gWmC4pE2APYDnYvHZ5MbAWYU6PQ2IJV9/t+tlS3OysO56jHRmMbiQZNaIiK0K+xSnNJ5ParLaoFC2YdNz/gIYldvL/w34VTuxJwEbSBrRzvanSN+yNy6UbQTM7eD1LK50xI0RsQcpIT0INJLnS6SzkIa3deb5ShTfg42AfzSqUXLcPyi8vtx89hY6+Ro7qkduetogx3gMeKTpy8TqEbFP4dhuTV2d+yf+i9TUNChSc9tzpA99IuJV0hnRZ0hNUMUvD48Bn2+q18oRcdubrZe1zcnCuiUi5gG/A86QtEbumN1M0gfa2X8hcDWpo3uV/G39kKZ95pDasH8OXBURr7TzXA8D5wCXStpV0gqSVpJ0oKQxOdYVwMmSVs8dn8eRklGHJK0jaVT+8H2N1ET3Rt48FXi/pI0krUlqjnmzviZpkKQNgS8Dl+fyJ0gJcYV2jrsUOEzSdpJWBE4B7oiIR7tZj3dK2l+ps/5Y0mu/HbgTeEHS1yWtLGmgpHdI2rGLzz8w/44ajxWA1UlfIOYDy0n6DrBG03EXkZrkPsqSyeInwPGStoJFAxo+2cU6WRc4WRjAdVryOotrOnncIcAKwAOktuZf0nHz0NGkZobHSf/4l5I+lIrGA1vTfhNUwzHAj0id5s8CfyN1Fl+Xt3+JdCYwC7iF1Hk6rvwlMYCUWP5Batr4APBFgIiYSPownwZMAX7diecrc21+rqnAb0h9EQB/AKYDj0t6qvmgSEOdvw1cRepf2IzUuf5m6vEp0u/xYGD/iHg9J96PkDqhHyGdtZ1H+j12xRhSM1zj8QfgRuAG4K+kJrVXaWo6iohbScn67ogoNrtdA5wGXJZHYd0P7I1VRrkzyKx2kk4D3hYRowtl7yedAWwcffyPU1IAwyJiZovrcSLw9oj4TNm+rSDpD8AlEXFeq+vSn/nMwmojaUtJ2+QhqjsBhwPXFLYvT2qKOa+vJwrrnNzctQOLm+esRZwsrE6rk/otXiL9859Bav5A6UK0Z0nNWD9oTfWsJ5E0Hvg9cGxEvNDq+vR3boYyM7NSPrMwM7NSThZmZlaqT846O3jw4Bg6dGirq2Fm1qtMmTLlqYgY0ta2Ppkshg4dyuTJk1tdDTOzXkXS7Pa2uRnKzMxKOVmYmVkpJwszMytVWbLIk4XdKeleSdMl/Xcu30TSHUp3+Lq8MVGapBXz+sy8fWjhuY7P5Q813XTFzMxqUOWZxWukm49sS5qEbKSknUmTf50ZEW8nTVp2eN7/cOCZXH5m3g9Jw0kTpG0FjATOUb51p5mZ1aOyZBHJi3l1+fwI4EOk2UkhzTC6X14eldfJ23fLt+8cRbqV4msR8QgwE9ipqnqbmdnSKu2zyHPfTwWeBCaSppF+NiIW5F3msPjOVuuTpyfO258j3cxlUXkbx5iZWQ0qTRYRsTAitiPddWsnoCu3p+wSSUdKmixp8vz586sKY2bWL9VyUV5EPCvpJuDdwFqSlstnDxuw+DaQc0m3dpyT79a1Jumewo3yhuIxxRhjgbEAI0aM8OyIZiWGjvlN5TEePfXDlcewelQ5GmqIpLXy8sqkG67PAG4CPpF3G02eohqYkNfJ2/+Q72kwATgwj5baBBhGutWjmZnVpMozi3WB8Xnk0gDgioj4taQHSLdCPAm4h8W3kTwf+LmkmaTbWR4IEBHTJV1BunXnAuCofKtHMzOrSWXJIiKmAdu3UT6LNkYzRcSrQJs3XI+Ik4GTl3Udzcysc3wFt5mZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVmp5VpdAbP+bOiY31Qe49FTP1x5DOv7fGZhZmalnCzMzKyUk4WZmZVysjAzs1KVJQtJG0q6SdIDkqZL+nIuP1HSXElT82OfwjHHS5op6SFJexXKR+aymZLGVFVnMzNrW5WjoRYA/xkRd0taHZgiaWLedmZE/F9xZ0nDgQOBrYD1gN9L2jxv/jGwBzAHuEvShIh4oMK6m5lZQWXJIiLmAfPy8guSZgDrd3DIKOCyiHgNeETSTGCnvG1mRMwCkHRZ3tfJwsysJrX0WUgaCmwP3JGLjpY0TdI4SYNy2frAY4XD5uSy9srNzKwmlScLSasBVwHHRsTzwLnAZsB2pDOPM5ZRnCMlTZY0ef78+cviKc3MLKs0WUhanpQoLo6IqwEi4omIWBgRbwA/Y3FT01xgw8LhG+Sy9sqXEBFjI2JERIwYMmTIsn8xZmb9WGV9FpIEnA/MiIjvF8rXzf0ZAB8D7s/LE4BLJH2f1ME9DLgTEDBM0iakJHEg8Omq6m39j6fcMCtX5WioXYCDgfskTc1l3wAOkrQdEMCjwOcBImK6pCtIHdcLgKMiYiGApKOBG4GBwLiImF5hvc3MrEmVo6FuIZ0VNLu+g2NOBk5uo/z6jo4zM7Nq+QpuMzMr5WRhZmalnCzMzKyUk4WZmZVysjAzs1JOFmZmVsrJwszMSjlZmJlZKScLMzMr5WRhZmalnCzMzKyUk4WZmZVysjAzs1JOFmZmVsrJwszMSjlZmJlZKScLMzMr5WRhZmalnCzMzKyUk4WZmZVysjAzs1JOFmZmVsrJwszMSjlZmJlZKScLMzMr5WRhZmalKksWkjaUdJOkByRNl/TlXL62pImSHs4/B+VySfqhpJmSpknaofBco/P+D0saXVWdzcysbVWeWSwA/jMihgM7A0dJGg6MASZFxDBgUl4H2BsYlh9HAudCSi7ACcC7gJ2AExoJxszM6lFZsoiIeRFxd15+AZgBrA+MAsbn3cYD++XlUcBFkdwOrCVpXWAvYGJEPB0RzwATgZFV1dvMzJZWS5+FpKHA9sAdwDoRMS9vehxYJy+vDzxWOGxOLmuvvDnGkZImS5o8f/78ZfsCzMz6ucqThaTVgKuAYyPi+eK2iAgglkWciBgbESMiYsSQIUOWxVOamVlWabKQtDwpUVwcEVfn4idy8xL555O5fC6wYeHwDXJZe+VmZlaTKkdDCTgfmBER3y9smgA0RjSNBq4tlB+SR0XtDDyXm6tuBPaUNCh3bO+Zy8zMrCbLVfjcuwAHA/dJmprLvgGcClwh6XBgNnBA3nY9sA8wE3gZOAwgIp6W9D3grrzfdyPi6QrrbWZmTSpLFhFxC6B2Nu/Wxv4BHNXOc40Dxi272pmZWVf4Cm4zMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvVqWQh6QxJW1VdGTMz65k6e2YxAxgr6Q5JX5C0ZpWVMjOznqVTySIizouIXYBDgKHANEmXSPpglZUzM7OeodN9FpIGAlvmx1PAvcBxki6rqG5mZtZDdGrWWUlnAvsCk4BTIuLOvOk0SQ9VVTkzM+sZOjtF+TTgWxHxUhvbdlqG9TEzsx6os8niXmCLdPO7RZ4DZkfEc8u8VmZm1qN0NlmcA+xAOsMQ8A5gOrCmpC9GxO8qqp+ZmfUAne3g/gewfUSMiIh3AtsDs4A9gNOrqpyZmfUMnU0Wm0fE9MZKRDwAbBkRs6qplpmZ9SSdbYZ6QNK5QGOY7Kdy2YrA65XUzMzMeozOnlmMBmYCx+bHLOBQUqLwhXlmZn1c6ZlFvhjv+oj4IHBGG7u8uMxrZWZ92tAxv6k8xqOnfrjyGP1J6ZlFRCwE3vB8UGZm/Vdn+yxeBO6TNBFYdGFeRBxTSa3MzKxH6WyyuDo/zMysH+pUsoiI8ZJWBjaKCM8FZWbWz3T25kf7AlOBG/L6dpImlBwzTtKTku4vlJ0oaa6kqfmxT2Hb8ZJmSnpI0l6F8pG5bKakMV18fWZmtgx0dujsiaQJA58FiIipwKYlx1wIjGyj/MyI2C4/rgeQNBw4ENgqH3OOpIF5JNaPgb2B4cBBeV8zM6tRZ5PF621MGPhGRwdExJ+Apzv5/KOAyyLitYh4hHRNx075MTMiZkXEv0gXBY7q5HOamdky0tlkMV3Sp4GBkoZJOhu4rZsxj5Y0LTdTDcpl6wOPFfaZk8vaK1+KpCMlTZY0ef78+d2smpmZtaWzyeJLpCai14BLgedJV3J31bnAZsB2wDzavsivWyJibJ7ocMSQIUOW1dOamRmdHw31MvDN/Oi2iHiisSzpZ8Cv8+pcYMPCrhvkMjooNzOzmnT2tqqbA18FhhaPiYgPdSWYpHUjYl5e/RjQGCk1AbhE0veB9YBhwJ2ke2cMk7QJKUkcCHy6KzHNzOzN6+xFeVcCPwHOAxZ25gBJlwK7AoMlzQFOAHaVtB0QwKPA5wEiYrqkK4AHgAXAUXmaESQdDdwIDATGFadKNzOzenQ2WSyIiHO78sQRcVAbxed3sP/JwMltlF8PXN+V2GZmtmx1toP7Okn/IWldSWs3HpXWzMzMeozOnlmMzj+/VigLyi/MMzOzPqCzo6E2qboiZmbWc3XYDCXpvwrLn2zadkpVlTIzs56lrM/iwMLy8U3b2pr3yczM+qCyZKF2lttaNzOzPqosWUQ7y22tm5lZH1XWwb2tpOdJZxEr52Xy+kqV1szMzHqMDpNFRAysqyJmZtZzdfaiPDMz68ecLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMytVWbKQNE7Sk5LuL5StLWmipIfzz0G5XJJ+KGmmpGmSdigcMzrv/7Ck0VXV18zM2lflmcWFwMimsjHApIgYBkzK6wB7A8Py40jgXEjJBTgBeBewE3BCI8GYmVl9KksWEfEn4Omm4lHA+Lw8HtivUH5RJLcDa0laF9gLmBgRT0fEM8BElk5AZmZWsbr7LNaJiHl5+XFgnby8PvBYYb85uay9cjMzq1HLOrgjIoBYVs8n6UhJkyVNnj9//rJ6WjMzo/5k8URuXiL/fDKXzwU2LOy3QS5rr3wpETE2IkZExIghQ4Ys84qbmfVndSeLCUBjRNNo4NpC+SF5VNTOwHO5uepGYE9Jg3LH9p65zMzMarRcVU8s6VJgV2CwpDmkUU2nAldIOhyYDRyQd78e2AeYCbwMHAYQEU9L+h5wV97vuxHR3GluZmYVqyxZRMRB7WzarY19AziqnecZB4xbhlUzM7Mu8hXcZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKtSRZSHpU0n2SpkqanMvWljRR0sP556BcLkk/lDRT0jRJO7SizmZm/Vkrzyw+GBHbRcSIvD4GmBQRw4BJeR1gb2BYfhwJnFt7Tc3M+rme1Aw1Chifl8cD+xXKL4rkdmAtSeu2oH5mZv1Wq5JFAL+TNEXSkblsnYiYl5cfB9bJy+sDjxWOnZPLliDpSEmTJU2eP39+VfU2M+uXlmtR3PdGxFxJbwUmSnqwuDEiQlJ05QkjYiwwFmDEiBFdOtbMzDrWkjOLiJibfz4JXAPsBDzRaF7KP5/Mu88FNiwcvkEuMzOzmtSeLCStKmn1xjKwJ3A/MAEYnXcbDVyblycAh+RRUTsDzxWaq8zMrAataIZaB7hGUiP+JRFxg6S7gCskHQ7MBg7I+18P7APMBF4GDqu/ymZm/VvtySIiZgHbtlH+T2C3NsoDOKqGqpmZWTt60tBZMzProZwszMyslJOFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWSknCzMzK+VkYWZmpZwszMysVKumKO/Rho75TeUxHj31w5XHMDNbVnxmYWZmpZwszMyslJOFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWSknCzMzK+VkYWZmpZwszMyslJOFmZmVcrIwM7NSnkiwh/EkhmbWE/nMwszMSjlZmJlZqV7TDCVpJHAWMBA4LyJObXGVbBly85tZz9YrkoWkgcCPgT2AOcBdkiZExAOtrVnf4g9sM2tPb2mG2gmYGRGzIuJfwGXAqBbXycys31BEtLoOpSR9AhgZEZ/L6wcD74qIowv7HAkcmVe3AB6qsYqDgadqjOfYju3Y/Sd+nbE3joghbW3oFc1QnRERY4GxrYgtaXJEjHBsx3bsvhe71fFb/dobeksz1Fxgw8L6BrnMzMxq0FuSxV3AMEmbSFoBOBCY0OI6mZn1G72iGSoiFkg6GriRNHR2XERMb3G1ilrS/OXYju3Y/SJ+q1870Es6uM3MrLV6SzOUmZm1kJOFmZmVcrIwM7NSThZmZlaqV4yG6i0kbRkRD1b4/NcB7Y5IiIiPVhW7UIdD2ol9UYUx76Pt160UOrapKnZHJI2NiCPL9+z28w8EPke6ruiGiLi1sO1bEXFShbFXAY4mve9nk4ar7w88CHw3Il6sKnY79flrRGxeQ5yz6fh/7Jiq65DrcRRwcUQ8m9cHAQdFxDl1xG+zTh4NtexI+ntEbFTh838gL+4PvA34RV4/CHgiIr5SVexCHc4urK4E7AbcHRGfqDDmxh1tj4jZFcZeu71NwL0RsUGFsc8DVgHuBA4G/hgRx+Vtd0fEDhXGvgJ4DFiZNH3ODOBy4KPA2yLi4Apjv8DiD2zln6sAL5O+HKxRYezRHW2PiPFVxW6qx9SI2K6p7J6I2L6O+G1xsugiST9sbxMwuso/5EIdlrr8v1VTAkhaC7gsIkbWFG8dYMe8emdEPFlxvIXAbBZ/aEH6IBOwfkSsUGHsaY2zJknLAeeQ5gk6CLi9yg+OxoeVJAHzgHUjIvL6vVWezeX/sbWAr0XEE7nskYjYpKqYHdRlNYAWnEndB2wT+QM6n2VOi4it6qxHkfssuu4w4H5gStNjMvCvmuqwqqRNGyuSNgFWrSl2s5eAWv6JJR1A+pb9SeAA4I48yWSVZgG7RsQmhcem+YPriYpjL0pEEbEgN3lNBf4ArFZx7EbcAK5vfGjln5V+w8xNPWcBl0o6RtKAqmM2k/QOSfcA04EHJE2RVOcH9Q3A5ZJ2k7QbcGkuaxn3WXTdXcD9EXFb8wZJJ9ZUh68AN0uaRfqGuzGLZ9ytVFO/yQBgOHBFHbGBbwI7Ns4mJA0Bfg/8ssKYPwAGAX9vY9vpFcYFmCxpZEQs+pCIiO9K+gdwbg2xV4uIFyPis41CSZsBL1Qcm4iYIml3Ur/JH0lNnnUaCxwXETcBSNoV+Bnwnprifx34PPDFvD4ROK+m2G1yM1QX5TbsVyPi5RbXY0Vgy7z6YES8VlPcDxRWFwCzI2JOTbHvi4itC+sDSE0iW3dw2LKKrWj6Z5G0Yh3vezuxV4qIV1sUe+WIeKWu2JLWBbaPiOtrfM/vjYhty8oqiLtRRLT1xaTl3AzVRRHxdES8LGn//IFdK0lrSNosIl6LiHvz4zVJtYwIiog/Fh631pUoshsk3SjpUEmHAr8Brq8p9vnFldyW3crY1d/WsP3Yv64zdkTMy4mizvd8lqRvSxqaH98iNUlW7VeNBUlX1RCv05wsum9f4K+Sfi7pI7kDslK5zf5B4CpJ0yXtWNh8YcWxX5D0fBuPFyQ9X2Xshoj4GvBTYJv8GBsRX68jNjBH0jmwaBjj71g8Gs2x+17szwJDgKvzY0guq1pxIMWm7e7VAm6GehMkLQ/sDXwKeC8wsXE3v4riTQX2joh5knYCLgKOj4hrWj2sri55NNROpH6TykdDNcU+HVgDeCdwakTU9s3PseuPneOvTurXr2U0VHFIdNXDo7vKyeJNygljJGmU1PsjYnCFsZrb7NclNQmMBw6t6w9L0nuBYRFxgaTBwOoR8UgNcQ8A/he4mfQN7H2k4ZWVdXBL2r+4CnybNCLrBoCIuNqx+07sQh22Jn0Za1xn8xRpaPz9FcddSBphKNI1Lo2+0cYFqJUPzW+3bk4W3SOpcUaxK+nD6wrgdxGxoMKYtwEHR8TfCmWrk9o53xsRlfehSDoBGAFsERGbS1oPuDIidqkh9r3AHs2joarsdJR0QQebozhSyLF7f+xCHW4Dvtk0GuqUiKhrNFSP42TRTZIuJV3R+tsaRyJtC7wUETObypcHDoiIi2uow1Rge9JV29vnskUXj1Ucu2Wjoax/adVoqJ7MHdzdFBEHRcSv6koUOea9xUSRR0atDawO/LamavyreGGWpDovBmzZaChJ45WuVm+sD5I0zrH7ZmxaNxqqx3Ky6KY8dPZhSc/VPSpI0uclPQ5MY8kryKuOK+DXkn4KrCXpCNJFcT+rOja0fDTUNpEndct1eYZ0huXYfTN2q0ZD9Vi+grv7Tgf2jYgZLYj9VeAdEfFUnUHzBVKfBI4DnidNMPediJhYYzVuAxYCb5Cupq/LAEmD8gdW4+LMuv5/HLvm2DnmMZLWBN6IiMqvWu/pnCy674kWJQqAv7F4lETd7gaezd/yayXpc8B3SHMjCThb0ncjoo6miTOAv0i6Msf+BHByDXEduwWx8zVM40hNvEh6DvhsREypI35P5A7ubpJ0Fmma8F8Bi/otahrWtz1wAXBHU+zK59qX9CDwdtJMrC8VYtfRwf0Q8J6I+GdefwtwW0RsUXXsHG848CFSf81NEfFAHXEdu/7YkqYBR0XEn/P6e4Fz6vg776l8ZtF9a5C+3e9ZKAtS+2bVfkr6dn0fqTmmTnvVHK/onyw5id0Luawuy7P4Ctvla4zr2PXHXthIFAARcYukyobF9wY+s+iF+svV2s0kXQRsDVxLSsyjSJ380wAi4vsVxv4ycARwFenD62OkDvazOzzQsXtr7B+QLoq7lPS39ingVfJ0IxFxd9V16GmcLLpJ0gak2002Lkb7M/DlOibWk3QK8ChwHUs2Qz1ddexWyhcEtisi/rvC2NOAd0fES3l9VeAvNTW/OXb9sW/qYHNExIeqrkNP42ao7rsAuIR0Ix6Az+SyPWqIfVD+eXyhLOhhE49V4LRompZb0uCaRoWJNAqrYSFLTvrm2H0odkR8sI44vYmTRfcNiYjitAQXSjq26qD5quUxEXF51bF6oDslHRkRtwNI+jjwP8DmNcS+gHRnvmvy+n40Td/t2H0ntqTj2ih+DpgSEVPrqENP42aobpI0ifTHfGkuOgg4LCJ2qyF2S+633WpKk7uNI83FtR7wFuBzdTT95fg7kGYXBvhzRNxTR1zHrj+2pEtIc6Bdl4s+QuobG0qaC63quyT2OE4W3SRpY1KfxbtJTUC3AV+KiMdqiH0qaRbMy1ly+Gqf7rMAkLQf8HPSSKj3N8+TVUG8tTvaXuV77tj1xy7U4U/APpGnJtfiG06NJJ1dDK+6Dj2Nm6G677ukKYuLV5f+H/VMCfCp/POoQlmf77OQdD6wGWmqj81JU4+cHRE/rjDsFNJ722grb3y7EtW/545df+yGt1IYPAK8DqwTEa9Iqm0+uJ7EyaL7tmkkCkjfdvLFcpWLiE3qiNMD3UdqdgrgEUnvAiobLgutfa8du6UuJvWXXJvX9wUuySOyarsosSdxM1Q3Kd1bYdemM4s/1jFdttKU5F8E3p+LbgZ+GhGvVx27P1K6Ze7CPDfWhsC7gJl1dHQ6dv2xC3UYweKh8bdGROWTdfZknnW2+xrz1nxP0vdIfRZ1dXqdS7rN5Dn58c5c1qdJGibpl5IekDSr8ag45hHAk8DsvDyJNEfR5ZIqnfHWseuPXajDD4EVIuKs/OjXiQJ8ZvGmFOatAfhDjfPW9Msbs0i6BTgBOJPULHAYMCAivlNhzOmk0TirAzOAjSPiKUmrAHdFxFaO3XdiF+owmtQ3uAVwDXBZf08Y7rN4E3JyaEX75UJJm0W+vaqkTVny4qW+auWImCRJETEbOFHSFNJMtFX5V25qfEbSzMYFgBHxsqR/VRjXsVsTmxxrPDA+Ny9/HDhN0kYRMayO+D2Rk0Xv9DXgptwEI2Bj0rfsvu61fFHiw5KOBuYCq1Ucc+U8cGEAsEJeVn6s5Nh9LnaztwNbkv7HWnVLgh7BzVC9lKQVSafIAA9Fjbd3bRWlewzMANYCvkea+ff0iLijwpgdzRFU6bQQjl1/7EIdTidNXPg34DLgV1G4a19/5GTRi0h6f0fbI+JPddWlFfLolG+SvuU1pquOmiaWW6mNealWrCNJO3ZLYn+edLuBTYEVG+V9/X+sI26G6l3aujtdkC5S2xAYWG91ancx6T1oxX08bgN2aCr7Sxtljt03Yr9BumfMBsBUYOccu9/NNtvgZNGLRMS+xXVJuwDfAh4HvtSSStVrfkRMqDOgpLcB67O4Hb1xVfEawCqO3bdiFxwD7AjcHhEflLQlcEpNsXskJ4teSNJuwLdJZxWnRMTEFlepLidIOo807r6uW9nuBRxK+oZZvFr8eeAbFcZ17NbEbng1Il6V1Gj6elBSLbfv7ancZ9GLSPowqc3+OeDkiLilxVWqlaRfkEamTGdxM1REROXzcUn6eERcVXUcx+4xsa8hjTA8ltT09AywfETs04r69AROFr2IpDeAOcC9LJ5cbZGI+GjtlaqRpIcioiXf7nLTyMnAehGxd74g890RUfn9FRy7/thN9fgAsCZwQ0TUcp1HT+Rk0YvkP9p2RcQf66pLK0i6APjfuq6Ub4r9W9L9S74ZEdvmuYvuqWkuMMeuObYtzX0WvUhfTwadsDMwVdIjpD4LUdPQWWBwRFwh6XhS0AWS6rpq3rHrj21NnCx6oTwK6kTS9QbLsfhDs0/fz4J045lWeUnSW8jNf5J2JvUdOXbfjG1N3AzVC0l6EPgK6SYxi75pRcQ/W1apPk7p9p5nA+8A7geGAJ+IiGmO3fdi29KcLHohSXdExLtaXY/+JreZb0E6k3soarx/iGPXH9uW5GTRi+RvWgAHkK7Wvpolrze4uxX16staOcWKY9cf29rnZNGLlEywFhHRb6ciqIqk69ooXjTFSkRUNsWKY9cf29rnZGHWBYUpVgaRLoxs64PNsftIbFvMo6F6IUmnkKbmfjavDwL+MyK+1dKK9WGtnGLFsfvdtDY9ks8seiFJ90TE9k1ld0dEHbNx9iutnGLFsfvntDY9lZNFLyRpGrBjY15/SSsDk6OGexP3N62cYsWx++e0Nj2Vm6F6p4uBSXn6C0gTnl3Uwvr0ZZXflc2xe1Rsa4fPLHopSSOB3fPqxIi4sZX16U9yH9GGrbg4zLF9QV6rOFn0QpJOi4ivl5XZsiPpZuCjpLPxKcCTwK0RcZxj973YtrQBra6AdcsebZTtXXst+pc1I+J5YH/gonwF/e4lxzh2741tTZwsehFJX5R0H7CFpGmFxyOAT8+rtZykdUlXz//asft8bGviDu7e5RLgt8D/AGMK5S9ExNOtqVK/8V3gRlIzyF2SNgUeduw+G9uauM+iF5P0VmClxnpE/L2F1TGzPszNUL2QpH0lPQw8AvwReJR0xmEVkbS5pEmS7s/r20iq5Yp5x64/ti3NyaJ3Ool017i/RsQmwG7A7a2tUp/3M+B44HWAPITzQMfus7GtiZNF7/R6vtHRAEkDIuImYESrK9XHrRIRdzaVLXDsPhvbmriDu3d6VtJqwJ+AiyU9CbzU4jr1dU9J2ozFt/j8BDDPsftsbGviDu5eSNKqwCukM8N/B9YELvZtVauTR+KMBd4DPEPqL/r3iJjt2H0vti3NyaKXkzQY+Gf4F1mLnKgHAC8DB0bExY7dd2PbYu6z6EUk7SzpZklXS9o+jxK5H3gizxVly5ikNSQdL+lHkvYgfWCNBmaSLhZz7D4U29rnM4teRNJk4BukZqexwN4RcbukLYFLm+9xYW+epGtJTSB/IY06eysg4MsRMdWx+1Zsa5+TRS8iaWpEbJeXZ0TEvxW2LXVDJHvzJN0XEVvn5YGkDtaNIuJVx+57sa19bobqXd4oLL/StM1ZvxqvNxYiYiEwp8YPLceuP7a1w2cWvYikhaQhsgJWJrXlktdXiojlW1W3vqrwnsOS77uAiIg1HLvvxLb2OVmYmVkpN0OZmVkpJwszMyvlZGH9mqQXWxR3qKSQ9KVC2Y8kHdqK+piVcbIwq4GktuZhexL4sqQV6q6PWVc5WZg1yfcLuUPSPZJ+L2kdSQMkPSxpSN5ngKSZkobkx1WS7sqPXfI+J0r6uaRbgZ+3EWo+MIl0dXJzHY7Iz3Vvfu5VcvmFks6VdLukWZJ2lTRO0gxJFxaO31PSXyTdLenKPPGkWbc5WZgt7RZg53yR42XAf0XEG8AvSBM3AuwO3BsR84GzgDMjYkfg48B5hecaDuweEQe1E+s04Kv54rOiqyNix4jYFpgBHF7YNgh4N/AVYAJwJrAVsLWk7fJ8Yd/KcXcAJgPHdfldMCvwFOVmS9sAuFzSusAKpNlOAcYB1wI/AD4LXJDLdweGS2ocv0bhm/yEiGi+gHKRiJgl6Q7g002b3iHpJGAtYDXSvagbrouIkHQf8ERE3AcgaTowNNd/OHBrrtMKpKkzzLrNycJsaWcD34+ICZJ2BU4EiIjHJD0h6UPATiw+yxhAOhNZ4irj/EHdmfuMnAL8knSL3IYLgf0i4t7c6b1rYdtr+ecbheXG+nLAQmBiB2czZl3mZiizpa0JzM3Lzf0J55Gao67MU1EA/A4ojmrarivBIuJB4AFg30Lx6sA8ScuzOCl11u3ALpLenuuzqqTNu/gcZktwsrD+bhVJcwqP40hnEldKmgI81bT/BFKz0AWFsmOAEZKmSXoA+EI36nEyqfmo4dvAHcCtwINdeaLcj3IocKmkaaQmqC27USezRTzdh1kXSBpB6sx+X6vrYlYn91mYdZKkMcAX6XqzkFmv5zMLMzMr5T4LMzMr5WRhZmalnCzMzKyUk4WZmZVysjAzs1JOFmZmVur/AfOWbWWLh55WAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df['Layer Name'], df['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f6cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b54bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4895d559",
   "metadata": {},
   "source": [
    "# VGG 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda4652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92920ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "58.34 W\n",
      "gotstarted\n",
      "127.81 W\n",
      "gotstarted\n",
      "118.93 W\n",
      "gotstarted\n",
      "119.62 W\n",
      "gotstarted\n",
      "114.84 W\n",
      "gotstarted\n",
      "116.45 W\n",
      "gotstarted\n",
      "106.24 W\n",
      "gotstarted\n",
      "117.42 W\n",
      "gotstarted\n",
      "128.70 W\n",
      "gotstarted\n",
      "118.69 W\n",
      "gotstarted\n",
      "118.60 W\n",
      "gotstarted\n",
      "107.84 W\n",
      "gotstarted\n",
      "109.24 W\n",
      "gotstarted\n",
      "117.98 W\n",
      "gotstarted\n",
      "117.51 W\n",
      "gotstarted\n",
      "118.40 W\n",
      "gotstarted\n",
      "116.60 W\n",
      "gotstarted\n",
      "113.93 W\n",
      "gotstarted\n",
      "118.99 W\n",
      "gotstarted\n",
      "122.72 W\n",
      "gotstarted\n",
      "126.30 W\n",
      "gotstarted\n",
      "123.43 W\n",
      "gotstarted\n",
      "106.20 W\n",
      "gotstarted\n",
      "125.36 W\n",
      "gotstarted\n",
      "122.31 W\n",
      "gotstarted\n",
      "116.95 W\n",
      "gotstarted\n",
      "118.69 W\n",
      "gotstarted\n",
      "107.71 W\n",
      "gotstarted\n",
      "118.77 W\n",
      "gotstarted\n",
      "107.50 W\n",
      "torch.Size([8, 10])\n",
      "Mean power:  115.40233333333332\n",
      "Average of values >= __th percentile: 127.60333333333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #q = multiprocessing.Queue()\n",
    "        #q.put('start')\n",
    "        #p = multiprocessing.Process(target=measure, args=(q,))\n",
    "        #p.start()\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        # Signal the end of the measurement\n",
    "        #q.put('stop')\n",
    "        # Wait for the measurement to finish\n",
    "        #p.join()\n",
    "        # Retrieve the power measurements from the queue\n",
    "       # while not q.empty():\n",
    "           # power_output = q.get()\n",
    "           # if power_output == 'stop':\n",
    "            #    break\n",
    "            #print(power_output)\n",
    "        return out\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = VGG16().to(device)\n",
    "batch_size = 8\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).to(device)\n",
    "model.train()\n",
    "power_measurements = []\n",
    "for i in range(30):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    \n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "    x = X.clone()\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    x = model.layer1(x)\n",
    "\n",
    "    x = model.layer2(x)\n",
    "\n",
    "    x = model.layer3(x)\n",
    "\n",
    "    x = model.layer4(x)\n",
    "\n",
    "    x = model.layer5(x)\n",
    "\n",
    "    x = model.layer6(x)\n",
    "\n",
    "    x = model.layer7(x)\n",
    "\n",
    "    x = model.layer8(x)\n",
    "\n",
    "    x = model.layer9(x)\n",
    "\n",
    "    \n",
    "    x = model.layer10(x)\n",
    "\n",
    "\n",
    "    x = model.layer11(x)\n",
    "\n",
    "    x = model.layer12(x)\n",
    "\n",
    "    x = model.layer13(x)\n",
    "\n",
    "\n",
    "    x = x.reshape(x.size(0), -1)\n",
    "\n",
    "    x = model.fc(x)\n",
    "\n",
    "    x = model.fc1(x)\n",
    "\n",
    "    x = model.fc2(x)\n",
    "    q.put('stop')\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    output = x\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "    p.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power)\n",
    "percentile_90 = np.percentile(power_measurements, 90)\n",
    "average_above_90th = np.mean(np.array(power_measurements)[np.array(power_measurements) >= percentile_90])\n",
    "print('Average of values >= __th percentile:', average_above_90th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dff280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3955167",
   "metadata": {},
   "source": [
    "## multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42a566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "89.88 W\n",
      "111.72 W\n",
      "153.98 W\n",
      "196.10 W\n",
      "161.53 W\n",
      "164.14 W\n",
      "173.21 W\n",
      "166.65 W\n",
      "175.15 W\n",
      "164.47 W\n",
      "150.94 W\n",
      "149.29 W\n",
      "148.57 W\n",
      "109.36 W\n",
      "56.33 W\n",
      "46.37 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "106.47 W\n",
      "134.72 W\n",
      "180.70 W\n",
      "213.21 W\n",
      "164.53 W\n",
      "60.57 W\n",
      "169.68 W\n",
      "163.84 W\n",
      "194.01 W\n",
      "177.06 W\n",
      "152.06 W\n",
      "147.83 W\n",
      "149.75 W\n",
      "108.13 W\n",
      "55.50 W\n",
      "46.61 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "106.83 W\n",
      "134.12 W\n",
      "191.50 W\n",
      "197.00 W\n",
      "172.17 W\n",
      "173.59 W\n",
      "173.35 W\n",
      "168.29 W\n",
      "178.62 W\n",
      "173.19 W\n",
      "149.79 W\n",
      "147.85 W\n",
      "152.27 W\n",
      "109.46 W\n",
      "54.79 W\n",
      "46.66 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "106.00 W\n",
      "131.91 W\n",
      "183.25 W\n",
      "193.95 W\n",
      "161.61 W\n",
      "171.87 W\n",
      "174.76 W\n",
      "165.53 W\n",
      "177.79 W\n",
      "187.66 W\n",
      "147.88 W\n",
      "151.26 W\n",
      "152.59 W\n",
      "106.29 W\n",
      "54.97 W\n",
      "46.85 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "115.59 W\n",
      "135.89 W\n",
      "183.50 W\n",
      "205.11 W\n",
      "178.39 W\n",
      "173.37 W\n",
      "174.18 W\n",
      "165.41 W\n",
      "174.96 W\n",
      "187.30 W\n",
      "152.36 W\n",
      "150.68 W\n",
      "150.84 W\n",
      "112.17 W\n",
      "56.45 W\n",
      "46.85 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "105.89 W\n",
      "133.18 W\n",
      "191.73 W\n",
      "196.35 W\n",
      "166.00 W\n",
      "173.51 W\n",
      "175.92 W\n",
      "167.19 W\n",
      "181.42 W\n",
      "178.92 W\n",
      "147.42 W\n",
      "152.06 W\n",
      "152.58 W\n",
      "111.04 W\n",
      "55.57 W\n",
      "47.04 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "108.02 W\n",
      "134.15 W\n",
      "171.67 W\n",
      "211.07 W\n",
      "164.85 W\n",
      "170.51 W\n",
      "172.15 W\n",
      "163.14 W\n",
      "192.99 W\n",
      "171.19 W\n",
      "150.05 W\n",
      "151.86 W\n",
      "150.21 W\n",
      "109.47 W\n",
      "55.28 W\n",
      "47.24 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "107.54 W\n",
      "134.38 W\n",
      "175.34 W\n",
      "202.04 W\n",
      "163.45 W\n",
      "173.99 W\n",
      "172.83 W\n",
      "169.68 W\n",
      "179.63 W\n",
      "173.45 W\n",
      "148.77 W\n",
      "152.13 W\n",
      "146.31 W\n",
      "108.57 W\n",
      "55.86 W\n",
      "47.35 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "108.07 W\n",
      "135.98 W\n",
      "176.84 W\n",
      "206.02 W\n",
      "169.64 W\n",
      "194.11 W\n",
      "175.11 W\n",
      "169.91 W\n",
      "181.68 W\n",
      "177.05 W\n",
      "147.23 W\n",
      "151.48 W\n",
      "152.90 W\n",
      "94.70 W\n",
      "55.28 W\n",
      "47.43 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "107.50 W\n",
      "134.15 W\n",
      "188.42 W\n",
      "211.04 W\n",
      "176.98 W\n",
      "174.09 W\n",
      "175.83 W\n",
      "167.84 W\n",
      "182.00 W\n",
      "196.13 W\n",
      "147.08 W\n",
      "148.02 W\n",
      "139.01 W\n",
      "108.78 W\n",
      "55.98 W\n",
      "47.63 W\n",
      "torch.Size([32, 10])\n",
      "Mean power:  106.179\n",
      "Mean power1:  132.02\n",
      "Mean power2:  179.693\n",
      "Mean power3:  203.189\n",
      "Mean power4:  167.91499999999996\n",
      "Mean power5:  162.97500000000002\n",
      "Mean power6:  173.702\n",
      "Mean power7:  166.748\n",
      "Mean power8:  181.825\n",
      "Mean power9:  178.642\n",
      "Mean power10:  149.358\n",
      "Mean power11:  150.246\n",
      "Mean power12:  149.50300000000001\n",
      "Mean power13:  107.797\n",
      "Mean power14:  55.601\n",
      "Mean power15:  47.003\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #q = multiprocessing.Queue()\n",
    "        #q.put('start')\n",
    "        #p = multiprocessing.Process(target=measure, args=(q,))\n",
    "        #p.start()\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        # Signal the end of the measurement\n",
    "        #q.put('stop')\n",
    "        # Wait for the measurement to finish\n",
    "        #p.join()\n",
    "        # Retrieve the power measurements from the queue\n",
    "       # while not q.empty():\n",
    "           # power_output = q.get()\n",
    "           # if power_output == 'stop':\n",
    "            #    break\n",
    "            #print(power_output)\n",
    "        return out\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_ids):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=1 -i {gpu_ids}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = VGG16().to(device)\n",
    "batch_size = 8\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).to(device)\n",
    "model.train()\n",
    "power_measurements =[] \n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "power_measurements4 = []\n",
    "power_measurements5 = []\n",
    "power_measurements6 = []\n",
    "power_measurements7 = []\n",
    "power_measurements8 = [] \n",
    "power_measurements9 = []\n",
    "power_measurements10 = [] \n",
    "power_measurements11 = []\n",
    "power_measurements12 = []\n",
    "power_measurements13 = []\n",
    "power_measurements14 = []\n",
    "power_measurements15 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue()\n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue()\n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue() \n",
    "    rq5 = multiprocessing.Queue()\n",
    "    q5 = multiprocessing.Queue()\n",
    "    rq6 = multiprocessing.Queue()\n",
    "    q6 = multiprocessing.Queue()\n",
    "    q7 = multiprocessing.Queue()\n",
    "    rq7 = multiprocessing.Queue() \n",
    "    q8 = multiprocessing.Queue()\n",
    "    rq8 = multiprocessing.Queue() \n",
    "    q9 = multiprocessing.Queue()\n",
    "    rq9 = multiprocessing.Queue()  \n",
    "    q10 = multiprocessing.Queue()\n",
    "    rq10 = multiprocessing.Queue() \n",
    "    rq11 = multiprocessing.Queue()\n",
    "    q11 = multiprocessing.Queue()\n",
    "    rq12 = multiprocessing.Queue()\n",
    "    q12 = multiprocessing.Queue()\n",
    "    rq13 = multiprocessing.Queue()\n",
    "    q13 = multiprocessing.Queue()\n",
    "    rq14 = multiprocessing.Queue() \n",
    "    q14 = multiprocessing.Queue()\n",
    "    rq15 = multiprocessing.Queue() \n",
    "    q15 = multiprocessing.Queue()\n",
    "\n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    x = X.clone()\n",
    "    for i in range(10):\n",
    "        x1 = model.layer1(x)\n",
    "    q.put('stop')\n",
    "\n",
    "    \n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'               \n",
    "    for i in range(10):\n",
    "        x2 = model.layer2(x1)\n",
    "    q1.put('stop')\n",
    "    \n",
    "    \n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'              \n",
    "    for i in range(10):\n",
    "        x3 = model.layer3(x2)\n",
    "    q2.put('stop')\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'          \n",
    "    for i in range(10):\n",
    "        x4 = model.layer4(x3)        \n",
    "    q3.put('stop')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'          \n",
    "    for i in range(10):\n",
    "        x5 = model.layer5(x4)\n",
    "    q4.put('stop')\n",
    "    \n",
    "    \n",
    "    \n",
    "    p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "    p5.start()\n",
    "    q5.put('start')\n",
    "    m = rq5.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x6 = model.layer6(x5)        \n",
    "    q5.put('stop')\n",
    "    \n",
    "    #x = model.maxpool(x)\n",
    "    p6 = multiprocessing.Process(target=measure, args=(q6,rq6,1))\n",
    "    p6.start()\n",
    "    q6.put('start')\n",
    "    m = rq6.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x7 = model.layer7(x6)        \n",
    "    q6.put('stop')\n",
    "\n",
    "\n",
    "    p7 = multiprocessing.Process(target=measure, args=(q7,rq7,1))\n",
    "    p7.start()\n",
    "    q7.put('start')\n",
    "    m = rq7.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x8 = model.layer8(x7)            \n",
    "    q7.put('stop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p8 = multiprocessing.Process(target=measure, args=(q8,rq8,1))\n",
    "    p8.start()\n",
    "    q8.put('start')\n",
    "    m = rq8.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x9 = model.layer9(x8)            \n",
    "    q8.put('stop')\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    #output = model.forward(X)   \n",
    "\n",
    "\n",
    "    p9 = multiprocessing.Process(target=measure, args=(q9,rq9,1))\n",
    "    p9.start()\n",
    "    q9.put('start')\n",
    "    m = rq9.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x10 = model.layer10(x9)    \n",
    "    q9.put('stop') \n",
    "\n",
    "    p10 = multiprocessing.Process(target=measure, args=(q10,rq10,1))\n",
    "    p10.start()\n",
    "    q10.put('start')\n",
    "    m = rq10.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'          \n",
    "    for i in range(10):\n",
    "        x11 = model.layer11(x10)    \n",
    "    q10.put('stop')\n",
    "    \n",
    "    \n",
    "    \n",
    "    p11 = multiprocessing.Process(target=measure, args=(q11,rq11,1))\n",
    "    p11.start()\n",
    "    q11.put('start')\n",
    "    m = rq11.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x12 = model.layer12(x11)    \n",
    "    q11.put('stop')\n",
    "    \n",
    "    #x = model.maxpool(x)\n",
    "    p12 = multiprocessing.Process(target=measure, args=(q12,rq12,1))\n",
    "    p12.start()\n",
    "    q12.put('start')\n",
    "    m = rq12.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x13 = model.layer13(x12)    \n",
    "    q12.put('stop')\n",
    "\n",
    "\n",
    "    p13 = multiprocessing.Process(target=measure, args=(q13,rq13,1))\n",
    "    p13.start()\n",
    "    q13.put('start')\n",
    "    m = rq13.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        x14 = x13.reshape(x13.size(0), -1)\n",
    "        x14 = model.fc(x14)\n",
    "    q13.put('stop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p14 = multiprocessing.Process(target=measure, args=(q14,rq14,1))\n",
    "    p14.start()\n",
    "    q14.put('start')\n",
    "    m = rq14.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10): \n",
    "        x15 = model.fc1(x14)\n",
    "    q14.put('stop')\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "    #output = model.forward(X)   \n",
    "\n",
    "\n",
    "    p15 = multiprocessing.Process(target=measure, args=(q15,rq15,1))\n",
    "    p15.start()\n",
    "    q15.put('start')\n",
    "    m = rq15.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10): \n",
    "        x16 = model.fc2(x15)\n",
    "    q15.put('stop')\n",
    "\n",
    "\n",
    "    output = x16\n",
    "\n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference\n",
    "\n",
    "    # Signal the end of the power measurement\n",
    "\n",
    "\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "    p.join() \n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()\n",
    "    p4.join()\n",
    "    p5.join()\n",
    "    p6.join()\n",
    "    p7.join()\n",
    "    p8.join() \n",
    "    p9.join() \n",
    "    p10.join() \n",
    "    p11.join()\n",
    "    p12.join()\n",
    "    p13.join()\n",
    "    p14.join()\n",
    "    p15.join()\n",
    "\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq5.empty():\n",
    "        power_output = rq5.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements5.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq6.empty():\n",
    "        power_output = rq6.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements6.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq7.empty():\n",
    "        power_output = rq7.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements7.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq8.empty():\n",
    "        power_output = rq8.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements8.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    while not rq9.empty():\n",
    "        power_output = rq9.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements9.append(float(power_output.split()[0]))  # Remov]\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq10.empty():\n",
    "        power_output = rq10.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements10.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq11.empty():\n",
    "        power_output = rq11.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements11.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq12.empty():\n",
    "        power_output = rq12.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements12.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq13.empty():\n",
    "        power_output = rq13.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements13.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq14.empty():\n",
    "        power_output = rq14.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements14.append(float(power_output.split()[0]))  # Remov]  \n",
    "\n",
    "    while not rq15.empty():\n",
    "        power_output = rq15.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements15.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "\n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power)\n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"Mean power1: \", mean_power1) \n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"Mean power2: \", mean_power2)\n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"Mean power3: \", mean_power3) \n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"Mean power4: \", mean_power4) \n",
    "mean_power5 = np.mean(power_measurements5)\n",
    "print(\"Mean power5: \", mean_power5) \n",
    "mean_power6 = np.mean(power_measurements6)\n",
    "print(\"Mean power6: \", mean_power6)\n",
    "mean_power7 = np.mean(power_measurements7)\n",
    "print(\"Mean power7: \", mean_power7) \n",
    "mean_power8 = np.mean(power_measurements8)\n",
    "print(\"Mean power8: \", mean_power8) \n",
    "mean_power9 = np.mean(power_measurements9)\n",
    "print(\"Mean power9: \", mean_power9) \n",
    "mean_power10 = np.mean(power_measurements10)\n",
    "print(\"Mean power10: \", mean_power10)\n",
    "mean_power11 = np.mean(power_measurements11)\n",
    "print(\"Mean power11: \", mean_power11) \n",
    "mean_power12 = np.mean(power_measurements12)\n",
    "print(\"Mean power12: \", mean_power12) \n",
    "mean_power13 = np.mean(power_measurements13)\n",
    "print(\"Mean power13: \", mean_power13)\n",
    "mean_power14 = np.mean(power_measurements14)\n",
    "print(\"Mean power14: \", mean_power14) \n",
    "mean_power15 = np.mean(power_measurements15)\n",
    "print(\"Mean power15: \", mean_power15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbbc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fdef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:29 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:26:30 59794:59794 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 layer1         0.43%     151.000us         1.41%     493.000us     493.000us      11.000us         0.05%       1.508ms       1.508ms             1  \n",
      "                                           aten::conv2d         0.38%     132.000us         4.72%       1.644ms     126.462us      51.000us         0.25%      14.632ms       1.126ms            13  \n",
      "                                      aten::convolution         0.57%     200.000us         4.34%       1.512ms     116.308us      53.000us         0.26%      14.581ms       1.122ms            13  \n",
      "                                     aten::_convolution         1.25%     437.000us         3.77%       1.312ms     100.923us     116.000us         0.57%      14.528ms       1.118ms            13  \n",
      "                                aten::cudnn_convolution         1.15%     401.000us         1.54%     535.000us      41.154us      13.186ms        64.46%      13.186ms       1.014ms            13  \n",
      "                                  cudaStreamIsCapturing         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            28  \n",
      "                                  cudaStreamGetPriority         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            26  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            26  \n",
      "                                       cudaLaunchKernel         1.18%     412.000us         1.18%     412.000us       4.430us       0.000us         0.00%       0.000us       0.000us            93  \n",
      "                                        cudaMemsetAsync         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          aten::reshape         0.51%     177.000us         0.56%     195.000us      13.929us      58.000us         0.28%      76.000us       5.429us            14  \n",
      "                                   aten::_reshape_alias         0.05%      18.000us         0.05%      18.000us       1.286us      18.000us         0.09%      18.000us       1.286us            14  \n",
      "                                             aten::add_         0.61%     212.000us         0.90%     312.000us      12.000us       1.207ms         5.90%       1.207ms      46.423us            26  \n",
      "                                       aten::batch_norm         0.34%     119.000us         4.99%       1.737ms     133.615us      52.000us         0.25%       2.830ms     217.692us            13  \n",
      "                           aten::_batch_norm_impl_index         0.44%     154.000us         4.64%       1.618ms     124.462us      52.000us         0.25%       2.778ms     213.692us            13  \n",
      "                                 aten::cudnn_batch_norm         3.00%       1.046ms         4.20%       1.464ms     112.615us       2.594ms        12.68%       2.726ms     209.692us            13  \n",
      "                                       aten::empty_like         0.50%     174.000us         0.73%     256.000us      15.059us      69.000us         0.34%      86.000us       5.059us            17  \n",
      "                                            aten::empty         0.66%     231.000us         0.66%     231.000us       3.397us      69.000us         0.34%      69.000us       1.015us            68  \n",
      "                                             aten::view         0.04%      13.000us         0.04%      13.000us       1.000us      13.000us         0.06%      13.000us       1.000us            13  \n",
      "                                             aten::relu         0.63%     218.000us         1.25%     437.000us      29.133us      61.000us         0.30%       1.187ms      79.133us            15  \n",
      "                                        aten::clamp_min         0.44%     152.000us         0.63%     219.000us      14.600us       1.126ms         5.50%       1.126ms      75.067us            15  \n",
      "                                                 layer2         0.40%     139.000us         1.46%     510.000us     510.000us      13.000us         0.06%       3.604ms       3.604ms             1  \n",
      "                                       aten::max_pool2d         0.15%      53.000us         0.41%     142.000us      28.400us      21.000us         0.10%     494.000us      98.800us             5  \n",
      "                          aten::max_pool2d_with_indices         0.20%      70.000us         0.26%      89.000us      17.800us     473.000us         2.31%     473.000us      94.600us             5  \n",
      "                                                 layer3         0.32%     111.000us         1.13%     394.000us     394.000us      11.000us         0.05%       1.624ms       1.624ms             1  \n",
      "                                                 layer4         0.37%     129.000us         1.26%     438.000us     438.000us      14.000us         0.07%       2.242ms       2.242ms             1  \n",
      "                                                 layer5         0.32%     112.000us         1.16%     404.000us     404.000us      11.000us         0.05%     999.000us     999.000us             1  \n",
      "                                                 layer6         0.35%     122.000us         1.22%     424.000us     424.000us      11.000us         0.05%       1.613ms       1.613ms             1  \n",
      "                                                 layer7         0.38%     134.000us         1.28%     447.000us     447.000us      14.000us         0.07%       1.677ms       1.677ms             1  \n",
      "                                                 layer8         0.32%     113.000us         1.19%     416.000us     416.000us      11.000us         0.05%     941.000us     941.000us             1  \n",
      "                                                 layer9         0.32%     110.000us         1.25%     437.000us     437.000us      11.000us         0.05%       1.659ms       1.659ms             1  \n",
      "                                                layer10         0.39%     136.000us         1.30%     454.000us     454.000us      15.000us         0.07%       1.691ms       1.691ms             1  \n",
      "                                                layer11         0.33%     116.000us         1.19%     415.000us     415.000us      11.000us         0.05%     586.000us     586.000us             1  \n",
      "                                                layer12         0.31%     109.000us         1.11%     387.000us     387.000us      12.000us         0.06%     587.000us     587.000us             1  \n",
      "                                                layer13         0.38%     133.000us         1.26%     439.000us     439.000us      14.000us         0.07%     607.000us     607.000us             1  \n",
      "                                                     FC         0.27%      93.000us         0.99%     345.000us     345.000us       9.000us         0.04%     873.000us     873.000us             1  \n",
      "                                          aten::dropout         0.07%      24.000us         0.47%     163.000us      81.500us       7.000us         0.03%      51.000us      25.500us             2  \n",
      "                                   aten::native_dropout         0.20%      70.000us         0.40%     139.000us      69.500us      24.000us         0.12%      44.000us      22.000us             2  \n",
      "                                    aten::empty_strided         0.05%      19.000us         0.05%      19.000us       4.750us       4.000us         0.02%       4.000us       1.000us             4  \n",
      "                                           aten::linear         0.23%      81.000us         1.13%     395.000us     131.667us      20.000us         0.10%       1.024ms     341.333us             3  \n",
      "                                                aten::t         0.11%      39.000us         0.25%      87.000us      29.000us      12.000us         0.06%      27.000us       9.000us             3  \n",
      "                                        aten::transpose         0.12%      43.000us         0.14%      48.000us      16.000us      12.000us         0.06%      15.000us       5.000us             3  \n",
      "                                       aten::as_strided         0.01%       5.000us         0.01%       5.000us       1.667us       3.000us         0.01%       3.000us       1.000us             3  \n",
      "                                            aten::addmm         0.49%     171.000us         0.65%     227.000us      75.667us     974.000us         4.76%     977.000us     325.667us             3  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.04%      14.000us         0.04%      14.000us       3.500us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                                    FC1         0.27%      95.000us         1.02%     355.000us     355.000us      10.000us         0.05%     200.000us     200.000us             1  \n",
      "                                                    FC2         0.14%      48.000us         0.46%     161.000us     161.000us       4.000us         0.02%      41.000us      41.000us             1  \n",
      "                                  cudaDeviceSynchronize        81.24%      28.307ms        81.24%      28.307ms      28.307ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 34.843ms\n",
      "Self CUDA time total: 20.457ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time as timer_l\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from numba import cuda\n",
    "#import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.autograd.profiler.record_function(\"layer1\"):\n",
    "            out = self.layer1(x)\n",
    "        with torch.autograd.profiler.record_function(\"layer2\"):\n",
    "            out = self.layer2(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer3\"):\n",
    "            out = self.layer3(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer4\"):\n",
    "            out = self.layer4(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer5\"):\n",
    "            out = self.layer5(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer6\"):\n",
    "            out = self.layer6(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer7\"):\n",
    "            out = self.layer7(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer8\"):\n",
    "            out = self.layer8(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer9\"):\n",
    "            out = self.layer9(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer10\"):\n",
    "            out = self.layer10(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer11\"):\n",
    "            out = self.layer11(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer12\"):\n",
    "            out = self.layer12(out)\n",
    "        with torch.autograd.profiler.record_function(\"layer13\"):\n",
    "            out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        with torch.autograd.profiler.record_function(\"FC\"):\n",
    "            out = self.fc(out)\n",
    "        with torch.autograd.profiler.record_function(\"FC1\"):\n",
    "            out = self.fc1(out)\n",
    "        with torch.autograd.profiler.record_function(\"FC2\"):\n",
    "            out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = VGG16().to(device)    \n",
    "batch_size = 8\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).to(device) \n",
    "\n",
    "# Run the model and record the inference time of each layer\n",
    "for i in range(10):\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True)\n",
    "    model.forward(X)\n",
    "    with profiler:\n",
    "        output = model.forward(X)\n",
    "\n",
    "# Get the table of profiling results\n",
    "profiling_results2 = profiler.key_averages().table()\n",
    "print(profiling_results2) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "202b8d6a",
   "metadata": {},
   "source": [
    "## Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_VGG16.csv'.\n",
      "   Layer Name  CUDA Time Avg (ms)  Power Measurements(W)  Energy(mJ)\n",
      "0      layer1               3.176                 38.784  123.177984\n",
      "1      layer2               6.956                 38.852  270.254512\n",
      "2      layer3               3.265                 38.832  126.786480\n",
      "3      layer4               4.058                 39.519  160.368102\n",
      "4      layer5               1.523                 38.745   59.008635\n",
      "5      layer6               2.196                 39.691   87.161436\n",
      "6      layer7               2.318                146.362  339.267116\n",
      "7      layer8               1.177                143.792  169.243184\n",
      "8      layer9               1.859                164.232  305.307288\n",
      "9     layer10               1.927                159.023  306.437321\n",
      "10    layer11               0.696                 89.409   62.228664\n",
      "11    layer12               0.696                 90.742   63.156432\n",
      "12    layer13               0.717                 94.797   67.969449\n",
      "13         FC               0.796                102.154   81.314584\n",
      "14        FC1               0.204                 52.415   10.692660\n",
      "15        FC2               0.047                 44.095    2.072465\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results2.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'layer6', 'layer7', 'layer8', 'layer9', 'layer10','layer11','layer12','layer13','FC','FC1','FC2']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4, mean_power5, mean_power6, mean_power7, mean_power8,mean_power9, mean_power10, mean_power11, mean_power12,mean_power13,mean_power14,mean_power15]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file3 = 'profiler_results_VGG16.csv'\n",
    "with open(csv_file3, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file3}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df3 = pd.read_csv(csv_file3)\n",
    "\n",
    "# Display the table\n",
    "print(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df3['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =average_above_90th - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf54d35",
   "metadata": {},
   "source": [
    "## Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c6c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df3 = pd.read_csv(csv_file3)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df3['Layer Name'], df3['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fdb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704de982",
   "metadata": {},
   "source": [
    "# EfficientNet_B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting validators\n",
      "  Downloading validators-0.14.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python2.7/dist-packages (2.1.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /usr/lib/python2.7/dist-packages (from validators) (4.1.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python2.7/dist-packages (from validators) (1.11.0)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.14.2-py2-none-any.whl size=17491 sha256=801fe0acdb901378b2a400d8796e26857e47cae223913627881b5b0a9d4aaf33\n",
      "  Stored in directory: /home/daouda/.cache/pip/wheels/e2/6d/bc/878f25fb3f7a9ec6e473a9b30698f0c734c5ed7e9816c47cd5\n",
      "Successfully built validators\n",
      "Installing collected packages: validators\n",
      "Successfully installed validators-0.14.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daouda/.local/lib/python3.8/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /home/daouda/.cache/torch/hub/torchhub.zip\n",
      "/home/daouda/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/home/daouda/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/efficientnet_b0_pyt_amp/versions/20.12.0/files/nvidia_efficientnet-b0_210412.pth\" to /home/daouda/.cache/torch/hub/checkpoints/nvidia_efficientnet-b0_210412.pth\n",
      "100%|██████████| 20.5M/20.5M [00:04<00:00, 4.69MB/s]\n",
      "Using cache found in /home/daouda/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'validators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/daouda/workspace_test/Experiments.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install validators matplotlib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments.ipynb#Y136sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m efficientnet \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mNVIDIA/DeepLearningExamples:torchhub\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnvidia_efficientnet_b0\u001b[39m\u001b[39m'\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments.ipynb#Y136sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m utils \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mNVIDIA/DeepLearningExamples:torchhub\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnvidia_convnets_processing_utils\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[39m=\u001b[39mverbose, skip_validation\u001b[39m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/hub.py:587\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m     hub_module \u001b[39m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    586\u001b[0m     entry \u001b[39m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 587\u001b[0m     model \u001b[39m=\u001b[39m entry(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/entrypoints.py:51\u001b[0m, in \u001b[0;36mnvidia_convnets_processing_utils\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mvalidators\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mProcessing\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     56\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mprepare_input_from_uri\u001b[39m(uri, cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'validators'"
     ]
    }
   ],
   "source": [
    "#!pip install validators matplotlib\n",
    "#efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "#utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (stem): Sequential(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (activation): SiLU(inplace=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (expand): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=96, out_features=4, bias=True)\n",
       "          (expand): Linear(in_features=4, out_features=96, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=144, out_features=6, bias=True)\n",
       "          (expand): Linear(in_features=6, out_features=144, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=144, out_features=6, bias=True)\n",
       "          (expand): Linear(in_features=6, out_features=144, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=240, out_features=10, bias=True)\n",
       "          (expand): Linear(in_features=10, out_features=240, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=240, out_features=10, bias=True)\n",
       "          (expand): Linear(in_features=10, out_features=240, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block3): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (conv): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (activation): SiLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (squeeze): Flatten()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cd15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "\n",
    "        # Expansion phase\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * expand_ratio, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2, groups=in_channels * expand_ratio, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Squeeze-and-excitation phase\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio // se_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(in_channels * expand_ratio // se_ratio, in_channels * expand_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Output phase\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # Skip connection phase\n",
    "        self.use_skip = (stride == 1 and in_channels == out_channels)\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expand(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.se(out) * out\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.use_skip:\n",
    "            out = out + self.skip(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Stem convolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # MBConv blocks\n",
    "        self.block1 = nn.Sequential(MBConvBlock(32, 16, kernel_size=3, stride=1, expand_ratio=1, se_ratio=4),\n",
    "            MBConvBlock(16, 24, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 24, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 40, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(40, 40, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block2 = nn.Sequential(MBConvBlock(40, 80, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 80, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(112, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block3 = nn.Sequential(MBConvBlock(112, 192, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 192, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 320, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4)\n",
    "        )\n",
    "\n",
    "        # Head convolution\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.head(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1adb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "39.96 W\n",
      "gotstarted\n",
      "51.61 W\n",
      "gotstarted\n",
      "51.79 W\n",
      "gotstarted\n",
      "52.37 W\n",
      "gotstarted\n",
      "50.54 W\n",
      "gotstarted\n",
      "51.61 W\n",
      "gotstarted\n",
      "50.12 W\n",
      "gotstarted\n",
      "48.10 W\n",
      "gotstarted\n",
      "51.97 W\n",
      "gotstarted\n",
      "50.25 W\n",
      "gotstarted\n",
      "50.73 W\n",
      "gotstarted\n",
      "50.73 W\n",
      "gotstarted\n",
      "51.79 W\n",
      "gotstarted\n",
      "52.87 W\n",
      "gotstarted\n",
      "49.27 W\n",
      "gotstarted\n",
      "51.02 W\n",
      "gotstarted\n",
      "49.07 W\n",
      "gotstarted\n",
      "49.39 W\n",
      "gotstarted\n",
      "51.29 W\n",
      "gotstarted\n",
      "51.00 W\n",
      "torch.Size([8, 1000])\n",
      "Mean power0:  50.274\n",
      "Average of values >= __th percentile: 52.62\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "\n",
    "        # Expansion phase\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * expand_ratio, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2, groups=in_channels * expand_ratio, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Squeeze-and-excitation phase\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio // se_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(in_channels * expand_ratio // se_ratio, in_channels * expand_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Output phase\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # Skip connection phase\n",
    "        self.use_skip = (stride == 1 and in_channels == out_channels)\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expand(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.se(out) * out\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.use_skip:\n",
    "            out = out + self.skip(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Stem convolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # MBConv blocks\n",
    "        self.block1 = nn.Sequential(MBConvBlock(32, 16, kernel_size=3, stride=1, expand_ratio=1, se_ratio=4),\n",
    "            MBConvBlock(16, 24, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 24, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 40, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(40, 40, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block2 = nn.Sequential(MBConvBlock(40, 80, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 80, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(112, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block3 = nn.Sequential(MBConvBlock(112, 192, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 192, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 320, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4)\n",
    "        )\n",
    "\n",
    "        # Head convolution\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.head(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def measure(q, rq, gpu_id):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=2 -i {gpu_id}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = EfficientNetB0().to(device)\n",
    "batch_size = 8\n",
    "X= torch.randn(size=(batch_size, 3, 224, 224)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = []\n",
    "\n",
    "for i in range(20):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    \n",
    "\n",
    "    # Signal the start of the measurement\n",
    "\n",
    "    # Run the inference########################\n",
    "    #output = model.forward(X)\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'          \n",
    "    x = X.clone()\n",
    "    x = model.stem(x)\n",
    "\n",
    "    \n",
    "\n",
    "    x = model.block1(x)\n",
    "            \n",
    "\n",
    "    \n",
    "    x = model.block2(x)\n",
    "\n",
    "    \n",
    "\n",
    "    x = model.block3(x)\n",
    "    \n",
    "    x = model.head(x)\n",
    "\n",
    "    \n",
    "    x = model.classifier(x)\n",
    "\n",
    "    output = x\n",
    "    q.put('stop')\n",
    "\n",
    "    \n",
    "    #################\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "    p.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power_1 = np.mean(power_measurements)\n",
    "print(\"Mean power0: \", mean_power_1)\n",
    "percentile_90 = np.percentile(power_measurements, 90)\n",
    "average_above_90th = np.mean(np.array(power_measurements)[np.array(power_measurements) >= percentile_90])\n",
    "print('Average of values >= __th percentile:', average_above_90th)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7094e0b1",
   "metadata": {},
   "source": [
    "## Multiprocess_power measurement Efficient NetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d5af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "45.49 W\n",
      "50.82 W\n",
      "50.32 W\n",
      "48.29 W\n",
      "48.88 W\n",
      "46.27 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "55.67 W\n",
      "56.07 W\n",
      "51.51 W\n",
      "51.02 W\n",
      "49.47 W\n",
      "46.46 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "54.44 W\n",
      "54.89 W\n",
      "51.90 W\n",
      "51.48 W\n",
      "49.77 W\n",
      "46.37 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "51.59 W\n",
      "54.99 W\n",
      "51.20 W\n",
      "50.44 W\n",
      "49.65 W\n",
      "46.46 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "52.97 W\n",
      "58.38 W\n",
      "48.97 W\n",
      "48.98 W\n",
      "50.63 W\n",
      "46.36 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "52.76 W\n",
      "58.74 W\n",
      "51.30 W\n",
      "47.24 W\n",
      "49.96 W\n",
      "46.45 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "54.99 W\n",
      "58.59 W\n",
      "50.15 W\n",
      "50.84 W\n",
      "49.56 W\n",
      "46.66 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "53.83 W\n",
      "56.65 W\n",
      "50.92 W\n",
      "51.32 W\n",
      "49.56 W\n",
      "46.56 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "53.55 W\n",
      "55.39 W\n",
      "50.04 W\n",
      "50.93 W\n",
      "49.47 W\n",
      "46.65 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "54.01 W\n",
      "56.73 W\n",
      "51.52 W\n",
      "49.55 W\n",
      "49.37 W\n",
      "46.75 W\n",
      "torch.Size([8, 1000])\n",
      "stem power:  52.93000000000001\n",
      "block1 power:  56.125\n",
      "block2 power:  50.783\n",
      "block3 power:  50.009\n",
      "head power:  49.632000000000005\n",
      "classifier power:  46.499\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "\n",
    "        # Expansion phase\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * expand_ratio, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2, groups=in_channels * expand_ratio, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Squeeze-and-excitation phase\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio // se_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(in_channels * expand_ratio // se_ratio, in_channels * expand_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Output phase\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # Skip connection phase\n",
    "        self.use_skip = (stride == 1 and in_channels == out_channels)\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expand(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.se(out) * out\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.use_skip:\n",
    "            out = out + self.skip(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Stem convolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # MBConv blocks\n",
    "        self.block1 = nn.Sequential(MBConvBlock(32, 16, kernel_size=3, stride=1, expand_ratio=1, se_ratio=4),\n",
    "            MBConvBlock(16, 24, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 24, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 40, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(40, 40, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block2 = nn.Sequential(MBConvBlock(40, 80, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 80, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(112, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block3 = nn.Sequential(MBConvBlock(112, 192, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 192, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 320, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4)\n",
    "        )\n",
    "\n",
    "        # Head convolution\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.head(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def measure(q, rq, gpu_id):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=2 -i {gpu_id}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = EfficientNetB0().to(device)\n",
    "batch_size = 8\n",
    "X= torch.randn(size=(batch_size, 3, 224, 224)).type(FloatTensor).to(device)\n",
    "model.train()\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "power_measurements4 = []\n",
    "power_measurements5 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue()\n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue()\n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue() \n",
    "    q5 = multiprocessing.Queue()\n",
    "    rq5 = multiprocessing.Queue()\n",
    "    \n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    x = X.clone()\n",
    "    for i in range(10):\n",
    "        x1 = model.stem(x)\n",
    "    q.put('stop')\n",
    "    p.join() \n",
    "    \n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'               \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(100):\n",
    "        x2 = model.block1(x1)\n",
    "    q1.put('stop')\n",
    "    p1.join()\n",
    "    \n",
    "    \n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'              \n",
    "    #x = model.maxpool(x)\n",
    "    for i in range(10):\n",
    "\n",
    "        x3= model.block2(x2)\n",
    "    q2.put('stop')\n",
    "    p2.join()\n",
    "    \n",
    "            \n",
    "        \n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'  \n",
    "    for i in range(10):\n",
    "        x4 = model.block3(x3)\n",
    "    q3.put('stop')\n",
    "    p3.join()       \n",
    "\n",
    "\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        x5 = model.head(x4)\n",
    "    q4.put('stop')\n",
    "    p4.join()        \n",
    "    \n",
    "    p5 = multiprocessing.Process(target=measure, args=(q5,rq5,1))\n",
    "    p5.start()\n",
    "    q5.put('start')\n",
    "    m = rq5.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'\n",
    "    for i in range(10):\n",
    "        x6 = model.classifier(x5)\n",
    "    q5.put('stop')\n",
    "\n",
    "    p5.join()\n",
    "    # Run the inference########################\n",
    "    #output = model.forward(X)\n",
    "\n",
    "    output = x6\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #################\n",
    "\n",
    "    # Wait for the measurement to finish\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remov] \n",
    "\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remov\n",
    "            \n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq5.empty():\n",
    "        power_output = rq5.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements5.append(float(power_output.split()[0]))  # Remov\n",
    "        \n",
    "\n",
    "\n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"stem power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"block1 power: \", mean_power1)\n",
    "mean_power2= np.mean(power_measurements2)\n",
    "print(\"block2 power: \", mean_power2)\n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"block3 power: \", mean_power3)\n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"head power: \", mean_power4)\n",
    "mean_power5 = np.mean(power_measurements5)\n",
    "print(\"classifier power: \", mean_power5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e7bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:54 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 11:56:55 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Stem         0.74%     153.000us         2.53%     526.000us     526.000us      89.000us         0.43%     592.000us     592.000us             1  \n",
      "                                           aten::conv2d         3.24%     672.000us        24.14%       5.010ms      80.806us     532.000us         2.56%       6.311ms     101.790us            62  \n",
      "                                      aten::convolution         4.52%     939.000us        20.90%       4.338ms      69.968us     735.000us         3.54%       5.779ms      93.210us            62  \n",
      "                                     aten::_convolution         4.43%     919.000us        16.37%       3.399ms      54.823us     651.000us         3.13%       5.044ms      81.355us            62  \n",
      "                                aten::cudnn_convolution         8.28%       1.718ms         9.76%       2.027ms      40.540us       3.303ms        15.89%       3.303ms      66.060us            50  \n",
      "                                  cudaStreamIsCapturing         0.00%       1.000us         0.00%       1.000us       0.011us       0.000us         0.00%       0.000us       0.000us            88  \n",
      "                                  cudaStreamGetPriority         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            88  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            88  \n",
      "                                       cudaLaunchKernel         4.38%     910.000us         4.38%     910.000us       3.889us       0.000us         0.00%       0.000us       0.000us           234  \n",
      "                                        cudaMemsetAsync         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             7  \n",
      "                                             aten::add_         1.56%     324.000us         2.18%     453.000us      11.921us     268.000us         1.29%     268.000us       7.053us            38  \n",
      "                                       aten::batch_norm         1.74%     362.000us        25.31%       5.253ms     138.237us     276.000us         1.33%       6.036ms     158.842us            38  \n",
      "                           aten::_batch_norm_impl_index         2.17%     450.000us        23.56%       4.891ms     128.711us     314.000us         1.51%       5.760ms     151.579us            38  \n",
      "                                 aten::cudnn_batch_norm        15.36%       3.189ms        21.39%       4.441ms     116.868us       3.571ms        17.18%       5.446ms     143.316us            38  \n",
      "                                       aten::empty_like         2.03%     421.000us         2.90%     601.000us      15.816us     362.000us         1.74%     655.000us      17.237us            38  \n",
      "                                            aten::empty         3.16%     656.000us         3.16%     656.000us       3.232us       1.373ms         6.61%       1.373ms       6.764us           203  \n",
      "                                             aten::view         0.24%      49.000us         0.24%      49.000us       1.289us     209.000us         1.01%     209.000us       5.500us            38  \n",
      "                                        aten::hardtanh_         4.76%     989.000us        18.05%       3.747ms      98.605us     654.000us         3.15%       3.984ms     104.842us            38  \n",
      "                                            aten::clone         3.96%     822.000us         7.79%       1.617ms      42.553us     526.000us         2.53%       1.874ms      49.316us            38  \n",
      "                                    aten::empty_strided         1.03%     214.000us         1.03%     214.000us       5.632us     289.000us         1.39%     289.000us       7.605us            38  \n",
      "                                            aten::copy_         1.06%     219.000us         2.80%     581.000us      15.289us       1.059ms         5.09%       1.059ms      27.868us            38  \n",
      "                                        cudaMemcpyAsync         1.74%     362.000us         1.74%     362.000us       9.526us       0.000us         0.00%       0.000us       0.000us            38  \n",
      "                                         aten::hardtanh         1.98%     412.000us         5.50%       1.141ms      30.026us     297.000us         1.43%       1.456ms      38.316us            38  \n",
      "                                            aten::clamp         2.92%     607.000us         3.51%     729.000us      19.184us       1.035ms         4.98%       1.159ms      30.500us            38  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us     124.000us         0.60%     124.000us       3.263us            38  \n",
      "                                                 block1         9.37%       1.946ms        38.42%       7.976ms       7.976ms     722.000us         3.47%       7.937ms       7.937ms             1  \n",
      "                                aten::_conv_depthwise2d         1.56%     323.000us         2.18%     453.000us      37.750us     953.000us         4.58%       1.090ms      90.833us            12  \n",
      "                                          aten::resize_         0.22%      46.000us         0.22%      46.000us       3.833us      76.000us         0.37%      76.000us       6.333us            12  \n",
      "                              aten::adaptive_avg_pool2d         0.72%     150.000us         2.10%     436.000us      33.538us     101.000us         0.49%     479.000us      36.846us            13  \n",
      "                                             aten::mean         1.13%     234.000us         1.38%     286.000us      22.000us     378.000us         1.82%     378.000us      29.077us            13  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.19%      40.000us         0.19%      40.000us       0.303us       0.000us         0.00%       0.000us       0.000us           132  \n",
      "                                          aten::sigmoid         0.81%     168.000us         1.01%     209.000us      17.417us     261.000us         1.26%     261.000us      21.750us            12  \n",
      "                                              aten::mul         0.85%     176.000us         1.07%     222.000us      18.500us     395.000us         1.90%     395.000us      32.917us            12  \n",
      "                                              aten::add         0.28%      59.000us         0.37%      76.000us      15.200us     102.000us         0.49%     102.000us      20.400us             5  \n",
      "                                                 block2         7.86%       1.631ms        31.52%       6.542ms       6.542ms       1.068ms         5.14%       6.546ms       6.546ms             1  \n",
      "                                                 block3         5.85%       1.214ms        23.73%       4.926ms       4.926ms     748.000us         3.60%       4.931ms       4.931ms             1  \n",
      "                                                   head         0.62%     129.000us         2.28%     474.000us     474.000us      59.000us         0.28%     477.000us     477.000us             1  \n",
      "                                             classifier         0.45%      94.000us         1.44%     299.000us     299.000us      85.000us         0.41%     303.000us     303.000us             1  \n",
      "                                          aten::flatten         0.07%      14.000us         0.10%      20.000us      20.000us      13.000us         0.06%      23.000us      23.000us             1  \n",
      "                                   aten::_reshape_alias         0.03%       6.000us         0.03%       6.000us       6.000us      10.000us         0.05%      10.000us      10.000us             1  \n",
      "                                           aten::linear         0.10%      21.000us         0.72%     150.000us     150.000us      16.000us         0.08%     156.000us     156.000us             1  \n",
      "                                                aten::t         0.06%      12.000us         0.13%      27.000us      27.000us      12.000us         0.06%      30.000us      30.000us             1  \n",
      "                                        aten::transpose         0.06%      13.000us         0.07%      15.000us      15.000us      13.000us         0.06%      18.000us      18.000us             1  \n",
      "                                       aten::as_strided         0.01%       2.000us         0.01%       2.000us       2.000us       5.000us         0.02%       5.000us       5.000us             1  \n",
      "                                            aten::addmm         0.37%      77.000us         0.49%     102.000us     102.000us     102.000us         0.49%     110.000us     110.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.07%      15.000us         0.07%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 20.758ms\n",
      "Self CUDA time total: 20.786ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### import torch\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import time as timer_l\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "\n",
    "        # Expansion phase\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * expand_ratio, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2, groups=in_channels * expand_ratio, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * expand_ratio),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Squeeze-and-excitation phase\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio // se_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(in_channels * expand_ratio // se_ratio, in_channels * expand_ratio, kernel_size=1, stride=1,\n",
    "                      padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Output phase\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * expand_ratio, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # Skip connection phase\n",
    "        self.use_skip = (stride == 1 and in_channels == out_channels)\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.expand(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.se(out) * out\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.use_skip:\n",
    "            out = out + self.skip(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Stem convolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # MBConv blocks\n",
    "        self.block1 = nn.Sequential(MBConvBlock(32, 16, kernel_size=3, stride=1, expand_ratio=1, se_ratio=4),\n",
    "            MBConvBlock(16, 24, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 24, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(24, 40, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(40, 40, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block2 = nn.Sequential(MBConvBlock(40, 80, kernel_size=3, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 80, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(80, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(112, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4))\n",
    "        self.block3 = nn.Sequential(MBConvBlock(112, 192, kernel_size=5, stride=2, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 192, kernel_size=5, stride=1, expand_ratio=6, se_ratio=4),\n",
    "            MBConvBlock(192, 320, kernel_size=3, stride=1, expand_ratio=6, se_ratio=4)\n",
    "        )\n",
    "\n",
    "        # Head convolution\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.autograd.profiler.record_function(\"Stem\"):\n",
    "            out = self.stem(x)\n",
    "        with torch.autograd.profiler.record_function(\"block1\"):\n",
    "            out = self.block1(out)\n",
    "        with torch.autograd.profiler.record_function(\"block2\"):\n",
    "            out = self.block2(out)\n",
    "        with torch.autograd.profiler.record_function(\"block3\"):\n",
    "            out = self.block3(out)\n",
    "        with torch.autograd.profiler.record_function(\"head\"):\n",
    "            out = self.head(out)\n",
    "        with torch.autograd.profiler.record_function(\"classifier\"):\n",
    "            out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = EfficientNetB0().to(device)    \n",
    "batch_size = 8\n",
    "X = torch.randn(size=(batch_size, 3, 227, 227)).to(device) \n",
    "\n",
    "# Run the model and record the inference time of each layer\n",
    "for i in range(10):\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True) \n",
    "    model.forward(X) #warm up\n",
    "    with profiler:\n",
    "        output = model.forward(X)\n",
    "\n",
    "# Get the table of profiling results\n",
    "profiling_results4 = profiler.key_averages().table()\n",
    "print(profiling_results4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13621171",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bbfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.609 ms\n"
     ]
    }
   ],
   "source": [
    "# Find the index of \"Self CUDA time total\"\n",
    "start_index = profiling_results4.find(\"Self CUDA time total: \")\n",
    "if start_index != -1:\n",
    "    # Extract the substring starting from the index of the value\n",
    "    start_index += len(\"Self CUDA time total: \")\n",
    "    end_index = profiling_results4.find(\"ms\", start_index)\n",
    "    if end_index != -1:\n",
    "        # Extract the value as a float\n",
    "        self_cuda_time_total = float(profiling_results4[start_index:end_index].strip())\n",
    "        print( self_cuda_time_total, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90ecc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_EfficientNetB0.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>CUDA Time Avg (ms)</th>\n",
       "      <th>Power Measurements(W)</th>\n",
       "      <th>Energy(mJ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stem</td>\n",
       "      <td>0.592</td>\n",
       "      <td>52.930</td>\n",
       "      <td>8.838560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>block1</td>\n",
       "      <td>7.937</td>\n",
       "      <td>56.125</td>\n",
       "      <td>143.858125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>block2</td>\n",
       "      <td>6.546</td>\n",
       "      <td>50.783</td>\n",
       "      <td>83.677518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>block3</td>\n",
       "      <td>4.931</td>\n",
       "      <td>50.009</td>\n",
       "      <td>59.216379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>head</td>\n",
       "      <td>0.477</td>\n",
       "      <td>49.632</td>\n",
       "      <td>5.548464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>classifier</td>\n",
       "      <td>0.303</td>\n",
       "      <td>46.499</td>\n",
       "      <td>2.575197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer Name  CUDA Time Avg (ms)  Power Measurements(W)  Energy(mJ)\n",
       "0        Stem               0.592                 52.930    8.838560\n",
       "1      block1               7.937                 56.125  143.858125\n",
       "2      block2               6.546                 50.783   83.677518\n",
       "3      block3               4.931                 50.009   59.216379\n",
       "4        head               0.477                 49.632    5.548464\n",
       "5  classifier               0.303                 46.499    2.575197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results4.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['Stem','block1','block2','block3','head','classifier']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4, mean_power5]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "power_idle = 38 #depends on the gpu\n",
    "power_measurements_kernel = [power_measurement - power_idle for power_measurement in power_measurements]\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements_kernel[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file4 = 'profiler_results_EfficientNetB0.csv'\n",
    "with open(csv_file4, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file4}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df4 = pd.read_csv(csv_file4)\n",
    "\n",
    "# Display the table\n",
    "display(df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8891fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum of Energy</th>\n",
       "      <th>Self CUDA Time * X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303.714243</td>\n",
       "      <td>315.92358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sum of Energy  Self CUDA Time * X\n",
       "0     303.714243           315.92358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df4['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =average_above_90th - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f12057b",
   "metadata": {},
   "source": [
    "## Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0144146f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(csv_file4)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df['Layer Name'], df['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy(mJ)')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ccc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70050f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84bb16f3",
   "metadata": {},
   "source": [
    "# GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1020d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b1d43a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb791b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/daouda/.local/lib/python3.8/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/daouda/.local/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/daouda/.local/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/daouda/.local/lib/python3.8/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/daouda/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (0.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.7.4.3)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m737.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Installing collected packages: urllib3, packaging\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.7.1 requires importlib-resources>=3.2.0; python_version < \"3.10\", which is not installed.\n",
      "spyder 4.1.5 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.1.5 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "datasets 2.12.0 requires fsspec[http]>=2021.11.1, but you have fsspec 0.8.3 which is incompatible.\n",
      "datasets 2.12.0 requires tqdm>=4.62.1, but you have tqdm 4.50.2 which is incompatible.\n",
      "evaluate 0.4.0 requires fsspec[http]>=2021.05.0, but you have fsspec 0.8.3 which is incompatible.\n",
      "evaluate 0.4.0 requires tqdm>=4.62.1, but you have tqdm 4.50.2 which is incompatible.\n",
      "influxdb-client 1.36.1 requires urllib3>=1.26.0, but you have urllib3 1.25.11 which is incompatible.\n",
      "matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed packaging-23.1 urllib3-1.25.11\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce459e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch \n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe31dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokernizer\n",
    "GPT2_VARIANT = 'gpt2' # choices: gpt2 | gpt2-medium | gpt2-large | gpt2-xl\n",
    "config = GPT2Config(GPT2_VARIANT)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(GPT2_VARIANT, force_download = False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_VARIANT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2fd3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d379740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/gpt2/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(GPT2_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151596f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# carry out inference with a single sample\n",
    "input_str = \"Hello, my dog is \"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56b54193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -35.2362,  -35.3266,  -38.9753,  ...,  -44.4645,  -43.9974,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5832, -116.5725,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [ -88.7435,  -89.8644,  -93.1977,  ...,  -92.3839,  -96.1782,\n",
      "           -92.1273],\n",
      "         [ -85.1646,  -88.3380,  -92.8703,  ...,  -99.8017,  -94.7657,\n",
      "           -90.9330],\n",
      "         [-116.7281, -119.3950, -121.7259,  ..., -129.1003, -124.6102,\n",
      "          -121.6092],\n",
      "         [ -61.9848,  -63.7082,  -65.6898,  ...,  -76.0924,  -71.7897,\n",
      "           -66.1154]]]) torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'], use_cache = False)\n",
    "outputs\n",
    "logits = outputs.logits\n",
    "print(logits, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94a4df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv1d\n",
    "\n",
    "\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, resid_pdrop, attn_pdrop):\n",
    "        super().__init__()\n",
    "        self.c_attn = Conv1d(n_embd, 3*n_embd, 1, bias=False)\n",
    "        self.c_proj = Conv1d(n_embd, n_embd, 1, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
    "        self.n_head = n_head\n",
    "        self.split_size = n_embd // n_head\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        return x.view(x.size(0), self.n_head, self.split_size, -1).transpose(2, 3)\n",
    "        \n",
    "    def merge_heads(self, x):\n",
    "        return x.transpose(2, 3).contiguous().view(x.size(0), -1, self.n_head * self.split_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query, key, value = self.c_attn(x).chunk(3, dim=1)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        \n",
    "        attn_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "        attn_weights = attn_weights / (self.split_size ** 0.5)\n",
    "        attn_weights = nn.Softmax(dim=-1)(attn_weights)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        attn_output = self.merge_heads(attn_output)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "    \n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, n_embd, resid_pdrop, act):\n",
    "        super().__init__()\n",
    "        self.c_fc = Conv1d(n_embd, 4*n_embd, 1, bias=False)\n",
    "        self.c_proj = Conv1d(4*n_embd, n_embd, 1, bias=False)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(resid_pdrop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h = self.c_proj(h)\n",
    "        h = self.dropout(h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, resid_pdrop, attn_pdrop, act):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(n_embd, eps=1e-5)\n",
    "        self.attn = GPT2Attention(n_embd, n_head, resid_pdrop, attn_pdrop)\n",
    "        self.ln_2 = nn.LayerNorm(n_embd, eps=1e-5)\n",
    "        self.mlp = GPT2MLP(n_embd, resid_pdrop, act)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_embd, n_head, resid_pdrop, attn_pdrop, act):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "        self.wpe = nn.Embedding(1024, n_embd)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.h = nn.ModuleList([GPT2Block(n_embd, n_head, resid_pdrop, attn_pdrop, act) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd, eps=1e-5)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        input_embs = self.wte(input_ids)\n",
    "        position_embs = self.wpe(position_ids)\n",
    "        hidden_states = input_embs + position_embs\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states)\n",
    "        \n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_input_batch = [\n",
    "               \"Hello world How are you! I am aymen\"\n",
    "]\n",
    "big_input_batch = [\"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d44a6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotstarted\n",
      "52.97 W\n",
      "gotstarted\n",
      "52.25 W\n",
      "gotstarted\n",
      "54.30 W\n",
      "gotstarted\n",
      "50.54 W\n",
      "gotstarted\n",
      "48.98 W\n",
      "gotstarted\n",
      "46.17 W\n",
      "gotstarted\n",
      "49.08 W\n",
      "gotstarted\n",
      "48.51 W\n",
      "gotstarted\n",
      "47.64 W\n",
      "gotstarted\n",
      "43.66 W\n",
      "gotstarted\n",
      "51.50 W\n",
      "gotstarted\n",
      "44.21 W\n",
      "gotstarted\n",
      "51.40 W\n",
      "gotstarted\n",
      "50.46 W\n",
      "gotstarted\n",
      "47.62 W\n",
      "gotstarted\n",
      "51.70 W\n",
      "gotstarted\n",
      "56.47 W\n",
      "gotstarted\n",
      "51.77 W\n",
      "gotstarted\n",
      "48.22 W\n",
      "gotstarted\n",
      "46.25 W\n",
      "torch.Size([1, 6, 768])\n",
      "torch.Size([1, 6, 768])\n",
      "Mean power0:  49.685\n",
      "Average of values >= __th percentile: 55.385\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config\n",
    "import torch\n",
    "config = GPT2Config(vocab_size=50257, n_layer=12, n_embd=768, n_head=12, resid_pdrop=0.1, attn_pdrop=0.1, act='gelu')\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model(config=config).to(device)\n",
    "\n",
    "inputs = tokenizer(big_input_batch, return_tensors=\"pt\").to(device)\n",
    "\n",
    "power_measurement =[]\n",
    "\n",
    "# Signal the start of the measurement \n",
    "for i in range(20):\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "\n",
    "    \n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    outputs = model.forward(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    last_hidden_states.shape\n",
    "    q.put('stop')\n",
    "    p.join()  \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurement.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurement)\n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurement)\n",
    "print(\"Mean power0: \", mean_power)\n",
    "percentile_90 = np.percentile(power_measurement, 90)\n",
    "average_above_90th = np.mean(np.array(power_measurement)[np.array(power_measurement) >= percentile_90])\n",
    "print('Average of values >= __th percentile:', average_above_90th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0049fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9149, -1.2219, -1.0224,  ..., -0.0561,  0.9457, -0.1336],\n",
      "         [ 0.7219, -0.2491, -0.2442,  ...,  0.4293, -0.4624, -0.4813],\n",
      "         [-0.5151,  1.4238, -0.6448,  ...,  0.0541,  0.7558, -0.7160],\n",
      "         ...,\n",
      "         [-0.3299,  0.4869,  0.3631,  ...,  0.8886,  0.3294, -0.0064],\n",
      "         [-0.0970, -0.3928, -0.7437,  ...,  1.0757, -0.9373,  0.8117],\n",
      "         [ 0.7028,  0.0504, -0.9948,  ..., -0.5414,  0.2818,  2.0515]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>) torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "config = GPT2Config(vocab_size=50257, n_layer=12, n_embd=768, n_head=12, resid_pdrop=0.1, attn_pdrop=0.1, act='gelu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model(config=config).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"Hello, I'm a language model, and also have more\", return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "\n",
    "input_embs = model.wte(input_ids)\n",
    "position_embs = model.wpe(position_ids)\n",
    "hidden_states = input_embs + position_embs\n",
    "hidden_states = model.drop(hidden_states)\n",
    "\n",
    "for block in model.h:\n",
    "    hidden_states = block(hidden_states)[0]\n",
    "    \n",
    "hidden_states = model.ln_f(hidden_states)\n",
    "print(hidden_states,hidden_states.shape)\n",
    "#outputs = model.forward(**inputs)\n",
    "#print(outputs)\n",
    "#last_hidden_states = outputs.last_hidden_state\n",
    "#last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da9ef6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ecfff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.32 W\n",
      "38.51 W\n",
      "38.51 W\n",
      "43.36 W\n",
      "39.08 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "39.19 W\n",
      "38.99 W\n",
      "65.67 W\n",
      "42.09 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "42.00 W\n",
      "42.19 W\n",
      "42.11 W\n",
      "50.05 W\n",
      "43.17 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.18 W\n",
      "43.25 W\n",
      "43.16 W\n",
      "62.38 W\n",
      "43.74 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.64 W\n",
      "43.93 W\n",
      "43.65 W\n",
      "54.34 W\n",
      "43.85 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.65 W\n",
      "43.83 W\n",
      "43.64 W\n",
      "52.48 W\n",
      "43.84 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.64 W\n",
      "43.93 W\n",
      "43.86 W\n",
      "56.09 W\n",
      "43.93 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.75 W\n",
      "43.93 W\n",
      "43.85 W\n",
      "49.97 W\n",
      "43.84 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.83 W\n",
      "43.93 W\n",
      "43.84 W\n",
      "59.16 W\n",
      "43.93 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "43.83 W\n",
      "43.93 W\n",
      "43.84 W\n",
      "48.12 W\n",
      "43.94 W\n",
      "torch.Size([1, 6, 768])\n",
      "Mean power:  42.473\n",
      "Mean power1:  42.662\n",
      "Mean power2:  42.545\n",
      "Mean power3:  54.162\n",
      "Mean power4:  43.141\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "config = GPT2Config(vocab_size=50257, n_layer=12, n_embd=768, n_head=12, resid_pdrop=0.1, attn_pdrop=0.1, act='gelu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model(config=config).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(big_input_batch, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "model.train()\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = [] \n",
    "power_measurements4 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#outputs = model(**inputs)\n",
    "#print(outputs)\n",
    "#last_hidden_states = outputs.last_hidden_state\n",
    "#last_hidden_states\n",
    "for i in range(10):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue() \n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue()\n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue() \n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue()\n",
    "    \n",
    "\n",
    "    # Signal the start of the measurement\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "\n",
    "    # Run the inference########################\n",
    "    #output = model.forward(X)\n",
    "    #x = X.clone()\n",
    "    input_embs = model.wte(input_ids)\n",
    "    q.put('stop')\n",
    "    p.join()\n",
    "    \n",
    "\n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'   \n",
    "    for i in range(10):\n",
    "        position_embs = model.wpe(position_ids)\n",
    "    q1.put('stop')\n",
    "    p1.join()\n",
    "\n",
    "    hidden_states = input_embs + position_embs\n",
    "\n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        hidden_states = model.drop(hidden_states)\n",
    "    q2.put('stop')\n",
    "    p2.join()\n",
    "\n",
    "\n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    for i in range(10):\n",
    "        for block in model.h:\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "    q3.put('stop')\n",
    "    p3.join()\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    for i in range(10):\n",
    "        hidden_states = model.ln_f(hidden_states)\n",
    "    q4.put('stop')\n",
    "    p4.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "        \n",
    "\n",
    "\n",
    "print(output.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"Mean power1: \", mean_power1)\n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"Mean power2: \", mean_power2)\n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"Mean power3: \", mean_power3) \n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"Mean power4: \", mean_power4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f95e33c",
   "metadata": {},
   "source": [
    "## Profiling Tine - GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b888fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "fast path:\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             embbedding         0.43%     157.000us         1.01%     364.000us     364.000us     159.000us         0.44%     370.000us     370.000us             1  \n",
      "                                        aten::embedding         0.23%      83.000us         0.85%     307.000us     153.500us      59.000us         0.16%     315.000us     157.500us             2  \n",
      "                                          aten::reshape         1.85%     670.000us         2.10%     759.000us      15.180us     648.000us         1.79%     977.000us      19.540us            50  \n",
      "                                   aten::_reshape_alias         0.25%      89.000us         0.25%      89.000us       1.780us     329.000us         0.91%     329.000us       6.580us            50  \n",
      "                                     aten::index_select         0.27%      96.000us         0.49%     178.000us      89.000us     145.000us         0.40%     198.000us      99.000us             2  \n",
      "                                            aten::empty         2.27%     823.000us         2.27%     823.000us       4.757us       1.673ms         4.62%       1.673ms       9.671us           173  \n",
      "                                          aten::resize_         0.20%      72.000us         0.20%      72.000us       5.143us     138.000us         0.38%     138.000us       9.857us            14  \n",
      "                                       cudaLaunchKernel         4.95%       1.791ms         4.95%       1.791ms       4.907us       0.000us         0.00%       0.000us       0.000us           365  \n",
      "                                             aten::view         0.95%     344.000us         0.95%     344.000us       1.755us       1.268ms         3.50%       1.268ms       6.469us           196  \n",
      "                                                    wpe         0.15%      56.000us         0.43%     156.000us     156.000us      56.000us         0.15%     160.000us     160.000us             1  \n",
      "                                                   drop         0.20%      74.000us         0.63%     227.000us     227.000us      68.000us         0.19%     231.000us     231.000us             1  \n",
      "                                              aten::add         1.58%     573.000us         2.12%     766.000us      15.633us       1.020ms         2.82%       1.020ms      20.816us            49  \n",
      "                                          aten::dropout         1.53%     555.000us         9.36%       3.389ms      91.595us     518.000us         1.43%       3.548ms      95.892us            37  \n",
      "                                   aten::native_dropout         3.70%       1.340ms         7.83%       2.834ms      76.595us       1.390ms         3.84%       3.030ms      81.892us            37  \n",
      "                                       aten::empty_like         3.03%       1.096ms         4.25%       1.540ms      17.907us       1.037ms         2.86%       1.922ms      22.349us            86  \n",
      "                                    aten::empty_strided         1.22%     442.000us         1.22%     442.000us       5.140us     869.000us         2.40%     869.000us      10.105us            86  \n",
      "                                  cudaStreamIsCapturing         0.03%      11.000us         0.03%      11.000us       0.297us       0.000us         0.00%       0.000us       0.000us            37  \n",
      "                                                  block        36.17%      13.095ms        97.35%      35.250ms      35.250ms       9.566ms        26.41%      35.257ms      35.257ms             1  \n",
      "                                       aten::layer_norm         0.96%     346.000us         7.41%       2.683ms     107.320us     340.000us         0.94%       2.793ms     111.720us            25  \n",
      "                                aten::native_layer_norm         5.05%       1.827ms         6.45%       2.337ms      93.480us       1.434ms         3.96%       2.453ms      98.120us            25  \n",
      "                                            aten::addmm         6.54%       2.369ms         8.80%       3.185ms      66.354us       3.421ms         9.45%       3.906ms      81.375us            48  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.25%      89.000us         0.25%      89.000us       0.674us       0.000us         0.00%       0.000us       0.000us           132  \n",
      "                                            aten::split         1.33%     481.000us         3.84%       1.390ms     115.833us     361.000us         1.00%       1.447ms     120.583us            12  \n",
      "                                           aten::narrow         1.22%     440.000us         2.51%     909.000us      25.250us     450.000us         1.24%       1.086ms      30.167us            36  \n",
      "                                            aten::slice         3.03%       1.097ms         3.14%       1.136ms      13.524us       1.085ms         3.00%       1.537ms      18.298us            84  \n",
      "                                       aten::as_strided         0.34%     123.000us         0.34%     123.000us       0.641us       1.070ms         2.95%       1.070ms       5.573us           192  \n",
      "                                          aten::permute         1.82%     659.000us         1.90%     687.000us      14.312us     644.000us         1.78%     911.000us      18.979us            48  \n",
      "                                        aten::transpose         0.51%     185.000us         0.53%     191.000us      15.917us     189.000us         0.52%     258.000us      21.500us            12  \n",
      "                                           aten::matmul         4.94%       1.788ms        11.03%       3.994ms     166.417us       1.216ms         3.36%       4.110ms     171.250us            24  \n",
      "                                           aten::expand         1.82%     658.000us         1.96%     708.000us      14.750us     653.000us         1.80%     935.000us      19.479us            48  \n",
      "                                              aten::bmm         1.64%     593.000us         2.06%     746.000us      31.083us     880.000us         2.43%     880.000us      36.667us            24  \n",
      "                                     aten::_unsafe_view         0.10%      36.000us         0.10%      36.000us       1.500us     152.000us         0.42%     152.000us       6.333us            24  \n",
      "                                             aten::full         1.61%     583.000us         2.36%     854.000us      35.583us     526.000us         1.45%       1.031ms      42.958us            24  \n",
      "                                            aten::fill_         0.28%     103.000us         0.49%     177.000us       7.375us     302.000us         0.83%     302.000us      12.583us            24  \n",
      "                                              aten::div         0.58%     209.000us         0.71%     258.000us      21.500us     322.000us         0.89%     322.000us      26.833us            12  \n",
      "                                               aten::to         0.38%     137.000us         2.17%     787.000us      16.396us     296.000us         0.82%     998.000us      20.792us            48  \n",
      "                                         aten::_to_copy         0.81%     293.000us         1.80%     650.000us      54.167us     236.000us         0.65%     702.000us      58.500us            12  \n",
      "                                            aten::copy_         0.57%     208.000us         1.33%     481.000us      20.042us     601.000us         1.66%     601.000us      25.042us            24  \n",
      "                                        cudaMemcpyAsync         0.39%     141.000us         0.39%     141.000us      11.750us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                  cudaStreamSynchronize         0.20%      73.000us         0.20%      73.000us       6.083us       0.000us         0.00%       0.000us       0.000us            12  \n",
      "                                            aten::where         1.12%     405.000us         1.56%     566.000us      47.167us     436.000us         1.20%     627.000us      52.250us            12  \n",
      "                                          aten::softmax         0.42%     151.000us         0.91%     328.000us      27.333us     129.000us         0.36%     373.000us      31.083us            12  \n",
      "                                         aten::_softmax         0.36%     129.000us         0.49%     177.000us      14.750us     244.000us         0.67%     244.000us      20.333us            12  \n",
      "                                       aten::contiguous         0.39%     142.000us         2.33%     842.000us      70.167us     139.000us         0.38%     888.000us      74.000us            12  \n",
      "                                            aten::clone         0.78%     284.000us         1.93%     700.000us      58.333us     220.000us         0.61%     749.000us      62.417us            12  \n",
      "                                              aten::mul         1.62%     585.000us         2.21%     802.000us      16.708us       1.052ms         2.90%       1.052ms      21.917us            48  \n",
      "                                              aten::pow         1.36%     493.000us         1.54%     556.000us      46.333us     491.000us         1.36%     624.000us      52.000us            12  \n",
      "                                      aten::result_type         0.00%       0.000us         0.00%       0.000us       0.000us      71.000us         0.20%      71.000us       5.917us            12  \n",
      "                                             aten::tanh         0.35%     126.000us         0.49%     177.000us      14.750us     243.000us         0.67%     243.000us      20.250us            12  \n",
      "                                                   ln_f         0.20%      72.000us         0.54%     195.000us     195.000us      72.000us         0.20%     200.000us     200.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.04%      16.000us         0.04%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 36.208ms\n",
      "Self CUDA time total: 36.218ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 12:38:43 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:38:43 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:38:43 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "config = GPT2Config(vocab_size=50257, n_layer=12, n_embd=768, n_head=12, resid_pdrop=0.1, attn_pdrop=0.1, act='gelu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model(config=config).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\", return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "ITERATIONS = 10 \n",
    "def forward(model ,input_ids):\n",
    "    with torch.autograd.profiler.record_function(\"embbedding\"):\n",
    "        input_embs = model.wte(input_ids)\n",
    "    with torch.autograd.profiler.record_function(\"wpe\"):\n",
    "        position_embs = model.wpe(position_ids)\n",
    "    with torch.autograd.profiler.record_function(\"drop\"):\n",
    "        hidden_states = input_embs + position_embs\n",
    "        hidden_states = model.drop(hidden_states)\n",
    "\n",
    "    with torch.autograd.profiler.record_function(\"block\"):\n",
    "        for block in model.h:\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "    with torch.autograd.profiler.record_function(\"ln_f\"):\n",
    "        hidden_states = model.ln_f(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "for i in range(ITERATIONS):\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True) \n",
    "    with profiler:\n",
    "        with torch.no_grad():\n",
    "            output = forward(model,input_ids)\n",
    "profiling_results5 = profiler.key_averages().table()\n",
    "print(profiling_results5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da440855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.345 ms\n"
     ]
    }
   ],
   "source": [
    "# Find the index of \"Self CUDA time total\"\n",
    "start_index = profiling_results5.find(\"Self CUDA time total: \")\n",
    "if start_index != -1:\n",
    "    # Extract the substring starting from the index of the value\n",
    "    start_index += len(\"Self CUDA time total: \")\n",
    "    end_index = profiling_results5.find(\"ms\", start_index)\n",
    "    if end_index != -1:\n",
    "        # Extract the value as a float\n",
    "        self_cuda_time_total = float(profiling_results5[start_index:end_index].strip())\n",
    "        print( self_cuda_time_total, \"ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406e50ef",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c7cdb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_EfficientNetB0.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>CUDA Time Avg (ms)</th>\n",
       "      <th>Power Measurements(W)</th>\n",
       "      <th>Energy(mJ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embbedding</td>\n",
       "      <td>0.332</td>\n",
       "      <td>49.685</td>\n",
       "      <td>3.879420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wpe</td>\n",
       "      <td>0.122</td>\n",
       "      <td>42.662</td>\n",
       "      <td>0.568764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drop</td>\n",
       "      <td>0.222</td>\n",
       "      <td>42.545</td>\n",
       "      <td>1.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>block</td>\n",
       "      <td>32.529</td>\n",
       "      <td>54.162</td>\n",
       "      <td>525.733698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ln_f</td>\n",
       "      <td>0.140</td>\n",
       "      <td>43.141</td>\n",
       "      <td>0.719740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer Name  CUDA Time Avg (ms)  Power Measurements(W)  Energy(mJ)\n",
       "0  embbedding               0.332                 49.685    3.879420\n",
       "1         wpe               0.122                 42.662    0.568764\n",
       "2        drop               0.222                 42.545    1.008990\n",
       "3       block              32.529                 54.162  525.733698\n",
       "4        ln_f               0.140                 43.141    0.719740"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results5.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['embbedding','wpe','drop','block','ln_f']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "power_idle = 38 #depends on the gpu\n",
    "power_measurements_kernel = [power_measurement - power_idle for power_measurement in power_measurements]\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements_kernel[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file5 = 'profiler_results_EfficientNetB0.csv'\n",
    "with open(csv_file5, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file5}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df5 = pd.read_csv(csv_file5)\n",
    "\n",
    "# Display the table\n",
    "display(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d1d7742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum of Energy</th>\n",
       "      <th>Self CUDA Time * X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>531.910612</td>\n",
       "      <td>580.53645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sum of Energy  Self CUDA Time * X\n",
       "0     531.910612           580.53645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df5['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X = 55.41 - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "409e8490",
   "metadata": {},
   "source": [
    "## Bar PLot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d17acee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df5 = pd.read_csv(csv_file5)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df5['Layer Name'], df5['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "224ea2b3",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "We download the XLM-R model from the pre-defined torchtext models by following the instructions in torchtext.models. We also set the DEVICE to execute on-accelerator tests. (Enable GPU execution for your environment as appropriate.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e3fdee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1+cu117\n",
      "torch cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
    "\n",
    "import torch, torchtext\n",
    "from torchtext.models import RobertaClassificationHead\n",
    "from torchtext.functional import to_tensor\n",
    "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
    "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
    "model = xlmr_large.get_model(head=classifier_head)\n",
    "transform = xlmr_large.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7a44d125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (token_embedding): Embedding(250002, 1024, padding_idx=1)\n",
       "      (layers): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (positional_embedding): PositionalEmbedding(\n",
       "        (embedding): Embedding(514, 1024, padding_idx=1)\n",
       "      )\n",
       "      (embedding_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (activation_fn): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5fd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_input_batch = [\n",
    "               \"Hello world\",\n",
    "               \"How are you!\",\"I am aymen\"\n",
    "]\n",
    "big_input_batch = [\"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e3412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_input_string = \" \".join(big_input_batch)\n",
    "\n",
    "# Split the string into words and calculate the number of words\n",
    "number_of_words = len(big_input_string.split()) \n",
    "number_of_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68d60dbb",
   "metadata": {},
   "source": [
    "## Power Measurement _ Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f2b77cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "44.61 W\n",
      "122.71 W\n",
      "112.00 W\n",
      "148.07 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "107.21 W\n",
      "116.66 W\n",
      "105.86 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "105.37 W\n",
      "89.93 W\n",
      "113.21 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "108.68 W\n",
      "115.62 W\n",
      "118.66 W\n",
      "38.80 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "79.85 W\n",
      "116.35 W\n",
      "97.38 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "84.52 W\n",
      "91.34 W\n",
      "97.56 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.99 W\n",
      "61.30 W\n",
      "92.10 W\n",
      "99.54 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.99 W\n",
      "84.15 W\n",
      "92.37 W\n",
      "94.71 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.98 W\n",
      "115.42 W\n",
      "97.51 W\n",
      "86.29 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "70.10 W\n",
      "99.53 W\n",
      "114.22 W\n",
      "38.80 W\n",
      "torch.Size([3, 2])\n",
      "Mean power:  39.491\n",
      "Mean power1:  93.931\n",
      "Mean power2:  102.341\n",
      "Mean power:  107.55\n",
      "Mean power1:  38.822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store power measurements\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def measure(q, rq, gpu_id):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=2 -i {gpu_id}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "input_batch=big_input_batch\n",
    "model.to(DEVICE)\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1).to(DEVICE)\n",
    "model.train()\n",
    "power_measurements = [] \n",
    "power_measurements1 = [] \n",
    "power_measurements2 = [] \n",
    "power_measurements3 = [] \n",
    "power_measurements4 = [] \n",
    "power_measurements5 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Start the power measurement\n",
    "    q = multiprocessing.Queue()\n",
    "    rq = multiprocessing.Queue()\n",
    "    q1 = multiprocessing.Queue()\n",
    "    rq1 = multiprocessing.Queue() \n",
    "    q2 = multiprocessing.Queue()\n",
    "    rq2 = multiprocessing.Queue()\n",
    "    q3 = multiprocessing.Queue()\n",
    "    rq3 = multiprocessing.Queue()\n",
    "    q4 = multiprocessing.Queue()\n",
    "    rq4 = multiprocessing.Queue() \n",
    "    q5 = multiprocessing.Queue()\n",
    "    rq5 = multiprocessing.Queue()\n",
    "\n",
    "\n",
    "\n",
    "    p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "    p.start()\n",
    "    q.put('start')\n",
    "    m = rq.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    output = model.encoder.transformer.token_embedding(model_input) \n",
    "    q.put('stop') \n",
    "    p.join() \n",
    "\n",
    "\n",
    "    p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "    p1.start()\n",
    "    q1.put('start')\n",
    "    m = rq1.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    layer1 = model.encoder.transformer.layers.layers[0]\n",
    "    output = layer1(output)\n",
    "    layer2 = model.encoder.transformer.layers.layers[1]\n",
    "    output = layer2(output)\n",
    "    layer3 = model.encoder.transformer.layers.layers[2]\n",
    "    output = layer3(output)\n",
    "    layer4 = model.encoder.transformer.layers.layers[3]\n",
    "    output = layer4(output)\n",
    "    layer5 = model.encoder.transformer.layers.layers[4]\n",
    "    output = layer5(output)\n",
    "    layer6 = model.encoder.transformer.layers.layers[5]\n",
    "    output = layer6(output)\n",
    "    layer7 = model.encoder.transformer.layers.layers[6]\n",
    "    output = layer7(output)\n",
    "    q1.put('stop') \n",
    "\n",
    "    p1.join()\n",
    "\n",
    "##########################################################################Block2#############################\n",
    "    p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "    p2.start()\n",
    "    q2.put('start')\n",
    "    m = rq2.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    layer8 = model.encoder.transformer.layers.layers[7]\n",
    "    output = layer8(output)\n",
    "    layer9 = model.encoder.transformer.layers.layers[8]\n",
    "    output = layer9(output)\n",
    "    layer10 = model.encoder.transformer.layers.layers[9]\n",
    "    output = layer10(output)\n",
    "    layer11 = model.encoder.transformer.layers.layers[10]\n",
    "    output = layer11(output) \n",
    "    layer12 = model.encoder.transformer.layers.layers[11]\n",
    "    output = layer12(output)\n",
    "    layer13 = model.encoder.transformer.layers.layers[12]\n",
    "    output = layer13(output)\n",
    "    layer14 = model.encoder.transformer.layers.layers[13]\n",
    "    output = layer14(output)\n",
    "    layer15 = model.encoder.transformer.layers.layers[14]\n",
    "    output = layer15(output)\n",
    "\n",
    "    q2.put('stop') \n",
    "    p2.join()\n",
    "##########################################################################Block3#############################\n",
    "    p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "    p3.start()\n",
    "    q3.put('start')\n",
    "    m = rq3.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started' \n",
    "    layer16 = model.encoder.transformer.layers.layers[15]\n",
    "    output = layer16(output)\n",
    "    layer17 = model.encoder.transformer.layers.layers[16]\n",
    "    output = layer17(output)\n",
    "    layer18 = model.encoder.transformer.layers.layers[17]\n",
    "    output = layer18(output)\n",
    "    layer19 = model.encoder.transformer.layers.layers[18]\n",
    "    output = layer19(output)\n",
    "    layer20 = model.encoder.transformer.layers.layers[19]\n",
    "    output = layer20(output)\n",
    "    layer21 = model.encoder.transformer.layers.layers[20]\n",
    "    output = layer21(output)\n",
    "    layer22 = model.encoder.transformer.layers.layers[21]\n",
    "    output = layer22(output)\n",
    "    layer23 = model.encoder.transformer.layers.layers[22]\n",
    "    output = layer23(output)\n",
    "    q3.put('stop') \n",
    "    p3.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################Block3#############################\n",
    "\n",
    "\n",
    "    p4 = multiprocessing.Process(target=measure, args=(q4,rq4,1))\n",
    "    p4.start()\n",
    "    q4.put('start')\n",
    "    m = rq4.get()\n",
    "    print('got' + m)\n",
    "    assert m == 'started'               \n",
    "    #x = model.maxpool(x)\n",
    "    output = model.head(output)\n",
    "    q4.put('stop')\n",
    "    \n",
    "\n",
    "    x = output\n",
    "\n",
    "    p4.join()\n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq.empty():\n",
    "        power_output = rq.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq1.empty():\n",
    "        power_output = rq1.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements1.append(float(power_output.split()[0]))  # Remov \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq2.empty():\n",
    "        power_output = rq2.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements2.append(float(power_output.split()[0]))  # Remov\n",
    "\n",
    "    while not rq3.empty():\n",
    "        power_output = rq3.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float \n",
    "\n",
    "    # Retrieve the power measurements from the queue\n",
    "    while not rq4.empty():\n",
    "        power_output = rq4.get()\n",
    "        if power_output == 'stop':\n",
    "            break\n",
    "        print(power_output)\n",
    "        power_measurements4.append(float(power_output.split()[0]))  # Remov \n",
    "\n",
    "\n",
    "\n",
    "print(x.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"Mean power1: \", mean_power1) \n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"Mean power2: \", mean_power2) \n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"Mean power: \", mean_power3) \n",
    "mean_power4 = np.mean(power_measurements4)\n",
    "print(\"Mean power1: \", mean_power4) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5faadb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "input_batch=big_input_batch\n",
    "model.to(DEVICE)\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1).to(DEVICE)\n",
    "output = model.encoder.transformer.token_embedding(model_input)\n",
    "#for layer in model.encoder.transformer.layers.layers:\n",
    "    #output = layer(output) \n",
    "layer1 = model.encoder.transformer.layers.layers[0]\n",
    "output = layer1(output)\n",
    "layer2 = model.encoder.transformer.layers.layers[1]\n",
    "output = layer2(output)\n",
    "layer3 = model.encoder.transformer.layers.layers[2]\n",
    "output = layer3(output)\n",
    "layer4 = model.encoder.transformer.layers.layers[3]\n",
    "output = layer4(output)\n",
    "layer5 = model.encoder.transformer.layers.layers[4]\n",
    "output = layer5(output)\n",
    "layer6 = model.encoder.transformer.layers.layers[5]\n",
    "output = layer6(output)\n",
    "layer7 = model.encoder.transformer.layers.layers[6]\n",
    "output = layer7(output)\n",
    "layer8 = model.encoder.transformer.layers.layers[7]\n",
    "output = layer8(output)\n",
    "layer9 = model.encoder.transformer.layers.layers[8]\n",
    "output = layer9(output)\n",
    "layer10 = model.encoder.transformer.layers.layers[9]\n",
    "output = layer10(output)\n",
    "layer11 = model.encoder.transformer.layers.layers[10]\n",
    "output = layer11(output) \n",
    "layer12 = model.encoder.transformer.layers.layers[11]\n",
    "output = layer12(output)\n",
    "layer13 = model.encoder.transformer.layers.layers[12]\n",
    "output = layer13(output)\n",
    "layer14 = model.encoder.transformer.layers.layers[13]\n",
    "output = layer14(output)\n",
    "layer15 = model.encoder.transformer.layers.layers[14]\n",
    "output = layer15(output)\n",
    "layer16 = model.encoder.transformer.layers.layers[15]\n",
    "output = layer16(output)\n",
    "layer17 = model.encoder.transformer.layers.layers[16]\n",
    "output = layer17(output)\n",
    "layer18 = model.encoder.transformer.layers.layers[17]\n",
    "output = layer18(output)\n",
    "layer19 = model.encoder.transformer.layers.layers[18]\n",
    "output = layer19(output)\n",
    "layer20 = model.encoder.transformer.layers.layers[19]\n",
    "output = layer20(output)\n",
    "layer21 = model.encoder.transformer.layers.layers[20]\n",
    "output = layer21(output)\n",
    "layer22 = model.encoder.transformer.layers.layers[21]\n",
    "output = layer22(output)\n",
    "\n",
    "layer23 = model.encoder.transformer.layers.layers[22]\n",
    "output = layer23(output)\n",
    "\n",
    "output = model.head(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa248782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 1: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 2: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 3: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 4: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 5: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 6: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 7: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 8: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 9: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 10: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 11: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 12: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 13: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 14: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 15: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 16: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 17: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 18: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 19: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 20: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 21: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 22: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Layer 23: TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_input = to_tensor(transform(input_batch), padding_value=1).to(DEVICE)\n",
    "output = model.encoder.transformer.token_embedding(model_input)\n",
    "for idx, layer in enumerate(model.encoder.transformer.layers.layers):\n",
    "    print(f\"Layer {idx}: {layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b217fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (token_embedding): Embedding(250002, 1024, padding_idx=1)\n",
       "      (layers): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (positional_embedding): PositionalEmbedding(\n",
       "        (embedding): Embedding(514, 1024, padding_idx=1)\n",
       "      )\n",
       "      (embedding_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (activation_fn): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac272168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e3f6a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast path:\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-21 14:50:11 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:12 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:12 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:12 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:12 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:12 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:13 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:14 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-21 14:50:15 27936:27936 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             embbedding         0.23%     150.000us         0.48%     315.000us     315.000us     152.000us         0.23%     321.000us     321.000us             1  \n",
      "                                        aten::embedding         0.07%      49.000us         0.25%     165.000us     165.000us      32.000us         0.05%     169.000us     169.000us             1  \n",
      "                                          aten::reshape         2.39%       1.569ms         4.86%       3.191ms      27.509us     660.000us         1.00%       1.471ms      12.681us           116  \n",
      "                                   aten::_reshape_alias         0.24%     157.000us         0.24%     157.000us       1.688us     170.000us         0.26%     170.000us       1.828us            93  \n",
      "                                     aten::index_select         0.07%      45.000us         0.13%      88.000us      88.000us      72.000us         0.11%     101.000us     101.000us             1  \n",
      "                                            aten::empty         1.97%       1.293ms         1.97%       1.293ms       4.634us     591.000us         0.90%     591.000us       2.118us           279  \n",
      "                                          aten::resize_         0.02%      13.000us         0.02%      13.000us      13.000us      17.000us         0.03%      17.000us      17.000us             1  \n",
      "                                       cudaLaunchKernel         4.02%       2.640ms         4.02%       2.640ms       5.156us       0.000us         0.00%       0.000us       0.000us           512  \n",
      "                                             aten::view         0.93%     611.000us         0.93%     611.000us       1.559us     590.000us         0.90%     590.000us       1.505us           392  \n",
      "                                              block 1-7         7.72%       5.065ms        30.61%      20.095ms      20.095ms       1.014ms         1.54%      20.717ms      20.717ms             1  \n",
      "                                        aten::transpose         4.95%       3.247ms         5.37%       3.526ms      13.827us       1.207ms         1.83%       1.575ms       6.176us           255  \n",
      "                                       aten::as_strided         0.62%     409.000us         0.62%     409.000us       0.838us     650.000us         0.99%     650.000us       1.332us           488  \n",
      "                                           aten::linear         5.39%       3.537ms        22.38%      14.692ms     156.298us       1.114ms         1.69%      42.908ms     456.468us            94  \n",
      "                                                aten::t         1.77%       1.162ms         3.66%       2.401ms      25.543us     501.000us         0.76%       1.167ms      12.415us            94  \n",
      "                                           aten::matmul         5.72%       3.752ms        15.83%      10.392ms     150.609us       1.144ms         1.74%      15.919ms     230.710us            69  \n",
      "                                            aten::clone         2.38%       1.564ms         5.84%       3.832ms      55.536us     522.000us         0.79%       2.148ms      31.130us            69  \n",
      "                                       aten::empty_like         4.46%       2.926ms         6.36%       4.177ms      16.253us       1.114ms         1.69%       1.470ms       5.720us           257  \n",
      "                                            aten::copy_         1.16%     762.000us         1.69%       1.108ms      16.058us       1.190ms         1.81%       1.190ms      17.246us            69  \n",
      "                                     aten::_unsafe_view         0.18%     119.000us         0.18%     119.000us       1.293us     125.000us         0.19%     125.000us       1.359us            92  \n",
      "                                               aten::mm         1.14%     749.000us         1.39%     910.000us      39.565us       9.770ms        14.83%       9.770ms     424.783us            23  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.21%     141.000us         0.21%     141.000us       0.306us       0.000us         0.00%       0.000us       0.000us           461  \n",
      "                                             aten::add_         0.40%     263.000us         0.53%     350.000us      15.217us     616.000us         0.94%     616.000us      26.783us            23  \n",
      "                                        aten::unflatten         0.43%     285.000us         0.53%     348.000us      15.130us      97.000us         0.15%     126.000us       5.478us            23  \n",
      "                                        aten::unsqueeze         0.47%     306.000us         0.51%     332.000us      14.435us      92.000us         0.14%     119.000us       5.174us            23  \n",
      "                                          aten::squeeze         0.47%     306.000us         0.49%     320.000us      13.913us      95.000us         0.14%     118.000us       5.130us            23  \n",
      "                                       aten::contiguous         0.69%     455.000us         4.36%       2.859ms      62.152us     205.000us         0.31%       1.751ms      38.065us            46  \n",
      "                                           aten::select         1.41%     925.000us         1.46%     960.000us      13.714us     285.000us         0.43%     365.000us       5.214us            70  \n",
      "                     aten::scaled_dot_product_attention         0.58%     381.000us        19.51%      12.804ms     556.696us     106.000us         0.16%       9.000ms     391.304us            23  \n",
      "               aten::_scaled_dot_product_attention_math         2.66%       1.743ms        18.93%      12.423ms     540.130us     678.000us         1.03%       8.894ms     386.696us            23  \n",
      "                                              aten::div         1.98%       1.302ms         2.31%       1.519ms      33.022us     581.000us         0.88%     581.000us      12.630us            46  \n",
      "                                           aten::expand         1.71%       1.125ms         1.77%       1.164ms      12.652us     448.000us         0.68%     572.000us       6.217us            92  \n",
      "                                              aten::bmm         1.68%       1.106ms         2.01%       1.320ms      28.696us       2.907ms         4.41%       2.907ms      63.196us            46  \n",
      "                                          aten::softmax         0.37%     244.000us         0.83%     543.000us      23.609us      89.000us         0.14%     996.000us      43.304us            23  \n",
      "                                         aten::_softmax         0.33%     215.000us         0.46%     299.000us      13.000us     907.000us         1.38%     907.000us      39.435us            23  \n",
      "                                          aten::dropout         1.84%       1.206ms        11.86%       7.783ms      82.798us     396.000us         0.60%       4.418ms      47.000us            94  \n",
      "                                   aten::native_dropout         4.75%       3.121ms        10.02%       6.577ms      69.968us       2.988ms         4.54%       4.022ms      42.787us            94  \n",
      "                                    aten::empty_strided         1.34%     882.000us         1.34%     882.000us       4.691us     239.000us         0.36%     239.000us       1.271us           188  \n",
      "                                  cudaStreamIsCapturing         0.02%      13.000us         0.02%      13.000us       0.138us       0.000us         0.00%       0.000us       0.000us            94  \n",
      "                                          aten::permute         0.45%     295.000us         0.47%     310.000us      13.478us     104.000us         0.16%     130.000us       5.652us            23  \n",
      "                                            aten::addmm         4.68%       3.073ms         6.71%       4.402ms      62.000us      28.821ms        43.75%      29.005ms     408.521us            71  \n",
      "                                        cudaMemsetAsync         0.46%     299.000us         0.46%     299.000us       4.333us       0.000us         0.00%       0.000us       0.000us            69  \n",
      "                                              aten::add         0.94%     614.000us         1.21%     797.000us      17.326us     676.000us         1.03%     676.000us      14.696us            46  \n",
      "                                       aten::layer_norm         0.89%     585.000us         6.81%       4.473ms      97.239us     240.000us         0.36%       2.062ms      44.826us            46  \n",
      "                                aten::native_layer_norm         4.58%       3.008ms         5.92%       3.888ms      84.522us       1.389ms         2.11%       1.822ms      39.609us            46  \n",
      "                                             aten::gelu         0.48%     318.000us         0.65%     426.000us      18.522us     779.000us         1.18%     779.000us      33.870us            23  \n",
      "                                           block 8 - 15        10.56%       6.929ms        35.19%      23.101ms      23.101ms       1.703ms         2.58%      23.406ms      23.406ms             1  \n",
      "                                           block16 - 23         8.03%       5.270ms        30.93%      20.304ms      20.304ms     768.000us         1.17%      21.269ms      21.269ms             1  \n",
      "                                                   head         0.34%     226.000us         1.08%     711.000us     711.000us      21.000us         0.03%     168.000us     168.000us             1  \n",
      "                                            aten::slice         0.06%      41.000us         0.06%      42.000us      21.000us       8.000us         0.01%      10.000us       5.000us             2  \n",
      "                                             aten::relu         0.02%      15.000us         0.05%      36.000us      36.000us       4.000us         0.01%       8.000us       8.000us             1  \n",
      "                                        aten::clamp_min         0.02%      15.000us         0.03%      21.000us      21.000us       4.000us         0.01%       4.000us       4.000us             1  \n",
      "                                  cudaDeviceSynchronize         1.70%       1.116ms         1.70%       1.116ms       1.116ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 65.642ms\n",
      "Self CUDA time total: 65.881ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model.to(DEVICE)\n",
    "model_input = model_input.to(DEVICE)\n",
    "ITERATIONS = 10 \n",
    "def forward(model ,model_input):\n",
    "    with torch.autograd.profiler.record_function(\"embbedding\"):\n",
    "        output = model.encoder.transformer.token_embedding(model_input)\n",
    "    with torch.autograd.profiler.record_function(\"block 1-7\"):\n",
    "        layer1 = model.encoder.transformer.layers.layers[0]\n",
    "        output = layer1(output)\n",
    "        layer2 = model.encoder.transformer.layers.layers[1]\n",
    "        output = layer2(output)\n",
    "        layer3 = model.encoder.transformer.layers.layers[2]\n",
    "        output = layer3(output)\n",
    "        layer4 = model.encoder.transformer.layers.layers[3]\n",
    "        output = layer4(output)\n",
    "        layer5 = model.encoder.transformer.layers.layers[4]\n",
    "        output = layer5(output)\n",
    "        layer6 = model.encoder.transformer.layers.layers[5]\n",
    "        output = layer6(output)\n",
    "        layer7 = model.encoder.transformer.layers.layers[6]\n",
    "        output = layer7(output)        \n",
    "    with torch.autograd.profiler.record_function(\"block 8 - 15\"):\n",
    "        layer8 = model.encoder.transformer.layers.layers[7]\n",
    "        output = layer8(output)\n",
    "        layer9 = model.encoder.transformer.layers.layers[8]\n",
    "        output = layer9(output)\n",
    "        layer10 = model.encoder.transformer.layers.layers[9]\n",
    "        output = layer10(output)\n",
    "        layer11 = model.encoder.transformer.layers.layers[10]\n",
    "        output = layer11(output) \n",
    "        layer12 = model.encoder.transformer.layers.layers[11]\n",
    "        output = layer12(output)\n",
    "        layer13 = model.encoder.transformer.layers.layers[12]\n",
    "        output = layer13(output)\n",
    "        layer14 = model.encoder.transformer.layers.layers[13]\n",
    "        output = layer14(output)\n",
    "        layer15 = model.encoder.transformer.layers.layers[14]\n",
    "        output = layer15(output)        \n",
    "    with torch.autograd.profiler.record_function(\"block16 - 23\"): \n",
    "        layer16 = model.encoder.transformer.layers.layers[15]\n",
    "        output = layer16(output)\n",
    "        layer17 = model.encoder.transformer.layers.layers[16]\n",
    "        output = layer17(output)\n",
    "        layer18 = model.encoder.transformer.layers.layers[17]\n",
    "        output = layer18(output)\n",
    "        layer19 = model.encoder.transformer.layers.layers[18]\n",
    "        output = layer19(output)\n",
    "        layer20 = model.encoder.transformer.layers.layers[19]\n",
    "        output = layer20(output)\n",
    "        layer21 = model.encoder.transformer.layers.layers[20]\n",
    "        output = layer21(output)\n",
    "        layer22 = model.encoder.transformer.layers.layers[21]\n",
    "        output = layer22(output)\n",
    "        layer23 = model.encoder.transformer.layers.layers[22]\n",
    "        output = layer23(output)       \n",
    "    with torch.autograd.profiler.record_function(\"head\"):\n",
    "        output = model.head(output)\n",
    "\n",
    "        return output\n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "for i in range(ITERATIONS):\n",
    "    profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True) \n",
    "    with profiler:\n",
    "        with torch.no_grad():\n",
    "            output = forward(model,model_input)\n",
    "profiling_results6 = profiler.key_averages().table()\n",
    "print(profiling_results6)\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c03a9a9c",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd099f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profiling_results6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/daouda/workspace_test/Experiments_Final.ipynb Cell 105\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final.ipynb#Y200sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final.ipynb#Y200sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Split the profiler table into lines\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final.ipynb#Y200sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lines \u001b[39m=\u001b[39m profiling_results6\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final.ipynb#Y200sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define the layer names\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daouda/workspace_test/Experiments_Final.ipynb#Y200sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m layer_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39membbedding\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mblock 1-7\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mblock 8 - 15\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mblock16 - 23\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhead\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profiling_results6' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results6.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['embbedding','block 1-7','block 8 - 15','block16 - 23','head']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3, mean_power4]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file6 = 'profiler_results_Transformer.csv'\n",
    "with open(csv_file6, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file6}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df6 = pd.read_csv(csv_file6)\n",
    "\n",
    "# Display the table\n",
    "display(df6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c174a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df1['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = self_cuda_time_total\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =average_above_90th - power_idle\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a6a5ae",
   "metadata": {},
   "source": [
    "## BAR plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "73c47385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df6 = pd.read_csv(csv_file6)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df6['Layer Name'], df6['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27935bb3",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "896a4371",
   "metadata": {},
   "source": [
    "# EmbeedingNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "749f816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0f9bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7bf40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d50b4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f24fdd77",
   "metadata": {},
   "source": [
    "## dataset Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e21db5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_download(url, download_path):\n",
    "    archive_name = url.split('/')[-1]\n",
    "    folder_name, _ = os.path.splitext(archive_name)\n",
    "    \n",
    "    try:\n",
    "        r = urlopen(url)\n",
    "    except URLError as e:\n",
    "        print('Cannot download the data. Error: %s' % s)\n",
    "        return \n",
    "\n",
    "    assert r.status == 200\n",
    "    data = r.read()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(data)) as arch:\n",
    "        arch.extractall(download_path)\n",
    "        \n",
    "    print('The archive is extracted into folder: %s' % download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5334985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.csv':\n",
    "            files[filename.stem] = pd.read_csv(filename)\n",
    "        elif filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc6687d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick any other dataset instead\n",
    "archive_url = f'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "download_path = Path.home() / 'data' / 'movielens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb54be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The archive is extracted into folder: /home/daouda/data/movielens\n"
     ]
    }
   ],
   "source": [
    "try_download(archive_url, download_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efdf16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxrwxr-x 2 daouda g-daouda 4096 May 24 02:23 ml-1m\n",
      "drwxrwxr-x 2 daouda g-daouda 4096 May 24 02:35 ml-latest-small\n"
     ]
    }
   ],
   "source": [
    "!ls -l $download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "145d21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings, movies = read_data(download_path / 'ml-latest-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9308f923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8479a153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9742, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8934e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_preview(ratings, n=15):\n",
    "    \"\"\"Creates a cross-tabular view of users vs movies.\"\"\"\n",
    "    \n",
    "    user_groups = ratings.groupby('userId')['rating'].count()\n",
    "    top_users = user_groups.sort_values(ascending=False)[:15]\n",
    "\n",
    "    movie_groups = ratings.groupby('movieId')['rating'].count()\n",
    "    top_movies = movie_groups.sort_values(ascending=False)[:15]\n",
    "\n",
    "    top = (\n",
    "        ratings.\n",
    "        join(top_users, rsuffix='_r', how='inner', on='userId').\n",
    "        join(top_movies, rsuffix='_r', how='inner', on='movieId'))\n",
    "\n",
    "    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e87f8cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>110</th>\n",
       "      <th>260</th>\n",
       "      <th>296</th>\n",
       "      <th>318</th>\n",
       "      <th>356</th>\n",
       "      <th>480</th>\n",
       "      <th>527</th>\n",
       "      <th>589</th>\n",
       "      <th>593</th>\n",
       "      <th>1196</th>\n",
       "      <th>2571</th>\n",
       "      <th>2858</th>\n",
       "      <th>2959</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1     50    110   260   296   318   356   480   527   589   593   \\\n",
       "userId                                                                      \n",
       "68        2.5   3.0   2.5   5.0   2.0   3.0   3.5   3.5   4.0   3.5   3.5   \n",
       "182       4.0   4.5   3.5   3.5   5.0   4.5   5.0   3.5   4.0   2.0   4.5   \n",
       "249       4.0   4.0   5.0   5.0   4.0   4.5   4.5   4.0   4.5   4.0   4.0   \n",
       "274       4.0   4.0   4.5   3.0   5.0   4.5   4.5   3.5   4.0   4.5   4.0   \n",
       "288       4.5   NaN   5.0   5.0   5.0   5.0   5.0   2.0   5.0   4.0   5.0   \n",
       "307       4.0   4.5   3.5   3.5   4.5   4.5   4.0   3.5   4.5   2.5   4.5   \n",
       "380       5.0   4.0   4.0   5.0   5.0   3.0   5.0   5.0   NaN   5.0   5.0   \n",
       "387       NaN   4.5   3.5   4.5   5.0   3.5   4.0   3.0   NaN   3.5   4.0   \n",
       "414       4.0   5.0   5.0   5.0   5.0   5.0   5.0   4.0   4.0   5.0   4.0   \n",
       "448       5.0   4.0   NaN   5.0   5.0   NaN   3.0   3.0   NaN   3.0   5.0   \n",
       "474       4.0   4.0   3.0   4.0   4.0   5.0   3.0   4.5   5.0   4.0   4.5   \n",
       "599       3.0   3.5   3.5   5.0   5.0   4.0   3.5   4.0   NaN   4.5   3.0   \n",
       "603       4.0   NaN   1.0   4.0   5.0   NaN   3.0   NaN   3.0   NaN   5.0   \n",
       "606       2.5   4.5   3.5   4.5   5.0   3.5   4.0   2.5   5.0   3.5   4.5   \n",
       "610       5.0   4.0   4.5   5.0   5.0   3.0   3.0   5.0   3.5   5.0   4.5   \n",
       "\n",
       "movieId  1196  2571  2858  2959  \n",
       "userId                           \n",
       "68        5.0   4.5   5.0   2.5  \n",
       "182       3.0   5.0   5.0   5.0  \n",
       "249       5.0   5.0   4.5   5.0  \n",
       "274       4.5   4.0   5.0   5.0  \n",
       "288       4.5   3.0   NaN   3.5  \n",
       "307       3.0   3.5   4.0   4.0  \n",
       "380       5.0   4.5   NaN   4.0  \n",
       "387       4.5   4.0   4.5   4.5  \n",
       "414       5.0   5.0   5.0   5.0  \n",
       "448       5.0   2.0   4.0   4.0  \n",
       "474       5.0   4.5   3.5   4.0  \n",
       "599       5.0   5.0   5.0   5.0  \n",
       "603       3.0   5.0   5.0   4.0  \n",
       "606       4.5   5.0   4.5   5.0  \n",
       "610       5.0   5.0   3.5   5.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_preview(ratings, movies)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b02af9",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad083644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "    \n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "    \n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "    \n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b88886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 610 users, 9724 movies\n",
      "Dataset shape: (100836, 2)\n",
      "Target shape: (100836,)\n"
     ]
    }
   ],
   "source": [
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e49fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da173dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f483afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 224,  472],\n",
      "        [ 532, 3638],\n",
      "        [ 124, 4813],\n",
      "        [ 262,  348]])\n",
      "tensor([[3.],\n",
      "        [5.],\n",
      "        [2.],\n",
      "        [5.]])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in batches(X, y, bs=4):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5262f662",
   "metadata": {},
   "source": [
    "Embeddings\n",
    "\n",
    "As it is a natural language dataset, embedding has to be done. We haved done it with neural based embedding, we can also try other embedding methods availble like word2vec, glove or Onehot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f850718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a dense network with embedding layers.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        n_users:            \n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies: \n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors: \n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout: \n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of \n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts: \n",
    "            A single integer or a list of integers defining the dropout \n",
    "            layers rates applyied right after each of hidden layers.\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02, \n",
    "                 hidden=10, dropouts=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "        \n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and \n",
    "            their activations/dropouts.\n",
    "            \n",
    "            Note that the function captures `hidden` and `dropouts` \n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "            \n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "            \n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "        \n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "                \n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "    \n",
    "    \n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2048bcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (u): Embedding(610, 150)\n",
       "  (m): Embedding(9724, 150)\n",
       "  (drop): Dropout(p=0.02, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbeddingNet(n, m, n_factors=150, hidden=100, dropouts=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25dbdea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (u): Embedding(610, 150)\n",
       "  (m): Embedding(9724, 150)\n",
       "  (drop): Dropout(p=0.02, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=200, out_features=300, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbeddingNet(n, m, n_factors=150, hidden=[100, 200, 300], dropouts=[0.25, 0.5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da9c2550",
   "metadata": {},
   "source": [
    "Cyclical Learning Rate (CLR)\n",
    "\n",
    "One of the fastai library features is the cyclical learning rate scheduler. We can implement something similar inheriting the _LRScheduler class from the torch library. Following the original paper's pseudocode, this CLR Keras callback implementation, and making a couple of adjustments to support cosine annealing with restarts, let's create our own CLR scheduler.\n",
    "\n",
    "The implementation of this idea is quite simple. The base PyTorch scheduler class has the get_lr() method that is invoked each time when we call the step() method. The method should return a list of learning rates depending on the current training epoch. In our case, we have the same learning rate for all of the layers, and therefore, we return a list with a single value.\n",
    "\n",
    "The next cell defines a CyclicLR class that expectes a single callback function. This function should accept the current training epoch and the base value of learning rate, and return a new learning rate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ca0e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94c55c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular(step_size, max_lr, method='triangular', gamma=0.99):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        period = 2 * step_size\n",
    "        cycle = math.floor(1 + epoch/period)\n",
    "        x = abs(epoch/step_size - 2*cycle + 1)\n",
    "        delta = (max_lr - base_lr)*max(0, (1 - x))\n",
    "\n",
    "        if method == 'triangular':\n",
    "            pass  # we've already done\n",
    "        elif method == 'triangular2':\n",
    "            delta /= float(2 ** (cycle - 1))\n",
    "        elif method == 'exp_range':\n",
    "            delta *= (gamma**epoch)\n",
    "        else:\n",
    "            raise ValueError('unexpected method: %s' % method)\n",
    "            \n",
    "        return base_lr + delta\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5240945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2415dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aad7da0f",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48c7ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ff949b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 5.0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1974ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbeddingNet(\n",
    "    n_users=n, n_movies=m, \n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0c61799",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9dd68bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/100] train: 0.9952 - val: 0.8541\n",
      "loss improvement on epoch: 2\n",
      "[002/100] train: 0.7764 - val: 0.8089\n",
      "loss improvement on epoch: 3\n",
      "[003/100] train: 0.7242 - val: 0.7702\n",
      "loss improvement on epoch: 4\n",
      "[004/100] train: 0.6537 - val: 0.7633\n",
      "loss improvement on epoch: 5\n",
      "[005/100] train: 0.6590 - val: 0.7537\n",
      "[006/100] train: 0.6124 - val: 0.7542\n",
      "[007/100] train: 0.6229 - val: 0.7609\n",
      "[008/100] train: 0.5840 - val: 0.7668\n",
      "[009/100] train: 0.5903 - val: 0.7740\n",
      "[010/100] train: 0.5448 - val: 0.7794\n",
      "[011/100] train: 0.5488 - val: 0.7923\n",
      "[012/100] train: 0.4897 - val: 0.8074\n",
      "[013/100] train: 0.4851 - val: 0.8192\n",
      "[014/100] train: 0.4223 - val: 0.8480\n",
      "[015/100] train: 0.4196 - val: 0.8516\n",
      "early stopping after epoch 015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #print(x_batch[:,0])\n",
    "          \n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8a0e7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8007cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(lr_history[:2*iterations_per_epoch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "282290fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213909ea",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f5f1b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (u): Embedding(610, 150)\n",
       "  (m): Embedding(9724, 150)\n",
       "  (drop): Dropout(p=0.05, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e335446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "974a8bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.  5.  2.  ... 3.5 4.  4.5]\n"
     ]
    }
   ],
   "source": [
    "print(groud_truth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1aaf2b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.8683\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22dca7b7",
   "metadata": {},
   "source": [
    "## layer / block power measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1c3721de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "38.99 W\n",
      "39.74 W\n",
      "38.80 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.90 W\n",
      "38.99 W\n",
      "39.67 W\n",
      "38.69 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.99 W\n",
      "39.08 W\n",
      "39.49 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.90 W\n",
      "39.09 W\n",
      "39.77 W\n",
      "38.79 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.98 W\n",
      "38.98 W\n",
      "39.57 W\n",
      "38.80 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "38.98 W\n",
      "40.06 W\n",
      "38.89 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.89 W\n",
      "39.00 W\n",
      "39.76 W\n",
      "38.80 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "38.99 W\n",
      "38.99 W\n",
      "39.78 W\n",
      "38.80 W\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "gotstarted\n",
      "39.00 W\n",
      "38.99 W\n",
      "39.96 W\n",
      "38.80 W\n",
      "torch.Size([1024, 1])\n",
      "Mean power:  38.93666666666667\n",
      "Mean power1:  39.010000000000005\n",
      "Mean power2:  39.75555555555556\n",
      "Mean power3:  38.80666666666667\n"
     ]
    }
   ],
   "source": [
    "groud_truth1, predictions1 = [], []\n",
    "def measure(q, rq, gpu_id):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=2 -i {gpu_id}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "user_input = x_batch[:, 0].to(device)\n",
    "movie_input = x_batch[:, 1].to(device)\n",
    "net.to(device)\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=2048):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "#############################################################################\n",
    "        # Assuming x_batch[:, 0] represents user input and x_batch[:, 1] represents movie input\n",
    "\n",
    "        q = multiprocessing.Queue()\n",
    "        rq = multiprocessing.Queue() \n",
    "        q1 = multiprocessing.Queue()\n",
    "        rq1 = multiprocessing.Queue() \n",
    "        q2 = multiprocessing.Queue()\n",
    "        rq2 = multiprocessing.Queue() \n",
    "        q3 = multiprocessing.Queue()\n",
    "        rq3 = multiprocessing.Queue()\n",
    "        \n",
    "        p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "        p.start()\n",
    "        q.put('start')\n",
    "        m = rq.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started' \n",
    "        user_embedding = net.u(user_input)\n",
    "        movie_embedding = net.m(movie_input)\n",
    "        q.put('stop')\n",
    "        p.join() \n",
    "\n",
    "\n",
    "        p1 = multiprocessing.Process(target=measure, args=(q1,rq1,1))\n",
    "        p1.start()\n",
    "        q1.put('start')\n",
    "        m = rq1.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started' \n",
    "        x = torch.cat([user_embedding, movie_embedding], dim=1)\n",
    "        x = net.drop(x)\n",
    "        q1.put('stop')\n",
    "        p1.join() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        p2 = multiprocessing.Process(target=measure, args=(q2,rq2,1))\n",
    "        p2.start()\n",
    "        q2.put('start')\n",
    "        m = rq2.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started' \n",
    "        x = net.hidden[0](x)\n",
    "        x = net.hidden[1](x)\n",
    "        x = net.hidden[2](x)\n",
    "        x = net.hidden[3](x)\n",
    "        x = net.hidden[4](x)\n",
    "        x = net.hidden[5](x)\n",
    "        x = net.hidden[6](x)\n",
    "        x = net.hidden[7](x)\n",
    "        x = net.hidden[8](x)\n",
    "        q2.put('stop')\n",
    "        p2.join() \n",
    "\n",
    "        p3 = multiprocessing.Process(target=measure, args=(q3,rq3,1))\n",
    "        p3.start()\n",
    "        q3.put('start')\n",
    "        m = rq3.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started' \n",
    "        outputs = net.fc(x) \n",
    "        q3.put('stop')\n",
    "        p3.join()  \n",
    "\n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq.empty():\n",
    "            power_output = rq.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "\n",
    "        while not rq1.empty():\n",
    "            power_output = rq1.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements1.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "\n",
    "        while not rq2.empty():\n",
    "            power_output = rq2.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements2.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "\n",
    "        while not rq3.empty():\n",
    "            power_output = rq3.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements3.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "        groud_truth1.extend(y_batch.tolist())\n",
    "        predictions1.extend(outputs.tolist())\n",
    "print(outputs.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power) \n",
    "mean_power1 = np.mean(power_measurements1)\n",
    "print(\"Mean power1: \", mean_power1)\n",
    "mean_power2 = np.mean(power_measurements2)\n",
    "print(\"Mean power2: \", mean_power2) \n",
    "mean_power3 = np.mean(power_measurements3)\n",
    "print(\"Mean power3: \", mean_power3)\n",
    "\n",
    "\n",
    "groud_truth1 = np.asarray(groud_truth).ravel()\n",
    "predictions1 = np.asarray(predictions).ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37094c3",
   "metadata": {},
   "source": [
    "##  power measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "79d85957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "gotstarted\n",
      "40.83 W\n",
      "gotstarted\n",
      "40.34 W\n",
      "gotstarted\n",
      "40.74 W\n",
      "gotstarted\n",
      "40.63 W\n",
      "gotstarted\n",
      "40.64 W\n",
      "gotstarted\n",
      "40.35 W\n",
      "gotstarted\n",
      "40.63 W\n",
      "gotstarted\n",
      "40.83 W\n",
      "gotstarted\n",
      "40.83 W\n",
      "gotstarted\n",
      "40.63 W\n",
      "gotstarted\n",
      "40.33 W\n",
      "gotstarted\n",
      "40.44 W\n",
      "gotstarted\n",
      "40.15 W\n",
      "gotstarted\n",
      "40.14 W\n",
      "gotstarted\n",
      "40.44 W\n",
      "gotstarted\n",
      "40.74 W\n",
      "gotstarted\n",
      "40.54 W\n",
      "gotstarted\n",
      "40.35 W\n",
      "gotstarted\n",
      "40.33 W\n",
      "torch.Size([2048, 1])\n",
      "Mean power:  40.52157894736842\n"
     ]
    }
   ],
   "source": [
    "groud_truth1, predictions1 = [], []\n",
    "def measure(q, rq, gpu_id):\n",
    "    while True:\n",
    "        # Wait for start signal\n",
    "        start_signal = q.get()\n",
    "        if start_signal != 'start':\n",
    "            continue\n",
    "            \n",
    "        cmd = f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader --loop-ms=2 -i {gpu_id}\"\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        rq.put('started')\n",
    "\n",
    "        # Read and print power measurements until stop signal is received\n",
    "        while True:\n",
    "            power_output = process.stdout.readline().decode().strip()\n",
    "            rq.put(power_output)\n",
    "            if not power_output:\n",
    "                break\n",
    "\n",
    "            if q.get() == 'stop':\n",
    "                process.kill()\n",
    "                return\n",
    "\n",
    "        # Exit the loop if the stop signal is received\n",
    "        if q.get() =='':\n",
    "            break\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "user_input = x_batch[:, 0].to(device)\n",
    "movie_input = x_batch[:, 1].to(device)\n",
    "net.to(device)\n",
    "power_measurements = [] \n",
    "power_measurements1 = []\n",
    "power_measurements2 = []\n",
    "power_measurements3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=1024):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "#############################################################################\n",
    "        # Assuming x_batch[:, 0] represents user input and x_batch[:, 1] represents movie input\n",
    "\n",
    "        q = multiprocessing.Queue()\n",
    "        rq = multiprocessing.Queue() \n",
    "\n",
    "        \n",
    "        p = multiprocessing.Process(target=measure, args=(q,rq,1))\n",
    "        p.start()\n",
    "        q.put('start')\n",
    "        m = rq.get()\n",
    "        print('got' + m)\n",
    "        assert m == 'started' \n",
    "        user_embedding = net.u(user_input)\n",
    "        movie_embedding = net.m(movie_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x = torch.cat([user_embedding, movie_embedding], dim=1)\n",
    "        x = net.drop(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x = net.hidden[0](x)\n",
    "        x = net.hidden[1](x)\n",
    "        x = net.hidden[2](x)\n",
    "        x = net.hidden[3](x)\n",
    "        x = net.hidden[4](x)\n",
    "        x = net.hidden[5](x)\n",
    "        x = net.hidden[6](x)\n",
    "        x = net.hidden[7](x)\n",
    "        x = net.hidden[8](x)\n",
    " \n",
    "        outputs = net.fc(x) \n",
    "        q.put('stop')\n",
    "        p.join() \n",
    "        \n",
    "        # Retrieve the power measurements from the queue\n",
    "        while not rq.empty():\n",
    "            power_output = rq.get()\n",
    "            if power_output == 'stop':\n",
    "                break\n",
    "            print(power_output)\n",
    "            power_measurements.append(float(power_output.split()[0]))  # Remove the \"W\" string and convert to float\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "        groud_truth1.extend(y_batch.tolist())\n",
    "        predictions1.extend(outputs.tolist())\n",
    "print(outputs.shape) \n",
    "mean_power = np.mean(power_measurements)\n",
    "print(\"Mean power: \", mean_power) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7204bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "    x_batch, y_batch = [b.to(device) for b in batch]\n",
    "    outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "    groud_truth.extend(y_batch.tolist())\n",
    "    predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4497fa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.38736439, 4.29216433, 3.50974274, ..., 3.65618658, 3.57996368,\n",
       "       3.64016676])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f224cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.8683\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions1 - groud_truth1)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803918c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad5cfa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda :  True\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:41 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       aten::lift_fresh         0.15%       2.000us         0.15%       2.000us       1.000us      25.000us         1.84%      25.000us      12.500us             2  \n",
      "                                               aten::to         1.60%      21.000us        23.53%     308.000us      61.600us      30.000us         2.21%     323.000us      64.600us             5  \n",
      "                                             aten::view         0.31%       4.000us         0.31%       4.000us       4.000us       7.000us         0.52%       7.000us       7.000us             1  \n",
      "                                         aten::_to_copy         3.36%      44.000us        21.93%     287.000us     143.500us      36.000us         2.65%     293.000us     146.500us             2  \n",
      "                                    aten::empty_strided         1.07%      14.000us         1.07%      14.000us       7.000us      21.000us         1.55%      21.000us      10.500us             2  \n",
      "                                            aten::copy_         7.10%      93.000us        22.23%     291.000us      58.200us     142.000us        10.46%     309.000us      61.800us             5  \n",
      "                                       aten::empty_like         1.76%      23.000us         2.67%      35.000us      17.500us      21.000us         1.55%      42.000us      21.000us             2  \n",
      "                                            aten::empty         2.06%      27.000us         2.06%      27.000us       3.857us      58.000us         4.27%      58.000us       8.286us             7  \n",
      "                                        aten::expand_as         0.69%       9.000us         1.60%      21.000us      21.000us       9.000us         0.66%      25.000us      25.000us             1  \n",
      "                                           aten::expand         1.68%      22.000us         1.91%      25.000us      12.500us      21.000us         1.55%      32.000us      16.000us             2  \n",
      "                                       aten::as_strided         0.76%      10.000us         0.76%      10.000us       1.667us      34.000us         2.51%      34.000us       5.667us             6  \n",
      "                                       aten::contiguous         0.76%      10.000us         4.20%      55.000us      55.000us       9.000us         0.66%      58.000us      58.000us             1  \n",
      "                                            aten::clone         1.45%      19.000us         3.44%      45.000us      45.000us      15.000us         1.11%      49.000us      49.000us             1  \n",
      "                                        cudaMemcpyAsync         2.14%      28.000us         2.14%      28.000us      14.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                  cudaStreamSynchronize         1.15%      15.000us         1.15%      15.000us       7.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         5.27%      69.000us         5.27%      69.000us       4.600us       0.000us         0.00%       0.000us       0.000us            15  \n",
      "                                            u-embedding         5.81%      76.000us        13.52%     177.000us     177.000us      71.000us         5.23%     181.000us     181.000us             1  \n",
      "                                        aten::embedding         1.76%      23.000us         7.72%     101.000us      50.500us      22.000us         1.62%     110.000us      55.000us             2  \n",
      "                                     aten::index_select         4.13%      54.000us         5.96%      78.000us      39.000us      59.000us         4.35%      88.000us      44.000us             2  \n",
      "                                          aten::resize_         0.53%       7.000us         0.53%       7.000us       3.500us      15.000us         1.11%      15.000us       7.500us             2  \n",
      "                                               drop-net         3.82%      50.000us         5.12%      67.000us      67.000us      47.000us         3.46%      73.000us      73.000us             1  \n",
      "                                              aten::cat         0.92%      12.000us         1.30%      17.000us      17.000us      22.000us         1.62%      22.000us      22.000us             1  \n",
      "                                          aten::dropout         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us         1.11%      15.000us       3.750us             4  \n",
      "                                             hidden-net        16.12%     211.000us        46.29%     606.000us     606.000us     179.000us        13.19%     610.000us     610.000us             1  \n",
      "                                           aten::linear         7.10%      93.000us        32.39%     424.000us     106.000us      69.000us         5.08%     443.000us     110.750us             4  \n",
      "                                                aten::t         3.59%      47.000us         7.49%      98.000us      24.500us      49.000us         3.61%     115.000us      28.750us             4  \n",
      "                                        aten::transpose         3.36%      44.000us         3.90%      51.000us      12.750us      43.000us         3.17%      66.000us      16.500us             4  \n",
      "                                            aten::addmm        13.37%     175.000us        17.80%     233.000us      58.250us     220.000us        16.21%     259.000us      64.750us             4  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.23%       3.000us         0.23%       3.000us       0.750us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                             aten::relu         2.75%      36.000us         5.88%      77.000us      25.667us      32.000us         2.36%      87.000us      29.000us             3  \n",
      "                                        aten::clamp_min         2.22%      29.000us         3.13%      41.000us      13.667us      55.000us         4.05%      55.000us      18.333us             3  \n",
      "                                                 fc-net         2.37%      31.000us        10.47%     137.000us     137.000us      31.000us         2.28%     141.000us     141.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.61%       8.000us         0.61%       8.000us       8.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.309ms\n",
      "Self CUDA time total: 1.357ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-31 12:46:42 63839:63839 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") \n",
    "net = net.to(device)\n",
    "user_input = x_batch[:, 0].to(device)\n",
    "movie_input = x_batch[:, 1].to(device)\n",
    "def forward():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=1024):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "\n",
    "\n",
    "        with torch.autograd.profiler.record_function(\"u-embedding\"):\n",
    "            user_embedding = net.u(user_input)\n",
    "            movie_embedding = net.m(movie_input)\n",
    "        with torch.autograd.profiler.record_function(\"drop-net\"):\n",
    "            x = torch.cat([user_embedding, movie_embedding], dim=1)\n",
    "            x = net.drop(x)\n",
    "        with torch.autograd.profiler.record_function(\"hidden-net\"):\n",
    "            x = net.hidden[0](x)\n",
    "            x = net.hidden[1](x)\n",
    "            x = net.hidden[2](x)\n",
    "            x = net.hidden[3](x)\n",
    "            x = net.hidden[4](x)\n",
    "            x = net.hidden[5](x)\n",
    "            x = net.hidden[6](x)\n",
    "            x = net.hidden[7](x)\n",
    "            x = net.hidden[8](x)\n",
    "        with torch.autograd.profiler.record_function(\"fc-net\"):\n",
    "            outputs = net.fc(x) \n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "# Run the model and record the inference time of each layer\n",
    "for i in range(10):\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        profiler = torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=True) \n",
    "        net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        with profiler:\n",
    "            with torch.no_grad():\n",
    "                outputs = forward()\n",
    "    # Get the table of profiling results\n",
    "profiling_results9 = profiler.key_averages().table()\n",
    "print(profiling_results9) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc016c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8ea2175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiler results saved in 'profiler_results_embeddingNet.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>CUDA Time Avg (ms)</th>\n",
       "      <th>Power Measurements(W)</th>\n",
       "      <th>Energy(mJ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u-embedding</td>\n",
       "      <td>0.181</td>\n",
       "      <td>38.975032</td>\n",
       "      <td>7.054481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drop-net</td>\n",
       "      <td>0.073</td>\n",
       "      <td>39.049363</td>\n",
       "      <td>2.850604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hidden-net</td>\n",
       "      <td>0.610</td>\n",
       "      <td>40.624777</td>\n",
       "      <td>24.781114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc-net</td>\n",
       "      <td>0.141</td>\n",
       "      <td>38.870510</td>\n",
       "      <td>5.480742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Layer Name  CUDA Time Avg (ms)  Power Measurements(W)  Energy(mJ)\n",
       "0  u-embedding               0.181              38.975032    7.054481\n",
       "1     drop-net               0.073              39.049363    2.850604\n",
       "2   hidden-net               0.610              40.624777   24.781114\n",
       "3       fc-net               0.141              38.870510    5.480742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Split the profiler table into lines\n",
    "lines = profiling_results9.strip().split('\\n')\n",
    "\n",
    "# Define the layer names\n",
    "layer_names = ['u-embedding','drop-net','hidden-net','fc-net']\n",
    "power_measurements = [mean_power, mean_power1, mean_power2, mean_power3]\n",
    "\n",
    "# Extract the 'CUDA time avg' values for the specified layers\n",
    "cuda_time_avgs = {}\n",
    "for layer_name in layer_names:\n",
    "    layer_line = next(line for line in lines if layer_name in line)\n",
    "    cuda_time_avg_us = re.findall(r'\\d+\\.\\d+(?:us|ms)', layer_line)[-1]\n",
    "    cuda_time_avg_ms = float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0]) / 1000 if 'us' in cuda_time_avg_us else float(re.findall(r'\\d+\\.\\d+', cuda_time_avg_us)[0])\n",
    "    cuda_time_avgs[layer_name] = cuda_time_avg_ms\n",
    "\n",
    "# Calculate the energy by multiplying CUDA time avg and power measurements\n",
    "energy_values = [cuda_time_avgs[layer_name] * power_measurements[i] for i, layer_name in enumerate(layer_names)]\n",
    "\n",
    "# Save the results in a CSV file\n",
    "csv_file9 = 'profiler_results_embeddingNet.csv'\n",
    "with open(csv_file9, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Layer Name', 'CUDA Time Avg (ms)', 'Power Measurements(W)', 'Energy(mJ)'])\n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        writer.writerow([layer_name, cuda_time_avgs[layer_name], power_measurements[i], energy_values[i]])\n",
    "\n",
    "print(f\"Profiler results saved in '{csv_file9}'.\")\n",
    "\n",
    "# Read the CSV file\n",
    "df9 = pd.read_csv(csv_file9)\n",
    "\n",
    "# Display the table\n",
    "display(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "52d1eadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum of Energy</th>\n",
       "      <th>Self CUDA Time * X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.16694</td>\n",
       "      <td>54.84994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sum of Energy  Self CUDA Time * X\n",
       "0       40.16694            54.84994"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the sum of energy for all layers\n",
    "total_energy = df9['Energy(mJ)'].sum()\n",
    "\n",
    "# Extract the self CUDA time from the profiling results\n",
    "self_cuda_time = 1.357\n",
    "power_idle = 38\n",
    "# Multiply self CUDA time by X (adjust the value of X according to your needs)\n",
    "X =40.42\n",
    "result = self_cuda_time * X\n",
    "\n",
    "# Create a new DataFrame with two columns\n",
    "new_df = pd.DataFrame({'Sum of Energy': [total_energy], 'Self CUDA Time * X': [result]})\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77d842fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAFTCAYAAADfr7AAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvkklEQVR4nO3deXxMZ98G8Gsyk1VkM0lIaCRiSYilllgqCUlVUVXFq28tadXy0FZRqvU+tLZGUbQItaXqaQVtUbR0LBFbm1JFkpIgas1CEoKsc79/eJyaJhhJZk4m5/p+PvnInDlzzm/uHHPNue+zqIQQAkREpEhWchdARETyYQgQESkYQ4CISMEYAkRECsYQICJSMIYAEZGCMQSIqpjIyEhERETIXQYpBEOgCoqMjIRKpSr14+joKHdpleKvv/7Cv/71L/j6+sLW1hbe3t547rnnsHnzZijptJV169ZBpVKVmr5o0SJs3LhRhopMjwFX9WjkLoDK1rlzZ2zYsMFgmpWV6TO7sLAQNjY2Jlv+8ePH0bVrV9SvXx+ffvopmjZtipKSEuzZswfjxo1DWFgYXFxcTLZ+S+Ds7Cx3CRVWVFQEa2trucswmqm3+6qMewJVlI2NDWrXrm3w4+HhIT0fFhaGN954AzNmzEDt2rXh5uaGIUOGIC8vz2A569evR8uWLWFnZ4f69etj/PjxuH37tsFyhg0bhn//+9+oU6cOnnrqKQDA77//jvbt28PW1hYNGzbExo0bUb9+fcycORPAvW903bp1K1V3165dMWzYsDLfkxACQ4cOhbe3N3799Ve89NJLaNSoEQICAjBmzBicPHlS2tu5desWRo4cCXd3d9ja2qJNmzbYtWuXtKy0tDSoVCps2LABvXr1goODA/z8/BATE2OwzpUrVyIgIAB2dnZwc3NDSEgILl26BACIiYmBRmP4PejSpUtQqVTYt28fAGDfvn1QqVTYsWMHOnToAHt7e7Ru3RqJiYlITEzEM888AwcHB7Rr1w5JSUnScu4vW6fToWnTprCzs0NwcDCOHz8uLXfw4MEAIO3pRUZGSm374LdlIQTmzZsHPz8/2NjYoEGDBli4cKFB3fXr18fUqVMxduxYuLm5wdPTE+PGjUNxcXGZf4sH23DdunUIDw+Hvb09/Pz8sH79eoP50tPTERkZCXd3d9SsWROdOnXC/v37pefvt9H27dvxzDPPwM7ODitXrnzoeh/l66+/RnBwMJydnaHVatGzZ0+cOXNGej4sLAwjRowweI0QAg0aNMCMGTOkaeXd7hVJUJUzdOhQER4e/sh5QkNDhbOzs3jnnXdEcnKy2Llzp3B1dRX/93//J82zZs0a4eLiItauXSvOnj0r4uLiRFBQkBg0aJDBchwdHcXIkSNFYmKiOHHihLh9+7aoXbu26NWrl/jjjz/E4cOHRYcOHYS9vb2YMWOGEEKIQ4cOCZVKJc6dOyctKyUlRahUKnHkyJEya/79998FAPHVV189tg369esnfHx8xE8//SSSkpLE22+/LaytrUVycrIQQojz588LAMLX11fExsaKlJQU8f777wu1Wi1Onz4thBDit99+E2q1Wnz55ZciLS1NnDhxQqxYsUJcvHhRah+1Wm2w3osXLwoAYu/evUIIIfbu3SsAiJYtW4rdu3eLxMRE0b59exEUFCQ6d+4sdDqdSEpKEp06dRLt2rUzaHuVSiVatWol9u3bJ/744w/Rs2dP4eXlJe7cuSMKCgrE4sWLBQBx9epVcfXqVZGTkyOEKP33X7x4sbCzsxPLly8XZ86cEdHR0cLW1lasXLlSmsfHx0e4uLiIjz/+WJw5c0bExsYKjUZjMM8/3W/DOnXqiHXr1ok///xTTJkyRVhZWYljx44JIYS4c+eOCAgIEH379hUJCQkiJSVFzJw5U9jY2IikpCSDNmrcuLHYunWrOHfunNTG//S4bXv16tVi69atIjU1VRw7dky88MILwt/fXxQUFAghhPj666+Fo6OjuHXrlvQanU4n1Gq1uHz5stT25dnulYohUAUNHTpUqNVqUaNGDYOfXr16SfOEhoaK5s2bG7xu1KhRon379tJjHx8fER0dbTBPXFycACBu3LghLadhw4aipKREmueLL74QNWrUkD6UhBAiOTlZAJBCQAghgoKCxJQpU6THkydPLlXTg2JjYwUAcfTo0Ue+/5SUFAFAbN++3WB6q1atxGuvvSaE+PsDbP78+dLzxcXFwtHRUSxbtkwIIcR3330nnJycRG5ubpnreZIQ+P7776V5NmzYIACITZs2SdO+++47AUD6cFqzZo0AIHQ6nTTPjRs3RI0aNaQP5q+++kqU9T3snx+UdevWFRMnTjSY55133hG+vr7SYx8fH/HCCy8YzNO9e3cxcODAMt+7EH+34YNfHIQQokOHDtIH5po1a4S3t7coKioymKdLly5i7NixQoi/22jt2rUPXdfD3tvjXL9+XQAQBw4cEEIIkZ+fL7RarVixYoU0z8CBA0Xv3r2lx+Xd7pWK3UFV1P2ugwd/li9fbjBPixYtDB57eXkhPT0dAJCZmYkLFy5g/PjxcHR0lH6ef/55AEBqaqr0utatWxuMNyQlJSEgIMCgb7pJkyal+upHjhyJNWvWoKSkBMXFxYiJicHw4cMf+p6EkYO+97tVQkJCDKaHhIQgMTHRYFrLli2l39VqNTw8PKQ2ePbZZ+Hn5wdfX18MHDgQX3zxBbKysoyq4Z8ebOvatWsDAJo3b15qWkZGhsHrOnToIP3u6uqKgICAUu/hUW7evIlLly6VaovQ0FCkpaXhzp070rQH2wIw3B4e5cEaAaBTp05SjQkJCbh27RpcXFwMtqP4+HikpKQYvK5du3ZGv6+HOX78OF566SX4+vqiZs2aUjfNhQsXAAC2traIjIzEihUrAADXr1/H999/L213FdnulYoDw1WUvb09/P39HznPPweyVCoV9Ho9AEj/Llq0CF26dCn12rp160q/16hRo9TzZR218k+DBw/Ge++9h+3bt0Ov1yM3NxeDBg166PyNGzcGcO9D/umnn37s8o3xqDZwdHTEb7/9hoMHD0Kn02HZsmWYNGkSdu/e/dAPgKKiojLX8+Ag5/22KWva/XXL4VFtUV56vR4BAQH4/vvvSz3n4OBg8Lis7ehJ3LlzB926dcMzzzyDNWvWwNPTEwDQtGlTFBYWSvONHDkS8+fPx4kTJ7Bnzx64u7tLH/IV3e6ViDFYTXl6eqJevXo4ffo0/P39S/3Y2dk99LWBgYFITk5Gbm6uNO306dPIyckxmM/JyQkDBw7EihUrsGLFCvTv3/+RR/a0aNECQUFBmDNnTpkDlnl5eSguLkbTpk0BwGDw8f7jZs2aGfHu/6ZWqxESEoLp06fj6NGjqFOnDr7++msAgIeHB0pKSgy+LR87duyJlv84R44ckX7PyclBcnIyAgMDAfz9oV1SUvLQ1zs5OaFu3bql2iIuLg6+vr6lPogrWiMAHDp0SKqxTZs2OHfuHJycnEptQ15eXhVe94OSk5ORmZmJWbNmISwsDAEBAcjOzi61B+nv74+uXbtixYoVWLlyJV5//XWo1WoAFdvulYp7AlVUYWEhrl27Vmq6p6enUd/SAWDWrFkYNmwYXF1d8eKLL8La2hrJycn48ccfS3UtPejVV1/F1KlTMWTIEMyYMQN3797FhAkTYG9vX2rdI0eOlLoT4uLiHlmPSqVCTEwMwsPDERwcjH//+9/SIaJxcXGYM2cOjh07hgYNGqB///4YPXo0li9fDh8fH0RHR+PUqVPSB7gxtmzZgnPnziEkJATu7u44evQoLl68KH3AtWvXDjVr1sTkyZPxwQcf4OzZs5g+fbrRy38clUqFSZMm4dNPP4WrqyumTJmCmjVr4n//938BAL6+vgCArVu34plnnoG9vX2Z54K8//77mDBhAho2bIiwsDDs2bMH0dHRWLJkSaXUuWrVKjRp0gRt2rTBunXrcPjwYXz++ecA7m0LCxYsQM+ePTFr1iw0atQI6enp2LNnDwICAtCnT58nXl9eXp50lNR9dnZ28PHxga2tLT7//HNMmDABaWlpmDx5cpnb+8iRIzFo0CAUFxfjjTfeMHiuvNu9Ysk9KEGlDR06VAAo8yczM1MIcW9ga9iwYQavmzFjhvDx8TGY9v3334v27dsLe3t7UbNmTdGiRQvx0UcfSc+XtRwhhDh27JgIDg4WNjY2wt/fX2zcuFG4u7uLefPmlZq3ZcuWIjAw0Oj3d/78eTFixAjh4+MjrK2tRZ06dcRzzz0ntmzZIvR6vRBCiNzcXDFixAih1WqFjY2NaN26tdi5c6fBMgCI+Ph4g2U3aNBATJs2TQhxbzCwS5cuQqvVCltbW+Hv7y8+/vhjg/m3bdsmmjRpIuzs7ETHjh3FTz/9VObA8INHu8THxwsA4vz589K0w4cPCwAiJSVFCPH3oPPOnTtFkyZNhI2NjWjbtm2pQfGxY8cKd3d3AUAMHTpUCFF68FSv14tPPvlE1K9fX2g0GuHr6ysWLFhgsBwfHx+DQXshhBg2bJgIDQ195N8B/x3QDQ0NFba2tqJ+/friP//5j8F8WVlZYtSoUcLLy0tYW1sLLy8v0adPH+kIorLa6GEetm03btxYCCHExo0bhb+/v7C1tRUtW7YU+/btE2q1WqxZs8ZgOYWFhcLd3V306NGjzPWUd7tXIoYAGSUtLU0AEFu3bjWYXlhYKLy8vMTChQtlqqxqKuvIo6rmYUFqCbKysoSdnZ3YvHmz3KVYPHYHUZnWrVsHb29v+Pr64sKFC5g0aRJ8fHykE8T0ej2ysrKwfPly3L59G6+99prMFZMSFBUV4fr16/jwww/h7e2NF154Qe6SLB5DgMp0/fp1TJs2DZcvX4abmxs6deqEjRs3wtbWFsC96//4+vqiTp06WL16NZycnGSumJTg4MGD6NKlC3x9ffHVV1/xEM9KoBJCQVfsIiIiA4xRIiIFYwgQESkYQ4CISMEscmD4ypUrcpdQLlqtttzXriG2X0Wx/SrG0tvvYWd4c0+AiEjBGAJERArGECAiUjCzjAlkZWVhyZIlyMnJgUqlQkREBHr06IENGzZg9+7d0olGr7zySqVdYpiIiB7PLCGgVqsxePBg+Pn54e7du5g8ebJ0Q46ePXuid+/e5iiDiIj+wSwh4OrqCldXVwD3bpbi7e2NGzdumGPVRET0CGY/RDQjIwPnz5+Hv78//vzzT+zcuRP79++Hn58fhgwZUub11HU6HXQ6HQAgKioKWq3W3GVXCo1GY7G1VwVsv4ph+1VMdW0/s147KD8/H9OmTUPfvn0RHByMnJwcaTwgNjYW2dnZGD169GOXw/MElIntVzFsv4qx9PZ72HkCZtsTKC4uxvz589G5c2cEBwcDgMGtCMPDwzFnzhxzlUNkdiXD5R37evwt501LvWKrzBVQWcxyiKgQAsuWLYO3tzd69eolTc/OzpZ+//XXX1GvXj1zlENERP9llj2B06dPY//+/XjqqacwceJEAPcOBz148CDS0tKgUqng7u6OESNGmKMcIiL6L7OEQJMmTbBhw4ZS03lOABGRvHjGMBGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYIxBIiIFIwhQESkYAwBIiIFYwgQESkYQ4CISMEYAkRECsYQICJSMIYAEZGCMQSIiBSMIUBEpGAMASIiBWMIEBEpGEOAiEjBGAJERArGECAiUjCGABGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYJpzLGSrKwsLFmyBDk5OVCpVIiIiECPHj2Ql5eHBQsWIDMzE+7u7hg3bhwcHR3NURIREcFMIaBWqzF48GD4+fnh7t27mDx5Mpo3b459+/YhKCgIffr0webNm7F582YMGjTIHCURERHM1B3k6uoKPz8/AIC9vT28vb1x48YNJCQkIDQ0FAAQGhqKhIQEc5RDRET/ZZY9gQdlZGTg/Pnz8Pf3R25uLlxdXQEALi4uyM3NLfM1Op0OOp0OABAVFQWtVmu2eiuTRqOx2NqrAktvv3S5C5CZJf/tAMvf/h7GrCGQn5+P+fPnIzIyEg4ODgbPqVQqqFSqMl8XERGBiIgI6XFWVpZJ6zQVrVZrsbVXBWw/y2bpfztL3/68vLzKnG62o4OKi4sxf/58dO7cGcHBwQAAZ2dnZGdnAwCys7Ph5ORkrnKIiAhmCgEhBJYtWwZvb2/06tVLmt6mTRvExcUBAOLi4tC2bVtzlENERP9llu6g06dPY//+/XjqqacwceJEAMArr7yCPn36YMGCBdizZ490iCgREZmPWUKgSZMm2LBhQ5nPTZ061RwlEBFRGXjGMBGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYIxBIiIFIwhQESkYAwBIiIFYwgQESkYQ4CISMEYAkRECsYQICJSMIYAEZGCMQSIiBSMIUBEpGAMASIiBWMIEBEpGEOAiEjBGAJERArGECAiUjCGABGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwjTlWsnTpUhw7dgzOzs6YP38+AGDDhg3YvXs3nJycAACvvPIKnn76aXOUQ0RE/2WWEAgLC0P37t2xZMkSg+k9e/ZE7969zVECERGVwSzdQYGBgXB0dDTHqoiI6AkYHQI7duzAzZs3K3XlO3fuxLvvvoulS5ciLy+vUpdNRESPZ3R30KlTp/DNN9+gadOmCAkJQdu2bWFtbV3uFXfr1g39+vUDAMTGxmLt2rUYPXp0mfPqdDrodDoAQFRUFLRabbnXKyeNRmOxtVcFlt5+6XIXIDNL/tsBlr/9PYzRITBp0iTcunULBw8exPbt27FixQoEBwcjJCQEgYGBT7xiFxcX6ffw8HDMmTPnofNGREQgIiJCepyVlfXE66sKtFqtxdZeFbD9LJul/+0sffvz8vIqc/oTDQzXrFkT3bt3R/fu3XHhwgUsXrwYe/fuhVarRXh4OHr06AE7OzujlpWdnQ1XV1cAwK+//op69eo9SSlERFQJnvjooJMnTyI+Ph4JCQlo0KAB3nzzTWi1WuzYsQOzZ8/G9OnTS71m4cKFSEpKwq1btzBq1CgMGDAAiYmJSEtLg0qlgru7O0aMGFEpb4iIiIxndAisXbsWhw4dgoODA0JCQjB//ny4ublJzzds2BCvvfZama995513Sk3r2rXrk1dLRESVyugQKCoqwrvvvgt/f/+yF6TRICoqqtIKIyIi0zM6BIYNG/bYeby9vStUDBERmZfRITB16lSoVKrSC9BoUKtWLbRr1w5t2rSp1OKIiMi0jD5ZLDAwEBkZGQgICEDnzp0REBCAzMxMNGjQAM7OzoiOjsaWLVtMWSsREVUyo/cETpw4gSlTpqBu3brStM6dO2PJkiWYPXs2goODsWjRIrz44osmKZSIiCqf0XsCly9fhqenp8E0d3d3XLlyBQDg7++PnJycSi2OiIhMy+gQCAgIwNKlS3Ht2jUUFhbi2rVrWLZsGZo0aQIA+Ouvv6STv4iIyDIY3R305ptvYuXKlRg3bhz0ej3UajXatWsnXe9Ho9Fg7NixJiuUiIgqn1EhoNfrsX37dowePRpvv/02bt68CScnJ1hZ/b0j8bDrUhARUdVlVHeQlZUVdu3aBY1GAysrK7i4uBgEABERWSajP8lDQkLw888/m7IWIiIyM6PHBFJTU/HTTz9h69atqFWrlsGJYx999JFJiiMiItMyOgTCw8MRHh5uylqIiMjMjA6BsLAwE5ZBRERyMDoEhBDYvXs3Dh48iFu3bmHevHlISkpCTk4OOnbsaMoaiYjIRIweGI6NjcXevXsREREh3WKtVq1avF4QEZEFMzoE4uLi8N5776FTp07SoLCHhwcyMjJMVhwREZmW0SGg1+tL3T84Pz/f6HsKExFR1WN0CLRq1Qpr165FUVERgHtjBLGxsWjdurXJiiMiItMyOgSGDBmC7OxsREZG4s6dOxgyZAgyMzPx6quvmrI+IiIyIaOPDnJwcMDEiRORm5uLzMxMaLVauLi4mLA0IiIytXJdAKhmzZooKChAeno60tPTK7smIiIyE6P3BI4fP47o6OgybxwTGxtbmTUREZGZGB0Cq1atwssvv4ywsDDY2NiYsiYiIjITo0MgLy8Pzz77rMGF44iIyLIZPSbQtWtX7N2715S1EBGRmRm9J5CSkoIff/wRW7ZsKXVUEC8lTURkmYwOga5du6Jr166lprN7iIjIcj22O2j16tUA7l1KOiwsDHq9Xvo9LCwMCQkJJi+SiIhM47EhEBcXZ/D4q6++Mnh88uTJyq2IiIjM5rEhIISo0PNERFR1PTYEHtfnzzEBIiLL9diB4ZKSEpw6dUp6rNfrSz0mIiLL9NgQcHZ2RnR0tPTY0dHR4LGTk9NjV7J06VIcO3YMzs7OmD9/PoB7J58tWLAAmZmZcHd3x7hx4+Do6Fie90BEROX02BBYsmRJhVcSFhaG7t27Gyxr8+bNCAoKQp8+fbB582Zs3rwZgwYNqvC6iIjIeOW6iuiTCgwMLPUtPyEhAaGhoQCA0NBQHmpKRCQDs4RAWXJzc+Hq6goAcHFxQW5urlylEBEpltFnDJuSSqV65FFGOp0OOp0OABAVFQWtVmuu0iqVRqOx2NqrAktvP6XfecOS/3aA5W9/DyNbCDg7OyM7Oxuurq7Izs5+5ABzREQEIiIipMdZWVnmKLHSabVai629KmD7WTZL/9tZ+vbn5eVV5nTZuoPatGkjnY0cFxeHtm3bylUKEZFimWVPYOHChUhKSsKtW7cwatQoDBgwAH369MGCBQuwZ88e6RBRIiIyL7OEwDvvvFPm9KlTp5pj9URE9BCydQcREZH8GAJERArGECAiUjCGABGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYIxBIiIFIwhQESkYAwBIiIFYwgQESkYQ4CISMEYAkRECsYQICJSMLPcY7iqKBneW9b1p8u6dkC9YqvMFRBRVcM9ASIiBWMIEBEpGEOAiEjBGAJERArGECAiUjCGABGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKpqgLyBGR5VL6BSAB01wEUvYQGDNmDOzs7GBlZQW1Wo2oqCi5SyIiUgzZQwAApk2bBicnJ7nLICJSHI4JEBEpWJXYE5g1axYA4Nlnn0VERITM1RARKYfsITBjxgy4ubkhNzcXM2fOhJeXFwIDAw3m0el00Ol0AICoqChotdpyrasqDOzIqbztVlVoNBqLfg/c/ir2t1N6+wGm+T8sewi4ubkBAJydndG2bVukpqaWCoGIiAiDPYSsrCyz1lhdWHq7abVai38PSsa/XcVVpA29vLzKnC7rmEB+fj7u3r0r/X7ixAk89dRTcpZERKQosu4J5ObmYt68eQCAkpISPPPMM2jZsqWcJRERKYqsIeDp6Ym5c+fKWQIRkaLxEFEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYIxBIiIFIwhQESkYAwBIiIFYwgQESkYQ4CISMEYAkRECib7/QTIcpQM7y3r+uW+qYh6xVaZKyCqfNwTICJSMIYAEZGCMQSIiBSMIUBEpGAMASIiBWMIEBEpGEOAiEjBGAJERArGECAiUjCGABGRgjEEiIgUjCFARKRgDAEiIgVjCBARKRhDgIhIwRgCREQKxhAgIlIwhgARkYIxBIiIFIwhQESkYLLfaP748eNYs2YN9Ho9wsPD0adPH7lLIiJSDFn3BPR6PVatWoUPPvgACxYswMGDB3Hp0iU5SyIiUhRZQyA1NRW1a9eGp6cnNBoNOnbsiISEBDlLIiJSFFm7g27cuIFatWpJj2vVqoWUlJRS8+l0Ouh0OgBAVFQUvLy8yrfC7b+V73V0D9uvYth+FcP2MwmLGBiOiIhAVFQUoqKi5C6lQiZPnix3CRaN7VcxbL+Kqa7tJ2sIuLm54fr169Lj69evw83NTcaKiIiURdYQaNCgAa5evYqMjAwUFxfj0KFDaNOmjZwlEREpiqxjAmq1Gq+//jpmzZoFvV6PLl26oF69enKWZFIRERFyl2DR2H4Vw/armOrafiohhJC7CCIikodFDAwTEZFpMASIiBSMIUBEpGAMAarS/vzzT6OmUdnYfvQ4DAETW716damf9evX8/IYRlqzZo1R06hsbL+KmT59ulHTLJnsVxGt7oqKinDlyhW0b98eAPDLL7/Aw8MDFy5cQGJiIiIjI+UtsIo6c+YMTp8+jZs3b2Lbtm3S9Dt37kCv18tYmWVg+1VMYWEhCgsLcevWLeTl5UnT79y5gxs3bshYWeVjCJjYX3/9hRkzZsDK6t5OV7du3TB16lTMmDEDEyZMkLm6qqu4uBj5+fkoKSnB3bt3pekODg4YP368jJVZBrZfxeh0Omzfvh3Z2dl47733pOkODg7o3r27jJVVPoaAieXl5SE/Px8ODg4AgIKCAuTl5cHKygrW1tYyV1d1BQYGIjAwEGFhYXB3d0dBQQFsbW3lLstisP0qpkePHujRowd+/PFHPP/883KXY1IMARN78cUXMXHiRDRt2hRCCCQnJ+Oll15Cfn4+goKC5C6vysvOzsbs2bORn5+P6OhopKWlQafT4Y033pC7NIvA9quYLl264Ntvv0VWVhZGjhyJq1ev4sqVK2jdurXcpVUaDgybWNeuXTFz5ky0bdsW7dq1w/Tp0xEeHg47OzsMHjxY7vKqvJiYGEyZMgU1a9YEANSvXx/JyckyV2U52H4VEx0dDY1GgzNnzgC4d9HL9evXy1xV5WIImIEQAk5OTqhRowauXbuGpKQkuUuyKFqt1uDx/fEVMg7br/zS09Px4osvQq1WA0C17FJjd5CJrVu3DocPH0bdunWhUqkAACqVCoGBgTJXZhlq1aqF06dPQ6VSobi4GDt27IC3t7fcZVkMtl/FaDQaFBYWSv93r127Bo2men1s8gJyJjZ27FjMmzePg8DldPPmTcTExODkyZMQQqB58+Z47bXXpO4NejS2X8WcOHEC3377LS5duoQWLVrg9OnTGD16NJo2bSp3aZWGIWBis2fPxvjx42FnZyd3KURUDrdu3UJKSgqEEGjYsCGcnJzkLqlSVa/9mirIxsYGEydORFBQkMFu5Ouvvy5jVZbj5s2b0Ol0yMzMRElJiTR99OjRMlZlOdh+FVdUVIQaNWqgpKQEly5dAoBq1Z3LEDCxNm3a8G5pFfDJJ5+gSZMmCAoK4oBmObD9KkYJY3oMARMLCwuTuwSLVlBQgEGDBsldhsVi+1VMQkICFi5cWK3H9BgCJvLpp59i/PjxmDBhgvQN4kHz5s2ToSrL07p1axw7dgxPP/203KVYJLZfxXh6eqKkpKRahwAHhk0kOzsbrq6uyMzMLPN5d3d3M1dkmYYMGYKCggJoNBrpWG2VSoUvv/xS5sosA9uvYubNm4cLFy5U6zE9hgAR0UPs27evzOnVqZuXIWAiQ4YMKbMb6D5+E3tyGzZswIABA+Quw2Kx/Srm3Llz8PPzk7uMSscxARNZu3YtAGD9+vVwdXVFSEgIhBA4cOAAsrOzZa7OMh09epQfYhXA9quY5cuXY86cOXKXUel4zJiJHT16FM899xzs7e3h4OCAbt264bfffpO7LIvEndaKYftVTHVtP4aAidna2iI+Ph56vR56vR7x8fHV8iJU5lAdv4WZU1RUlNwlWLR+/frJXYJJcEzAxDIyMhATE4PTp08DABo3bozIyEh4eHjIXJllSE9Px5o1a5CSkgKVSoVGjRph6NCh8PT0lLs0i8Azhivm119/RbNmzaSbQt2+fRuJiYlo166dzJVVHo4JmJiHhwcmTZokdxkW67PPPsNzzz2HiRMnAgAOHjyIRYsWYfbs2TJXZhl4xnDFbNy40eADv0aNGti0aRNDgB5v9erVj3y+Oh1nbEoFBQUICQmRHoeEhOCHH36QsSLLwjOGK6asjpIH96iqA341MBE/Pz/4+fmhqKgI58+fR506dVCnTh1cuHABxcXFcpdnMVq2bInNmzcjIyMDmZmZ2LJlC1q1aoW8vDzk5eXJXV6Vd/+MYSofPz8/fPnll7h27RquXbuGmJiYaneYKMcETGzKlCmYPn26dLZmcXExpk2bhlmzZslcmWUYM2bMQ59TqVRYvHixGauxPA+eMazRaCCE4BnDRvj888/x1ltv4dtvv0V+fj5OnjwJAGjevDn69u1brS4Nz+4gE8vLy8Pdu3fh6OgIAMjPz+c32CewZMkSuUuwaPfPV6Enc+7cOdy4cQNHjhzBtGnT0Lt3b+nkz+q2J889ARPbu3cvNm7ciKZNm0IIgeTkZPTv379anXZuSsXFxdi1a5d0c/SmTZsiIiKi2t3iz1SEEIiPj0dGRgb69euHrKws5OTkwN/fX+7SqrQdO3bg559/Rnp6Otzc3KTp9/ekqtMeKEPADHJycpCSkgIAaNiwIVxcXOQtyIIsW7YMxcXFUmju378fVlZWGDVqlLyFWYgVK1ZApVIhMTERCxYsQF5eHmbNmoWPP/5Y7tIswooVKzB8+HC5yzApDgybmBACJ06cwIULF9C2bVsUFxcjNTVV7rIsxtmzZ/Hmm2+iWbNmaNasGUaPHo2zZ8/KXZbFSE1NxRtvvCFdCtnR0bHadWeYUnUPAIAhYHIrV67EmTNncPDgQQCAnZ0dVq1aJXNVlsPKygrXrl2THqenp/N49yegVquh1+ul/uybN28+8sKGpDzsWDWx1NRUzJkzRzphjN/EnszgwYPx0UcfwdPTE0IIZGVl4V//+pfcZVmM559/HnPnzkVubi6++eYbHDlyBAMHDpS7LKpCGAImxm9i5afX65GWlobPPvsMV65cAQB4eXlV67s8VbbOnTvDz89POsRx4sSJqFu3rsxVUVXCgWETi4+Px6FDh3Du3DmEhYVJ38Q6dOggd2kW4f333+cgZjk87jDk+4csEzEEzODy5cvSN7FmzZrxm9gTiImJQUlJCTp27Ghw9dXqdtZmZRszZgxUKpXUhebo6AghBG7fvg2tVsvzL0jC7iAzKCgokLqECgsL5S7Holy4cAHAvbtiPWjatGlylGMx7n/IL1u2DO3atZNuNP/7778jISFBztKoimEImNimTZtw+PBhBAcHAwCio6PRvn17vPzyyzJXVrVt27YNAPD0009L32jv45iK8VJSUgzOqWjVqhXWrVsnY0VU1TAETCw+Ph5z586FjY0NAKBPnz6YOHEiQ+Ax7t69CwC4cuUKzp49izZt2gC4d6e2Bg0ayFmaRXFzc8O3336Lzp07AwAOHDhgcAYsEUPAxNzc3FBUVCSFQFFREf8TGqF///4A7nX7zJkzB/b29tJ03iHLeGPHjsXGjRsxb948AEBAQADGjh0rc1VUlTAETOT+/QTs7e0xfvx4NG/eHCqVCidOnOB1W55ATk6OwXWCNBoNcnJy5CvIwjg6OuK1116Tuwyqwnh0kIns27fvkc/zAnLG+e6773D48GG0bdsWAJCQkICOHTvipZdekrmyqi0qKuqRYyfvvfeeGauhqowhQFXeuXPn8OeffwK4153h6+src0VVX1JSEgDgl19+QU5OjjQmcPDgQTg7OyMyMlLG6qgqYQiY2NGjRxEbG4vMzEzo9Xre1IPMavLkyaXGUMqaRsrFMQETi4mJwbvvvounnnqKhzaS2RUUFCA9PR2enp4AgIyMDBQUFMhcFVUlDAET02q1qFevHgOAZDF06FB8+OGHBhfgU8Llkcl47A4ysdTUVMTGxiIwMNDgwme9evWSsSpSkqKiIly+fBkA4O3tzQvwkQHuCZjY+vXrYWdnh6KiIl5Cmszm1KlTaNasGX755ReD6enp6QAgncFOxBAwsezsbMyfP1/uMkhhkpKS0KxZMxw9erTM5xkCdB+7g0xs3bp1CAoKQosWLeQuhRSosLAQv/zyCzIzM1FSUgLg3rWX+vXrJ3NlVFVwT8DEdu3ahR9++AEajQYajYaHiJJZzZ07FzVq1ICvr680FsCDFOhB3BMgqsYmTJjA7kh6JN6x28SEENi/fz82bdoEAMjKykJqaqrMVZFSNGrUCH/99ZfcZVAVxu4gE1u5ciVUKhUSExPRr18/2NnZYdWqVbxlIpnUhAkToFKpUFJSgn379sHDwwPW1tZSd+T9q4oSMQRMLDU1FXPmzMGkSZMA3LuqIw8VJVObPHmy3CWQhWAImJharZZuLQkAN2/e5MAcmZy7u7vcJZCF4MCwicXHx+PQoUM4f/48QkNDceTIEQwcOBAdOnSQuzQiIoaAOVy+fBknT54EADRr1gx169aVuSIiont4dJAZeHt7o3v37tBoNAwAIqpSGAJm9PPPP8tdAhGRAYaAGbHnjYiqGo4JmNj9k8SAeyeKabVaAOC1W4ioSuAhoiZma2sr/e7h4YGjR4/C29tbxoqIiP7GPQEzKyoqwqxZs/Dhhx/KXQoREccEzK2goADXr1+XuwwiIgDsDjK5+9dwAQC9Xo+bN29yPICIqgx2B5lYZmam9LtarYazszPUarWMFRER/Y0hQESkYBwTICJSMIYAEZGCMQSIiBSMIUAWbcyYMThx4oTcZRjYt28fBgwYgC1bthhMHzVqFBITE2WqiqhsDAGiCigpKSlzuqOjI7Zu3Yq7d++auSKiJ8PzBKhaysvLw+LFi5GSkgK9Xo/GjRtj+PDhqFWrFg4fPozNmzdjzpw50vzbtm1DUlISJk2ahKKiInzzzTc4fPgwiouL0bZtW0RGRsLGxgaJiYn4/PPP0b17d2zfvh3NmzfHW2+9VWr93t7eqFGjBrZt24b+/fuXej41NRVr1qzB5cuXYWNjg+DgYAwdOhQazb3/kgMGDMCwYcOwfft25OTkoEePHggLC8PixYtx8eJFtGjRAm+//bY0/9GjR7F+/XpkZmaibt26GD58OHx8fEzUulSdcE+AqiUhBMLCwrB06VIsXboUNjY2WLVqFQCgTZs2yMjIwKVLl6T59+/fj5CQEADAf/7zH1y9ehVz587FZ599hhs3bhhcCDAnJwd5eXlYunQpRo4c+dAaBg4ciB07diAvL6/Uc1ZWVhg6dChWrVqFmTNn4tSpU9i5c6fBPH/88QeioqIwa9YsbN26FV988QXeeustREdH4+LFizhw4AAA4Pz584iOjsaIESOwevVqRERE4JNPPkFRUVH5G5AUgyFA1VLNmjXRvn172Nrawt7eHn379kVycjIAwNraGh07dkR8fDwA4OLFi8jMzETr1q0hhMDu3bsxdOhQODo6Sq89ePCgtGyVSoUBAwbA2toaNjY2D62hfv36CAoKwubNm0s95+fnh0aNGkGtVsPDwwMRERFISkoymKd3795wcHBAvXr1UK9ePTRv3hyenp5wcHBAq1atkJaWBgDQ6XSIiIhAw4YNYWVlhbCwMGg0GqSkpFSwFUkJ2B1E1VJBQQG+/PJLHD9+HLdv3wYA3L17F3q9HlZWVggNDcWiRYswcOBA7N+/Hx06dIC1tTVyc3NRUFCAyZMnS8sSQkCv10uPnZycHvnh/6D/+Z//wQcffIBevXoZTL9y5QrWrl2Ls2fPorCwECUlJfDz8zOYx8XFRfrdxsam1OOcnBwA9y5RHhcXh59++kl6vri4GDdu3DCqRlI2hgBVSz/88AOuXLmC2bNnw8XFBWlpaZg0aZJ0Y59GjRpBo9EgOTkZBw4cwNixYwHc24OwsbHBp59+Cjc3tzKXff9aUMbw9vZGu3bt8N133xlMX7lyJerXr4+xY8fC3t4e27dvx5EjR8r1XmvVqoW+ffuib9++5Xo9KRu7g8jilZSUoLCwUPopKSlBfn4+bGxs4ODggLy8PGzcuLHU60JDQ7F69WpoNBo0adIEwL2++vDwcMTExCA3NxcAcOPGDRw/frzc9fXv3x/79u2T9kiAe3slDg4OsLOzw+XLl7Fr165yLz88PBw///wzUlJSIIRAfn4+jh07xiOTyCjcEyCL9/HHHxs87tu3L3r06IHPPvsMw4YNg5ubG3r16oWEhASD+UJCQhAbG4uXX37ZYPqrr76KTZs2YcqUKbh16xbc3Nzw7LPPomXLluWqz8PDAyEhIQYf9IMHD8YXX3yBLVu2wNfXFx07dsSpU6fKtfwGDRpg5MiRWL16Na5evQobGxs0adIEAQEB5VoeKQsvIEeKVVhYiDfeeANz5sxBnTp15C6HSBbsDiLF2rVrFxo0aMAAIEVjdxAp0pgxYyCEwMSJE+UuhUhW7A4iIlIwdgcRESkYQ4CISMEYAkRECsYQICJSMIYAEZGCMQSIiBTs/wF2nNOZcq94BwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df9 = pd.read_csv(csv_file9)\n",
    "\n",
    "# Plot the values of energy in a bar plot\n",
    "plt.bar(df9['Layer Name'], df9['Energy(mJ)'])\n",
    "plt.xlabel('Layer Name')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy Consumption per Layer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca03d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Dec  9 2021, 17:53:27) \n[GCC 8.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
